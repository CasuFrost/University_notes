\documentclass[12pt, letterpaper]{article}
\usepackage{graphicx} % Required for inserting images
\usepackage{hyperref}
\usepackage{xcolor}
\definecolor{light-gray}{gray}{0.95}
\definecolor{sap}{RGB}{130, 36, 51}
\newcommand{\code}[1]{\colorbox{light-gray}{\texttt{#1}}}
\usepackage{amssymb}
\usepackage{amsmath}
\usepackage[english]{babel}
\usepackage[paper=a4paper,left=20mm,right=20mm,bottom=25mm,top=25mm]{geometry}
\usepackage{wrapfig}
\usepackage{float}
\usepackage{listings}
\newcommand{\acc}{\\\hphantom{}\\}
\title{Sistemi Operativi 1}
\author{Marco Casu}
\date{\vspace{-5ex}}
\begin{document}



\maketitle
\begin{figure}[h]
    \centering{
    \includegraphics[width=0.7\textwidth ]{images/copertina.jpeg}
    }
\end{figure}
\newpage
\tableofcontents
\newpage
\section{Introduzione}
Non esiste una definizione universalmente riconosciuta di sistema operativo, ma una definizione accurata
può essere :
\begin{quote}
    Un sistema operativo, è un implementazione di una macchina virtuale, più facile da programmare rispetto che,
    lavorando direttamente sull'hardware.
\end{quote}
Il sistema operativo (che durante il corso denomineremo come "\textit{OS}"), si interfaccia, o interpone fra
l'hardware ed i programmi ed applicazioni di sistema.
\begin{figure}[h]
    \centering{
    \includegraphics[width=0.8\textwidth ]{images/osHardwareXApplication.png}
    }
\end{figure}\\
Per progettare un OS bisogna avere delle premesse funzionali per capire cosa includere o no dentro tale
sistema, esistono macchine diverse, con scopi ed esigenze diverse, durante lo svolgimento di tale corso
si tratteranno sistemi operativi per macchine a scopo \textit{generico}. \\Un OS è composto da 2 ingredienti :
\begin{itemize}
    \item \textbf{Kernel} - Il nucleo del sistema, costantemente in esecuzione.
    \item \textbf{Programmi di Sistema} - Tutto ciò che non è il nucleo, ossia i programmi che lo circondano.
\end{itemize}  
Non esiste un sistema operativo adatto a qualsiasi circostanza, è sempre necessario scendere a compromessi (
   concetto di \textbf{trade off}
) per soddisfare i requisiti necessari. In una macchina, un OS svolge diversi ruoli, il primo è quello di
\textbf{arbitro}, ossia, rendere equa ed efficente la gestione delle risorse fisiche a disposizione. Un altro 
ruolo è quello di \textbf{illusionista}, ossia servirsi della \textit{virtualizzazione} per dare la parvenza
all'utente che le risorse a disposizione siano infinite. Un ultimo ruolo è quello di \textbf{collante}, cioè 
interporsi fra software ed hardware per permettergli di comunicare, facendo interagire gli utenti con il
sistema piuttosto che con la macchina direttamente. La componente principale che gestisce un OS è la CPU, 
la memoria ed i dispositivi di Input ed Output (che durante il corso denomineremo come "\textit{I/O}"). Un 
componente da tenere in considerazione è il \textit{bus di sistema}, ossia il mezzo di comunicazione fra 
queste entità, tale bus è suddiviso in :
\begin{itemize}
    \item DATA BUS - trasporta i dati effettivi sulla quale si sta operando.
    \item ADRESS BUS - trasporta l'informazione sull'indirizzo dell'istruzione da eseguire.
    \item CONTROL BUS - trasporta l'informazione sul tipo di operazione da eseguire.
\end{itemize}
I dispositivi di I/O sono composti da i dispositivi fisici in se ed i loro \textbf{device controller}, che ne 
gestiscono la logica interfacciandoli con l'OS tramite i rispettivi \textit{driver}, riservando ad essi dei 
registri per determinarne ed immagazzinarne lo stato e la configurazione, per leggere e scrivere dati da 
essi. Per non fare confusione sul bus quando bisogna comunicare con i dispositivi di I/O, sul bus è previsto
uno switch fisico (M/\#IO) che indica se si vuole comunicare con la memoria o con i device controller. 
A tal proposito, le CPU ha 2 modi per comunicare con questi ultimi : \begin{itemize}
    \item \textbf{port mapped} - I registri dei device controller usano uno spazio di indirizzamento separato
    dalla memoria principale, ma è necessario estendere l'insieme delle istruzioni elementari del linguaggio 
    macchina per per poter comunicare con questo nuovo spazio.
    \item \textbf{memory mapped} - I registri dei device controller vengono mappati sugli stessi indirizzi
    riservati alla memoria principale, tale mappatura avviene all'avvio del sistema, non è quindi necessario
    prevedere nuove istruzioni.
\end{itemize}
\textbf{Direct Memory Access Controller}\\
La CPU controlla \textit{periodicamente} lo stato delle richieste di lettura/scrittura. Un altro modo possibile 
per gestirle è quello delle \textbf{interruzioni}, ossia ogni qual volta che la CPU ha richiesto un operazione
di I/O, quando questa viene completata viene inviato un segnale dal device controller.
Tale segnale, insieme al resto delle comunicazioni fra OS e device controller, avviene su un mezzo di comunicazione
dedicato chiamato \textbf{DMA} (\textit{Direct Memory Access Controller}), ed il suo scopo è quello di occuparsi di trasferire dati dalla memoria ai dispositivi di I/O, 
evitando di delegare tale compito alla CPU, soprattutto quando la quantità dei dati da trasferire è 
considerevole.
\subsection{Job scheduling e Time Sharing}\label{scheduling}
I sistemi operativi moderni devono eseguire contemporaneamente un'ampio numero di programmi ed applicazioni 
(pagine web, editor di testo ecc...). Se in passato i sistemi operativi risiedevano in un ambiente \textbf{uniprogrammato},
ossia che nella memoria era salvato un solo programma che veniva eseguito, adesso gli OS moderni godono di un 
ambiente \textbf{multiprogrammato}, dove vengono mantenuti più processi che vengono caricati in memoria.
Ciascun processo ha determinate istruzioni (jobs) che vengono caricati in memoria, il sistema operativo, come è 
di facile intuizione, è costantemente caricato in memoria.\\
Come organizzare l'esecuzione di più processi? Essi vengono salvati in memoria, se un processo richiede 
dei dati tramite un operazione di I/O, esso viene sospeso finché la richiesta non verrà terminata, nel 
mentre la CPU può "portarsi avanti" il lavoro eseguendo altri processi nel mentre. Usiamo il termine 
\textbf{chiamata bloccante} per indicare una chiamata fatta da un processo che, finchè non è terminata,
impedisce al processo di essere eseguito. Quando si ha un considerevole numero di chiamate bloccanti si rischia
di rallentare troppo l'esecuzione dei programmi, qui agisce lo \textbf{scheduler}, ossia un programma di 
sistema che implementa un algoritmo allo scopo di decidere quale processo deve essere eseguito dalla CPU nel 
momento in cui un altro processo in esecuzione viene arrestato da una chiamata bloccante.
\begin{figure}[h]
    \centering{
    \includegraphics[width=0.7\textwidth ]{images/jobScheduling.png}
    }
\end{figure}
\\Assciurando un buon bilanciamento fra processi in esecuzione e chiamate I/O, tramite il job scheduling
la CPU non si arresterà mai ed avrà sempre un processo in esecuzione. Sorge però un altro problema, nel caso 
dovessimo avere un processo considerevolmente lungo, esso verrà eseguito per un \textit{tempo indeterminato} lasciando 
la CPU sempre occupata, si utilizza quindi un sistema di \textbf{time sharing}, in cui ad ogni processo 
è riservato un \textbf{tempo limitato}, in modo tale che esso se troppo lungo, viene momentaneamente arrestato per 
lasciare spazio ad altri processi (il tempo limite per ogni processo è stabilito dal sistema), essendo tali tempi 
molto bassi per la nostra percezione, tramite il time sharing l'utente avrà un'illusione di \textit{parallelismo} 
(solo apparente dato che una CPU può eseguire un solo processo alla volta).\\
È importante considerare che sospendere un processo per mandarne in esecuzione un altro 
(\textbf{content switch}) ha un suo costo, in quanto bisogna salvare lo \textit{stato} del processo precedente
salvando i valori contenuti nei registri e l'ultima istruzione da eseguire in modo da poterlo poi 
\textit{ripristinare} correttamente, per cui il tempo limitato dal sistema secondo il time sharing non deve 
essere troppo piccolo altrimenti si rischia di fare content switch troppe volte rispetto agli effettivi 
calcoli da eseguire.
\section{Architettura Necessaria per i Servizi dell'OS}\subsection{User e Kernel Mode}
Per mantenere un certo livello di sicurezza, all'interno dell'OS è possibile eseguire istruzioni in due 
modalità differenti, \textbf{user mode} e \textbf{kernel mode}. Alcune istruzioni sono più
\textit{sensibili}, se spostare il contenuto da un registro ad un altro (MOV) può essere fatto senza 
problemi, alcune istruzioni di interruzione dovrebbero richiedere un accesso privilegiato, per questo 
si vuole implementare la \textit{kernel mode}, che ha la possibilità di eseguire qualsiasi istruzione.
Fisicamente, si implementa un \textit{bit} che descrive appunto, tramite i suoi 2 stati, se si sta 
operando da utente, o in modalità kernel. Il sistema, in user mode, non può interagire direttamente 
con l'I/O, e non può manipolare il contenuto della memoria. Se l'utente necessita di eseguire operazioni 
in cui è necessaria la kernel mode, utilizzia le \textbf{system call}, delle chiamate, che permettono 
la momentanea transazione in kernel mode per soddisfare la richiesta, per poi ritornare alla modalità 
utente.
\begin{wrapfigure}{l}{0.25\textwidth}
    \includegraphics[width=1\linewidth]{images/protectionRings.png} 
    \caption{Protection Rings}
    \label{fig:wrapfig}
    \end{wrapfigure}
     \hphantom{,}\\ \hphantom{,}\\ \hphantom{,}\\Al minimo è necessario un \textit{bit} per i 2 stati, ma è possibile implementarne molteplici 
se si vogliono definire dei \textbf{protection rings}, ossia più stati di privilegi definiti a strati 
dove al livello 0 c'è il kernel, e gradualmente si hanno meno privilegi più il livello è alto. \hphantom{,}\\
\hphantom{,}\\ \hphantom{,}\\ \hphantom{,}\\ \hphantom{,}\\ \hphantom{,}\\
È anche necessario proteggere la memoria, limitanto ogni processo senza dargli la possibilità di poter
operare su tutta la memoria disponibile, a livello hardware si implementano due ulteriori registri, denominati
\textbf{base} e \textbf{limit}, quando un processo è in corso, ad esso verrà riservata una limitata partizione 
di memoria, in \textit{base} sarà contenuto l'indirizzo iniziale dalla quale parte la memoria disponibile, 
ed in \textit{limit} quello finale, in modo da fornire un \textit{range} di memoria utilizzabile. I valori di 
tali registri si aggiornano ad ogni \textit{content switch}, dato che ad ogni processo è assegnato il suo 
range [base,limit].
\subsection{Le System Call}
Come abbiamo già accennato, l'utente non può interagire direttamente con le istruzioni privilegiate, esistono 
appositamente le \textbf{system call} (o chiamate di sistema), esse richiedono al sistema operativo di eseguire 
determinate operazioni (come scrivere dati su un disco o inviare dati ad un interfaccia di rete), quindi esiste una 
lista di operazioni che l'utente può effettuare tramite esse, sono praticamente l'interfaccia tra l'utente ed il 
sistema operativo. Tali richieste sono definite \textbf{trap}, ossia eventi che causano lo switch da 
user a kernel mode, tali \textit{trap} non sono esclusivamente le system call, ma anche le 
\textbf{eccezioni} (errori generati dal software, privilegi assenti per un istruzione o tentata 
divisione per 0), e le \textbf{interruzioni} (errori generati dall'hardware, come la scadenza del tempo 
prefissato per un job tramite il timer).
\begin{figure}[h]
    \centering{
    \includegraphics[width=0.7\textwidth ]{images/trapTerminologia.png}
    }
\end{figure}
\\Ci sono 6 categorie principali di system call :
\begin{itemize}
    \item \textbf{Controllo dei processi}
    \item \textbf{Gestione dei file}
    \item \textbf{Controllo dei dispositivi}
    \item \textbf{Manutenzione delle informazioni} 
    \item \textbf{Comunicazione tra processi} - per far comunicare due processi, o si utilizza un canale unico di 
    comunicazione tra i due (\textit{message passing}), oppure si fornisce un area di memoria condivisa tra due processi,
    che andrà poi liberata una volta finita la comunicazione(\textit{shared memory}).
    \item \textbf{Protezione} - forniscono agli utenti accesso temporaneo e limitato ai permessi.
\end{itemize}
\subsection{API per le System Call}
Usando le \textbf{API} (Application Programming Interface) al posto delle system call direttamente, è possibile 
fornite maggiore portatilità, e rendere un programma che necessita delle chiamate di sistema indipendente 
dall'hardware. Un API fornisce una libreria per un linguaggio di programmazione di alto livello (come il \textit{C}), 
che ci permette di chiamare funzioni che eseguiranno delle chiamate di sistema.
\begin{figure}[h]
    \centering{
    \includegraphics[width=0.9\textwidth ]{images/syscallFlow.png}
    }
\end{figure}
\newpage
Quando un utente chiama una funzione che richiede una syscall, viene generata un'interruzione, poi in un registro apposito 
(nell'immagine sovrastante "\textit{\%eax}") viene salvato il codice di quella specifica chiamata,
il segnale viene poi mandato alla \textbf{IVT} (Interrupt Vector Table), ossia un vettore all'interno 
del kernel che assegna ad ogni interruzione una casua. Quindi l'IVT, capisce che si tratta di una system call 
e manda il segnale al \textit{System Call Handler}, che si occupa di leggere il contenuto del registro 
prima citato ("\textit{\%eax}"), ed in base al codice, richiamare dalla \textit{System Call Table} la chiamata 
corretta.\\\hphantom{.}\\
Spesso, i parametri da passare non si limitano al codice identificativo della system call. Esistono 3 
diversi modi di passare parametri al sistema operativo :
\begin{itemize}
    \item Salvare parametri in dei \textbf{registri} (ma potrebbero esistere più parametri che registri).
    \item Salvare i parametri in \textbf{blocchi} o "tavole" in un'area di memoria dedicata, passando come 
    parametro nei registri l'indirizzo di tali blocchi.
    \item Passare i parametri inserendoli (\textit{push}) in uno \textbf{stack} dal programma, per poi farli 
    riprendere dallo stack (\textit{pop}) direttamente dal sistema operativo.
\end{itemize}
Il metodo migliore risulta quello dei \textit{blocchi} o dello \textit{stack}, dato che non hanno limiti sulla 
quantità di parametri che si possono possibilmente passare.\\\hphantom{.}\\
Le chiamate di I/O eseguite dalle system call possono essere \textbf{bloccanti} o \textbf{non bloccanti},
le chiamate bloccanti, interrompono il flusso del processo, lasciandolo in "stallo" finchè non si
riceveranno i dati richiesti dalla chiamata. Le chiamate non bloccanti invece, richiedono dati tramite 
le chiamate senza però interrompere il processo, sono quindi più difficili da implementare in quanto 
il programmatore deve considerare che dopo la chiamata, i dati richiesti potrebbero non essere 
da subito disponibili.\\\hphantom{.}\\
Come si è accennato precedentemente, risulta utile a livello fisico implementare un \textbf{timer} che 
segna semplicemente l'orario del giorno corrente (detto \textit{time stamp}), esso è utile allo 
\textit{scheduler}\ref{scheduling}, ad esempio, può generare un interruzione ogni 100 \textit{microsecondi}, in modo che 
lo scheduler possa prendere il sopravvento sul processo per poi decidere quale altro job va eseguito.\\\hphantom{.}\\
Alcune istruzioni sono dette \textbf{atomiche}, ossia, non possono essere fermate dalle interruzioni, le architetture 
che implementano tali sitruzioni devono far si che esse vengano eseguite per intero, piuttosto, vengono
totalmente abortite. Per eseguirle è possibile definirle nel linguaggio macchina come istruzioni speciali 
che sono nativamente eseguite in maniera atomica, oppure, è possibile \textit{disabilitare} momentaneamente 
tutte le interruzioni.
\subsection{La Memoria Virtuale}
La \textbf{memoria virtuale} è un \textit{astrazione} della memoria fisica, da l'illusione ad un processo di 
avere illimitato spazio di memoria per lavorare, e consente ad esso di non essere totalmente caricato 
in memoria, caricandolo appunto nella memoria virtuale. Tale memoria è implementata sia a livello 
hardware(MMU) che software(OS) :
\begin{itemize}
    \item \textbf{MMU} - è il componente che si occupa di tradurre gli indirizzi virtuali in indirizzi fisici.
    \item \textbf{OS} - è responsabile di gestire lo spazio degli indirizzi virtuali.
\end{itemize}
Un sistema a 64 \textit{bit}, è capace di indirzzare \(2^{64}\) \textit{bytes}, gli indirizzi virtuali sono 
suddivisi in blocchi della stessa dimensione, chiamati \textbf{pagine}, le pagine che non vengono caricate 
nella memoria principale, vengono salvate sul disco. Facendo ciò si fornisce ad un processo una quantità \(n\) di 
indirizzi virtuali, che sono salvati sia in memoria che su disco, essi vengono mappati tramite quella che si chiama 
\textbf{page table}, e si utilizza anche una cache chiamata \textbf{TLB} (Translation Look-aside Buffer), che salva 
i recenti "indirizzamenti" per potervi accedere più rapidamente. L'OS deve considerare quali pagine sono 
salvate sul disco, e quali sulla memoria principale.
\section{Design ed Implementazione di un Sistema Operativo}
La struttura interna di un sistema operativo può variare largamente in base alle necessità di utilizzo 
di tale sistema, è necessario separare quelle che sono le \textbf{politiche} del sistema 
(Le funzioni che deve svolgere) dal suo \textbf{meccanismo} (La possibile implementazione di tali funzioni).
Tale distinzione ci permette di rendere l'OS più flessibile alle modifiche, riusabile per implementare nuove 
politiche, e stabile. I primi sistemi operativi erano totalemnte implementati in linguaggio macchina, ciò 
consentiva ad essi di essere molto efficenti, di contro però, erano limitati esclusivamente all'hardware sulla 
quale erano scritti. I sistemi operativi odierni hanno esclusivamente una piccola porzione scritta in 
linguaggio macchina, il corpo principale è scritto in \textit{C}, ed i programmi di sistema possono essere scritti 
in \textit{C++}, ed altri linguaggi di scripting come \textit{Python}. Un OS dovrebbe essere partizionato 
in sotto-sistemi, ognungo con compiti ben definiti. Esistono varie strutture di sistema operativo :
\begin{itemize}
    \item \textbf{Simple Structured} - Un sistema non modulare, nella quale non esiste distinzione tra 
    user e kernel. Risulta facile da implementare, ma pecca di rigidità e sicurezza. Un esempio di un OS che 
    adopera tale struttura è \textit{MS-DOS}.
    \item \textbf{Kernel Monolitico} - Un sistema strutturato in modo che sia tutto un grande ed unico processo, con
    tutti i servizi che vivono nello stesso spazio di indirizzamento. Risulta efficente,
    ma essendo un unico processo non ci sono limiti di visibilità tra diverse componenti, risulta 
    quindi poco sicuro. Un esempio di un OS che adopera tale struttura è \textit{UNIX}.
    \item \textbf{Layered Structured} - Un sistema diviso in \(n\) strati, dove il livello 0 rappresenta l'hardware, 
    ed ogni livello \(k\) implenta delle funzionalità che potranno essere riutilizzate dal livello \(k+1\) per 
    implementare nuovi programmi. Essendo modulare, risulta portatile, ma bisogna implementare dei canali di 
    comunicazione fra i vari strati.
    \item \textbf{Micro Kernel} - È l'opposto del \textit{Kernel Monolitico}. Nel kernel si inseriscono 
    esclusivamente le funzionalità di base, tutto il resto sarà gestito dalle applicazioni a livello utente. Risulta
    sicuro ed estendibile, ma pecca nella comunicazione. 
    \item \textbf{Loadable Kernel} - Ogni componente è separata, l'approccio risulta simile ai linguaggi 
    di programmazione \textit{object-oriented}. I moduli vengono caricati separatamente all'interno del 
    kernel. Ogni componente comunica con le altre tramite un interfaccia, è simile al \textit{Layered Structured},
    ma più flessibile.
\end{itemize}
È importante in base alla struttura utilizzati, provvedere al giusto hardware da implementare. I sistemi 
moderni utilizzano per lo più approcci ibridi.\newpage
\section{La Gestione dei Processi}
Definiamo la differenza tra \textbf{programma} e \textbf{processo} :
\begin{itemize}
    \item Programma - rappresenta l'eseguibile di un certo applicativo, contiene le istruzioni da eseguire ed 
     è contenuto sul disco fisso. 
     \item Processo - rappresenta l'istanza del programma che viene avviato e caricato sulla memoria principale,
     una volta avviato, l'OS si occuperà di tale processo. 
\end{itemize}
Quindi un programma viene \textit{istanziato} in un processo, che è un entità dinamica e viene eseguito 
dalla CPU, ogni processo è un entità indipendente, e possono coesistere due processi istanza dello stesso
programma. Ad ogni processo viene assegnata la sua quantità di memoria disponibile, e le sue istruzioni sono 
eseguite in maniere sequenziale. Il sistema operativo si occupa di creare, distruggere, e gestire gli stati 
dei processi, dedica ad essi la stessa quantità di memoria virtuale, ed il numero di indirizzi disponibili dipendono 
dall'architettura della macchina (ad esempio, con un processore a 32 \text{bit}, si hanno \(2^32\) indirizzi disponibili).
\\\hphantom{.}\\
Quando si crea un processo, ad esso viene assegnata una quantità di memoria divisa in 5 unità logiche :
\begin{itemize}
    \item \code{Text} - contiene le istruzioni eseguibili, ossia il risultato della compilazione.
    \item \code{Data} - contiene le variabili globali o statiche inizializzate.
    \item \code{Data} - contiene le variabili globali o statiche non inizializzate, o inizializzate a 0.
    \item \code{Stack} - struttura LIFO utilizzata per memorizzare i dati ed i parametri necessari alle chiamate di funzioni.
    \item \code{Heap} - struttura dati utilizzata per l'allocazione dinamica della memoria.
\end{itemize}
Per ogni processo quindi, esiste tale area di memoria suddivisa in 5 unità. Lo \textbf{Stack} ha su di esso due 
operazioni, \code{push()} e \code{pop()}, ed un registro dedicato chiamato \textit{Stack Pointer} memorizza 
l'indirizzo alla cima dello stack. Ogni funzione utilizza una porzione dello stack che viene denominata 
\textbf{Stack Frame}, quindi quando si chiamano funzioni dentro altre funzioni, coesisteranno simultaneamente 
più stack frame, anche se esclusivamente uno di essi sarà attivo, ossia quello sulla quale risiede lo 
stack pointer (lo stack "cresce" verso il basso, quindi l'ultima funzione chiamata sarà quella attiva). 
\\\hphantom{.}\\ Lo stack frame contiene :\begin{itemize}
    \item Parametri della funzione ed indirizzo di ritorno
    \item Puntatore al precedente dello stack frame precedente
    \item Variabili locali
\end{itemize}
Quando si chiama una funzione che richiede dei parametri,
essi verrano inseriti (\code{push()}) nello stack, il valore dello stack pointer verrà aggiornato, e verrà inserito nello 
stack anche l'indirizzo dell'istruzione di ritorno. Il problema è che lo stack pointer viene aggiornato ogni 
volta che si chiama una nuova funzione, quindi si usa un altro puntatore detto \textit{Base Pointer} sul fondo 
dello stack, che rimane fisso per ogni stack frame senza aggiornarsi, differentemente dallo stack pointer che per 
forza di cosa, si aggiorna ogni qual volta viene aggiunto un nuovo valore.\newpage \subsection{Stati di un Processo}
Un processo in esecuzione può ritrovarsi in uno dei seguenti 5 \textbf{stati} : \begin{itemize}
    \item \code{new} - Il sistema operativo ha indirizzato le strutture per eseguirlo.
    \item \code{ready} - Il processo ha tutte le risorse necessarie per iniziare o ricominciare ad essere eseguito.
    \item \code{running} - Il processo è in esecuzione sulla CPU.
    \item \code{waiting} - Il processo è sospeso, in attesa che venga soddisfatta una chiamata/richiesta che necessita per poter continuare.
    \item \code{terminated} - Il processo è concluso, l'OS può liberare la memoria dalle risorse che utilizzava.
\end{itemize}
\begin{figure}[h]
    \centering{
    \includegraphics[width=0.7\textwidth ]{images/processStates.png}
    }
\end{figure}
\subsection{Creazione dei Processi}
Il sistema operativo per creare nuovi processi utilizza delle opportune chiamate di sistema. Per convenzione, 
un processo \textit{Parent} (detto anche padre) è quello dalla quale si esegue la chiamata per generare nuovi processi, detti 
\textit{Figli}. Ogni processo ha d.ue valori interi utilizzati per identificare se stesso: \code{PID}, ed 
il suo parent : \code{PPID}. In sistemi come Unix, il process scheduler ha come PID=0, esso inizializza 
come prima cosa un processo noto come \code{init}, che ha PID=1, e si occuperà di creare tutti i processi, sarà 
quindi il parent primario. I processi vengono creati tramite una chiamata di sistema denominata \code{fork()}.
Ogni processo crea più figli, generando una struttura gerarchica ad albero.\\
\hphantom{}\\ La chiamata \code{fork()} nello 
specifico, non crea un nuovo processo, ma crea un processo \textit{clone}, identico a quello chiamante, si utilizza 
poi una chiamata \code{exec()}, che prendendo come parametri l'indirizzo in memoria di un determinato programma, 
sostituirà al processo corrente le istruzioni del programma nuovo che si vuole eseguire. Sarà quindi la congiunzione 
di tali chiamate \code{fork()} ed \code{exec()} a generare un nuovo processo.
\\\hphantom{}\\ 
Quando un processo padre genera un figlio, ha due possibili opzioni :\begin{itemize}
    \item Arrestare la sua esecuzione, ed attendere che il processo figlio appena generato termini prima di ricominciare,
    tramite la chiamata \code{wait()}.
    \item Continuare la sua esecuzione, in maniera concorrente con il suo processo figlio.
\end{itemize}
Vediamo come un programma si occupa di generare un nuovo processo tramite la chiamata di sistema :\newpage
\begin{lstlisting}[language=C]
    #include <sys/types.h>
    #include <stdio.h>
    #include <unistd.h>
    
    int main(){
        pid_t pid;
        /* fork a child process */
        pid = fork();

        if(pid<0){
            /* error occurred */
            fprint("Fork Failed");
            exit(-1);
        }

        else if(pid==0){
            /* child process */
            execlp("bin/ls","ls",NULL);
        }

        else{
            /* parent process */
            wait(NULL);
            printf("Child Complete");
            exit(0);
        }
    }
\end{lstlisting}
Si osservi il codice sopra mostrato. Quando viene eseguito un \code{fork()} e creato un clone, i due processi 
padre e figlio differiranno esclusivamente per il loro PID. La funzione \code{fork()} ritorna il PID del 
processo appena clonato(quindi il padre avrà salvato nella variabile, il PID del figlio)
, il processo figlio, avrà la variabile pid=0, per questo si entrà nel blocco 
di codice che si occuperà di fare l'\code{exec()} sostituendo le istruzioni con quelle del 
programma presente all'indirizzo \code{"bin/ls"}, generando così un nuovo processo. Se il PID è diverso da 0, 
il programma eseguirà una \code{wait()}, aspettando che il processo figlio termini, prima di ricominciare.
\begin{figure}[h]
    \centering{
    \includegraphics[width=0.9\textwidth ]{images/forkGraph.png}
    }
\end{figure}
Sarà quindi il programma scritto dall'utente a dover implementare la logica adeguata (tramite la 
lettura dei PID) per capire se il processo attualmente in esecuzione è quello padre o quello figlio.
\\\hphantom{}\\ 
Un processo può esplicitamente richiedere la sua terminazione tramite la chiamata di 
sistema \code{exit()}, dando un codice di uscita, che per convenzione è 0 quando tale processo 
ha terminato la sua esecuzione senza errori, altrimenti -1, oppure può essere terminato in maniere 
forzata dal suo processo padre. Un processo non può esistere senza padre,
se un processo padre termina ma vi sono ancora dei processi figli in esecuzione, essi, detti 
processi \textit{orfani}, vengono ereditati dal processo \code{init}, che si occuperà di terminarli.
Quando un processo termina, lo spazio dedicato alle sue risorse viene liberato.
\subsection{Scheduling dei Processi}
Un sistema operativo deve far conto a due obbiettivi : \begin{itemize}
    \item far si che la CPU stia costantemente eseguendo processi 
    \item avere un t empo di risposta accettabile per i programmi che interagiscono con l'utente
\end{itemize}
Lo scheduler si occupa di decidere quale processo eseguire venendo in contro a tali obbiettivi, anche se 
essi possono entrare in conflitto fra loro. Il sistema operativo salva tutti i PCBs\footnote{
 Process Control Block
} dei processi in delle code, ci sono 5 code, una per ogni possibile stato. Quando un processo cambia stato, 
il suo PCB viene eliminato dalla coda precedente ed inserito nella nuova coda corrispondente. Ovviamente, 
nella \textit{Running Queue} vi può essere un solo processo per volta (in sistemi con architetture single core).
Nelle altre code, non ci sono limiti teorici di dimensioni.
\\\hphantom{}\\ 
Esistono due tipi di scheduler.\begin{itemize}
    \item Il \textbf{long-term scheduler} si occupa di selezionare i processi da eseguire dalla memoria secondaria 
    (ad esempio, il disco) e di caricarli in memoria principale, viene eseguito poco frequentemente.
    \item Lo \textbf{short-term scheduler} nvece, si occupa di selezionare i
     processi dalla coda dei pronti e di assegnarli alla CPU per l'esecuzione, viene eseguito molto frequentemente,
     circa ogni 50-100 millisecondi.
\end{itemize}
Abbiamo già definit l'operazione di \textit{Content Switch}, ossia quella di interrompere un processo per 
eseguirne un altro. Tale operazione risulta costosa in termini computazionali, dato che è necessario salvare 
lostato del processo da interrompere e caricare lo stato del processo da eseguire dai loro rispettivi 
PCB. Il content switch è quindi operazione da eseguire solo quando necessario, quando avviene un interruzione, oppure 
quando un processo impiega troppo tempo generando un interruzione del timer. I processi che utilizzano principalmente 
la CPU per eseguire esclusivamente calcoli (talvolta pesanti), che quindi richiedono più tempo per essere 
completati, senza però fare richieste di I/O, sono detti \textit{CPU-bound processes}.
\\\hphantom{}\\ 
Il tempo che impiega la CPU per fare content switch è "sprecato" dato che non si stanno effettuando 
calcoli utili all'esecuzione dei processi, quindi a seconda delle disponibilità, è possibile implementare 
un quanto di tempo massimo dedicato ad ogni processo, a seconda delle esigenze :\begin{itemize}
    \item Un quanto di tempo minore farà si che si eseguiranno più content switch, aumentando la responsività.
    \item Un quanto di tempo maggiore risulterà in meno content switch, minimizzando il tempo perso della CPU, massimizzandone
    il suo utilizzo.
\end{itemize}
\subsection{Comunicazione fra processi}
Due processi possono essere fra loro :  \begin{itemize}
    \item \textbf{cooperativi} - possono influire o venire influiti da altri processi per operare su computazioni comuni.
    \item \textbf{indipendenti} - operano in maniera concorrente sul sistema e non si influenzano fra loro.
\end{itemize}
Un processo particolarmente complesso può essere scomposto in più processi \textit{cooperativi}, 
è necessario predispore i processi cooperativi di adeguati canali di comunicazione, ci sono due possibili 
modi di farli comunicare :\begin{itemize}
    \item \textbf{Message Passing} - è il metodo più lento, dato che ogni trasferimento avviene tramite
    una system call, ma è semplice da mettere in atto quindi preferibile se la quantità o la frequenza dei messaggi 
    non è particolarmente elevata.
    \item \textbf{Shared Memory} - è più complesso da mettere in atto e non funziona perfettamente quando si lavora 
    su più computer, non richiede però l'utilizzo delle system call ed è quindi preferibile quando si devono 
    trasferire grandi quantità di informazioni sullo stesso computer.
\end{itemize}
Nel sistema \textit{Shared Memory} viene creato uno spazio di memoria condiviso per due processi differenti, inizialmente 
tale memoria appartiene al PCB di un solo processo, tramite una system call viene poi resa pubblica e disponibile 
ad altri processi, che possono fare delle system call per accedere ad essa.
\\\hphantom{}\\ 
Nel \textit{Message Passing}, il sistema operativo deve implementare delle specifiche system call adibite 
all'invio e ricezione dei messaggi, un collegamento va stabilito tra due processi cooperanti. Il processo 
mittente deve poter identificare il processo destinatario, è possibile creare un canale "uno ad uno" per 
ogni coppia di processi comunicanti, oppure creare delle "porte" condivise dove più processi possono 
comunicare, e trasferire informazioni su tali porte che potranno essere lette da ogni processo. Tali messaggi 
ovviamente occupano spazio in memoria,
solitamente vanno in una coda, 
ci sono 3 diversi modi di gestire la coda ad essi riservata :
\begin{itemize}
    \item \textbf{Zero Capacity} - I messaggi non hanno una coda nella quale vengono salvati, quindi il mittente 
    si blocca e non può inviare altri messaggi finche il destinatario non risponde.
    \item \textbf{Bounded Capacity} - I messaggi vengono salvati in una coda con una
     capacità di memoria finita predeterminata, quindi i mittenti potranno inviare messaggi affinché la coda 
     non sarà piena.
     \item \textbf{Unbounded Capacity} - La coda ha una capacità (teoricamente )infinita, quindi i mittenti 
     potranno inviare messaggi senza blocchi.
\end{itemize} 
\begin{figure}[h]
    \centering{
    \includegraphics[width=0.65\textwidth ]{images/processCommunication.eps}
    }
\end{figure}
\subsection{Algoritmi di Scheduling}
Vediamo in questo paragrafo quali sono alcuni dei criteri presi in considerazione dagli scheduler per selezionare 
il prossimo processo dalla coda pronti che andrà in esecuzione. Lo scheduler deve far si che si alternino in continuazione 
processi CPU-bound e processi che fanno operazioni di I/O, tenendo in considerazione che gli accessi alla memoria 
richiedono svariati cicli di clock, togliendo tempo al calcolo effettivo.\\
\hphantom{text}\\Ci sono 4 possibili situazioni in cui interviene lo short term scheduler per prendere un processo 
dalla coda pronti :\begin{enumerate}
    \item  Quando un processo passa dallo stato running allo stato waiting (ad esempio quando vi è una system call).
    \item  Quando un processo passa dallo stato running allo stato ready
     (quando scade il timer che determina il tempo massimo assegnato ad ogni processo).
     \item  Quando il processo passa dallo stato waiting allo stato ready
      ( il processo interrotto perché ha richiesto una risorsa, vede tale risorsa diventare disponibile).
      \item  Quando viene creato un nuovo processo ed entra nella coda dei processi pronti, oppure quando un processo termina.
\end{enumerate}
Si noti come, nei casi 1 e 4, lo scheduler è costretto ad intervenire, in quanto la CPU necessita di un nuovo 
processo da eseguire, nei casi 2 e 3 invece, l'intervento non è necessario, ed è a discrezione dello scheduler 
se intervenire o meno.\\Gli scheduler che intervengono esclusivamente quando è necessario, si dicono 
\textbf{Non-preemptive}, quelli che invece, intervongono anche nei casi 2 e 3, sono detti \textbf{Preemptive}.\\ 
\hphantom{text}\\Uno scheduler \textit{Preemptive} può essere problematico se agisce in situazioni delicate (Ad esempio, 
mentre il Kernel sta eseguendo una system call), per questo prima di eseguire un interruzione, l'OS si occupa di 
accertarsi che una chiamata di sistema sia completata prima di eseguire il blocco (se necessario, si interrompono 
momentaneamente le interruzioni).\\\hphantom{text}\\
È importante sapere che il modulo dello scheduler che si occupa specificamente di far eseguire 
alla CPU il processo selezionato è noto con il nome di \textbf{Dispatcher}, si occupa di fare context switch,
passare alla user mode, e "saltare" all'indirizzo di memoria dell'ultimo programma caricato.
\subsubsection{Criteri e Politiche dello Scheduling}
Introduciamo adesso alcune notazioni e definizioni importanti riguardo il calcolo dei tempi che un processo 
impiega sulla CPU.\begin{itemize}
    \item \textbf{Arrival Time} (\(T^{arrival}\)) : Il tempo che impiega un processo per arrivare nella coda pronti.
    \item \textbf{Completion Time}  (\(T^{completion}\)) : Il tempo che impiega un processo per completare la sua esecuzione.
    \item \textbf{Burst Time:}  (\(T^{burst}\)) : Il tempo totale nella quale il processo è in esecuzione sulla CPU.
    \item \textbf{ Turnaround Time} (\(T^{turnaround}\))  : La differenza fra il Completion Time e l'Arrival Time, definita 
    come \(T^{completion}-T^{arrival}\) .
    \item \textbf{ Waiting Time} (\(T^{waiting}\)) : La differenza fra il Turnaround Time ed il Burst Time, definita 
    come \(T^{turnaround}-T^{burst}\) .
\end{itemize}
Ci sono diversi criteri da poter prendere in considerazione quando si scrive una algoritmo di scheduling, ed in 
base a tali criteri, esistono algoritmi migliori di altri, tali criteri sono :\begin{itemize}
    \item \textbf{Utilizzo della CPU} - Il tempo in cui la CPU è occupata ad eseguire calcoli, idealmente, dovrebbe 
    essere occupata il 100\(\%\) del tempo, l'obbiettivo è quindi di \textit{massimizzare} tale utilizzo.
    \item \textbf{Throughput} - Il numero di processi da completare per unità di tempo, l'obbiettivo è quindi di \textit{massimizzare} tale numero.
    \item \textbf{Tournaround time} - L'obbiettivo è quello di \textit{minimizzare} il tempo impiegato da un processo, 
    dalla sua selezione da parte dello scheduling fino al completamento (includendo lo Waiting Time).
    \item \textbf{Waiting time} - L'obbiettivo è quello di \textit{minimizzare} il tempo che un processo 
    rimane fermo nella CPU in attesa di essere selezionato dallo scheduler.
    \item \textbf{Response time} - Il tempo che interocorre fra la richiesta di un comando, e la risposta ad esso, l'obbiettivo 
    è quello di \textit{minimizzare} tale tempo per rendere il sistema più interattivo e responsivo.
\end{itemize}
Idealmente sarebbe ottimo scegliere un algoritmo di scheduling che soddisfa 
tutte le richieste appena elencate, ovviamente in una situazione reale, la 
massimizzazione di certi criteri porta svantaggio agli altri, rientra 
quindi il concetto di \textit{trade-off}, sceglieremo quindi un certo 
algoritmo di scheduling in grado di soddisfare solo un certo tipo di 
politiche.\begin{itemize}
    \item Minimizzare il response time medio fa si che l'utente 
    riceva l'output il più rapidamente possibile, minimizzare il tempo massimo 
    di response time invece, previene dagli scenari in cui un processo 
    ha un response time molto più altro rispetto agli altri, minimizzare 
    la varianza invece, rende il response time dei processi più "predicibile" 
    dall'utente, queste politche son tipiche dei \textbf{sistemi interattivi}.
    \item Massimizzare il throughput si traduce in un utilizzo più efficente 
    delle risorse di sistema, e minimizzare lo waiting time da ad ogni processo 
    la stessa quantità di tempo da spendere sulla CPU (di contro, potrebbe 
    aumentare la media del response time), queste politiche son tipiche 
    dei \text{sistemi batch}\footnote{
        tipo di sistema in cui i processi vengono eseguiti  in gruppi o lotti. Invece di eseguire ogni processo in modo interattivo e immediato, un batch system raccoglie una serie di processi o comandi e li esegue in sequenza, senza richiedere l’interazione dell’utente durante l’esecuzione.
    }.
\end{itemize}
Vediamo adesso il funzionamento generale di alcuni algoritmi di scheduling noti,
i quali possono essere preemptive oppure non-preemptive.
\subsubsection{First Come First Served (FCFS)}
Tale algoritmo funziona, ed è implementato come una coda 
\textit{First-In-First-Out}, è quindi molto semplice, in quanto manda 
in esecuzione i processi in ordine di arrivo, lo scheduler entra in funzione 
solamente quando un processo in esecuzione fa una richiesta 
di I/O (oppure termina la sua esecuzione).\acc I processi non sono limitati 
da un tempo massimo quindi possono rimanere in esecuzione sulla CPU 
per un tempo indeterminato, è quindi uno scheduler \textit{non-preemptive}. 
\begin{itemize}
    \item \textbf{Pro} - Risulta molto semplice da implementare.
    \item \textbf{Contro} - Il tempo di attesa (waiting time) è molto 
    variabile. Vi è poca alternanza di processi I/O-bound e processi 
    CPU-bound dato che un processo che fa richiesta di I/O potrebbe trovarsi 
    dopo (e quindi attendere la terminazione) di un processo che 
    impiegherà molto tempo sulla CPU.
\end{itemize}
\subsubsection{Round-Robin (RR)}
È un algoritmo simile al FCFS, solo che impone un limite di tempo 
(detto quanto temporale) ai processi che impiegano troppo tempo 
sulla CPU. Quando un job viene assegnato alla CPU, parte un timer 
(implementato a livello hardware). \acc Se il processo termina prima della scadenza 
del timer, lo scheduler manda in esecuzione il prossimo come un semplice 
FCFS, se invece il timer scade durante l'esecuzione del processo, 
lo scheduler interviene dando in esecuzione alla CPU il prossimo processo, quello 
interrotto invece verrà re-inserito in fondo alla coda, è quindi un sistema 
\textit{preemptive}.\acc Come implicato, i processi pronti sono gestiti 
in una coda circolare, è uno scheduler equo in quanto garantisce a tutti 
i processi lo stesso tempo di esecuzione sulla CPU. Le performance di questo 
algoritmo sono sensibili alla durata del quanto temporale scelto, se il tempo 
del quanto è molto grande, l'algoritmo si comporterà come un FCFS, 
se troppo piccolo, verranno fatti troppi context switch.
\subsubsection{Shortest-Job-First (SJB)}
L'idea, è di dare la priorità ai processi che hanno il 
"carico di lavoro stimato" più grande sulla CPU, con carico di lavoro, si 
intende il tempo che impiegheranno in esecuzione, prima della prossima 
richiesta di I/O o della terminazione stessa.\begin{itemize}
    \item \textbf{Pro} - È ottimale quando l'obbiettivo è quello di 
    minimizzare il tempo di attesa.
    \item \textbf{Contro} - È quasi impossibile sapere con precisione 
    quale sia il prossimo processo con il minor carico di lavoro da eseguire.
    Inoltre, vi è il rischio che i processi con un carico di lavoro 
    elevato entrino nella condizione di \textit{starving}\footnote{
        Restano nella coda senza mai essere selezionati dallo scheduler, 
        in quanto hanno bassa proprità di essere selezionati.
    }.
\end{itemize}
Come si può però "predirre" il tempo che un processo impiegherà sulla 
CPU? L'idea, è quella di basarsi sui tempi di esecuzione dei processi 
precedenti, tramite una tecnica nota come \textbf{exponential smoothing}.\begin{center}
    \(x_t=\) Tempo attuale sulla CPU del \(t\)-esimo processo (valore noto). 
    \\\(s_t=\)  Tempo stimato sulla CPU del \(t\)-esimo processo (valore noto).
    \\Dati \(x_t\) e \(s_t\), vogliamo stimare il tempo di attesa del prossimo processo, 
    ossia \(s_{t+1}\). \\
    Sia \(\alpha\) un coefficente reale : \(\alpha\in [0,1]\), la predizione sarà :\\
\(s_{t+1}=\alpha\cdot x_t+(1-\alpha)\cdot s_t\)
\end{center}
Facciamo alcune osservazioni :\acc
    \(\alpha=0\implies s_{t+1}=s_t\implies\) Si ignora il tempo misurato del processo 
    precedente. e si assume un tempo di lavoro costante per tutti i processi.\acc 
    \(\alpha=1\implies s_{t+1}=x_t\implies\) Si assume che il tempo di lavoro 
    del prossimo processo sia lo stesso del processo precedente.\acc 
Solitamente, si decide che \(\alpha=\dfrac{1}{2}\). Tale algoritmo può essere implementato 
sia in maniera \textit{preemptive} che \textit{non-preemptive} : \begin{itemize}
    \item Una volta che un processo viene assegnato, rimane sulla CPU fino alla prossima 
    interruzione (o terminazione).
    \item Quando un nuovo processo arriva nella coda pronti, si controlla se 
    esso ha un tempo di lavoro stimato maggiore del processo in esecuzione, se si, 
    viene eseguito lo switch.
\end{itemize}
\subsubsection{Priority Scheduling} 
È il caso generale del SJF, in cui ad ogni job viene assegnato un 
grado di priorità in base a certi parametri, e lo scheduler 
prediligerà i processi con la priorità più alta (nel caso del SJF, 
più il carico di lavoro è basso, più il processo è prioritario). Il livello 
di priorità non è altro che un valore intero, mantenuto in un 
certo range (usualmente, più il numero è basso più la priorità 
è alta).\acc Le politiche possono essere assegnate :\begin{itemize}
    \item \textbf{Internamente} - Il grado di priorità è assegnato 
    dal sistema operativo considerando politiche e criteri interni come il tempo di lavoro 
    sulla CPU, l'alternanza fra richieste di I/O ed esecuzione ecc..
    \item \textbf{Esternamente} - Il grado di priorità è assegnato dall'utente 
    basandosi su criteri totalemente arbitrari in base all'importanza 
    del processo.
\end{itemize}
Come visto prima, tale algoritmo può essere implementato 
sia in maniera \textit{preemptive} che \textit{non-preemptive}.\acc
Si è accennato precedentemente della \textbf{starvation}, ossia della condizione 
in cui un processo con bassa priorità, rischia di non essere mai selezionato in 
quanto surclassato da processi sempre con gradi di priorità superiori. La 
soluzione più ovvia risulta essere quella denominata con \textbf{aging}, ossia, 
l'operazione di aumentare il grado di priorità di un processo nel tempo, in modo che,
anche i processi poco prioritari, con l'andare avanti nel tempo verranno prima o poi 
schedulati.
\subsubsection{Multi-Level-Queue (MLQ)}
L'insieme dei processi viene diviso in partizioni, in base alla categoria 
di ogni processo (ad esempio, una categoria per i processi grafici, una per i processi 
 correlati all'audio, ecc..), e si applicano algoritmi diversi di scheduling 
 su ogni categoria. \acc Un esempio è l'applicazione della \textbf{Strict Priority}, 
 in cui i processi vengono divisi in diverse code in base ad un grado di priorità 
 deciso, e poi ogni singola coda avrà il proprio algoritmo di scheduling, ad esempio, 
 il RR, dove ogni coda ha un quanto temporale diverso dalle altre.
  \acc \textbf{Attenzione} : Nessun processo può passare da una coda ad un altra.
  \begin{figure}[h]
    \centering{
    \includegraphics[width=0.8\textwidth ]{images/expSmoothing.eps}
    }
\end{figure}\\
\subsubsection{Multi-Level-Feedback-Queue (MLFQ)}
L'idea è simile alla normale MLQ, solo che in questo caso, i processi possono \textit{muoversi fra 
le diverse code}. Ciò può essere necessario in diverse situazioni, per rendere lo scheduling più 
adattivo, ad esempio : \begin{itemize}
    \item Un processo cambia le sue caratteristiche, alternandosi nell'essere un CPU-Bound ed un processo che 
    fa molte richieste di I/O.
    \item Un processo può rimanere in attesa troppo tempo, quindi l'aging interviene spostandolo in una coda con 
    priorità maggiore.
\end{itemize}
Di default, i processi vengono tutti inizializzati nella coda con priorità più alta, che ha a sua volta il 
quanto temporale più corto. Se il tempo a disposizione scade, vengono spostati nella coda successiva, di un grado 
di priorità inferiore, e così via. \acc Se il quanto temporale di un processo invece non lo blocca (ad esempio, il processo 
si interrompe con richieste di I/O sempre prima che il suo tempo a disposizione scada), allora viene spostato 
nella coda superiore, con priorità più alta. I processi che richiedono molto calcolo e tempo sulla CPU, cadranno rapidamente 
nelle code più "basse" con meno priorità, differentemente, i processi che fanno molte richieste di I/O 
rimarranno nelle code più "alte".\acc 
Uno scheduling MLFQ è estremamente flessibile, ma anche molto complesso da implementare, bisogna gestire svariati 
parametri, quali : \begin{enumerate}
    \item Il numero delle code.
    \item L'algoritmo di scheduling per ogni coda. 
    \item Le politiche adoperate per spostare i processi fra le code. 
    \item Il metodo utilizzato per determinare in quale coda un processo dovrebbe essere inizializzato.
\end{enumerate}
Questo algoritmo, si comporta in maniera simile al SJF in termini di tempo di attesa, in quanto cerca di privilegiare i 
processi corti, potrebbe quindi essere poco equo.
\subsubsection{Lottery Scheduling} 
Esiste un tipo di scheduling inusuale basato sulla casualità. Ad ogni processo, viene assegnato un certo 
numero di "ticket". Ad ogni intervallo di tempo, si estrae \textbf{casualmente} (con probabilità uniforme) un 
numero. Il processo in possesso del ticket con il numero estratto, verrà schedulato. Con l'andare avanti del tempo, 
per la \textit{legge dei grandi numeri}, prima o poi ogni processo verrà schedulato. \acc 
Risulta opportuno dare più ticket ai processi più corti, in modo che possano essere schedulati 
più frequentemente (simulando il SJF). Per evitare la starvation, si assegna ad ogni processo 
alemeno un ticket. Inoltre, aggiungere o rimuovere processi dalla code, condiziona in maniera proporzionale 
la probabilità di essere selezionati di tutti gli altri processi. Questo tipo di scheduling quindi, \textbf{non è 
deterministico}, e si basa sulla \textit{randomness}.\begin{center}
    \(m_i:=\)  numero di ticket assegnati al processi \(i\)\\\(N:=\) numero dei processi \\\(M=\displaystyle\sum_{i=1}^N:=\) numero totale dei ticket\\
\(\mathbb{P}(i)=\dfrac{m_i}{M}:=\) probabilità che il processo \(i\) venga schedulato.
\end{center}
\section{I Threads}
\subsection{Definizione e Motivazioni}
Fino ad ora, abbiamo sempre trattato ogni processo come un entità singola (ossia, \textit{single-threaded}), ma 
i sistemi operativi moderni sono per la maggiorparte orientati alla gestione dei processi \textit{multi-threaded}.\acc 
Un \textbf{thread}, è l'unità più piccola schedulabile sulla CPU, è composto da un program counter, uno stack, un insieme 
di registri dedicati ed un thread ID. I processi, sono composti da più thread, che risultano essere più unità di controllo.
Diversi thread di un singolo processo, hanno differenti PC, stack e registri, ma condividono lo stesso codice e la 
stessa quantità di memoria dedicata. \begin{itemize}
    \item Un \textbf{processo} definisce uno spazio di indirizzamento, il codice, e le risorse.
    \item Un \textbf{thread} definisce una singola sequenza/flusso di esecuzione all'interno di un processo.
\end{itemize}
\begin{figure}[h]
    \centering{
    \includegraphics[width=0.73\textwidth ]{images/threads.eps}
    }
\end{figure}Un thread ovviamente non esiste "singolarmente", ma è sempre parte di un processo. "Vivendo" nello stesso codice e 
nella stessa memoria, la cooperazione fra più thread di un processo risulta facile e non necessita di chiamate di sistema.\acc 
Quando un processo deve svolgere più compiti è oppurtuno suddividerlo in diversi thread, soprattutto quando un particolare 
compito potrebbe interrompersi, se esso viene programmato come thread singolo, la sua interruzione non causerà 
l'interruzione dell'intero processo.\acc 
Teoricamente, ogni compito di un processo potrebbe essere implementato come un nuovo processo single-threaded, ma non 
risulta essere la migliore opzione, in quanto la comunicazione fra thread di uno stesso processo è più rapida, ed i context-switch 
lo sono altrettanto. L'utilizzo dei thread, porta \textbf{4 benefici principali} : \begin{enumerate}
    \item \textbf{Responsività} - Un thread provvede ad essere rapido e responsivo, in quanto non viene interrotto, se il resto 
    dei thread del suo stesso processo sono bloccati o rallentati da computazione intensa.
    \item \textbf{Condivisione delle risorse} - Diversi thread condividono lo stesso codice e spazio di indirizzamento.
    \item \textbf{Convenienza} - Creare e gestire diversi thread è più rapido rispetto ad eseguire le stesse operazioni ma con 
    processi differenti. 
    \item \textbf{Scalabilità} - Nei sistemi a più processori, un singolo processo può vedere diversi thread venire 
    eseguiti contemporaneamente sui diversi processori.
\end{enumerate}
Spesso, le architetture moderne, prevedono diversi \textit{core}, ossia, più CPU sono disponibili al calcolo, 
permettendo il vero parallelismo. Ci sono due modi differenti per "parallelizzare" il carico di lavoro :\begin{itemize}
    \item \textbf{Parallelismo dei dati} - Si divide la porzione di dati su cui lavorare in diversi core, gestiti 
    da diversi thread, e si performa l'operazione su entrambe le porzioni di dati.
    \item \textbf{Parallelismo delle operazioni} - Si divide il compito da svolgere in diverse operazaioni che verranno 
    eseguite simultaneamente su diversi core.
\end{itemize}
Esistono diversi problemi complessi che svolgono sia operazioni intensive di I/O, che calcoli 
intensi sulla CPU. La suddivisione in thread può risultare utile anche in architetture a single-core. Dividendo le operazioni 
I/O e di calcolo, è possibile eseguirle nel solito pseudo parallelismo alla quale siamo abituati, senza che 
il calcolo intensivo blocchi le richieste di I/O e viceversa. \acc Anche se suddividere in thread un operazione 
puramente CPU-bound può essere contro producente, ciò risulta comunque più efficace in quanto elimina il tempo di attesa 
che il processo dovrebbe attendere nel mentre che una richiesta di I/O non è stata ancora completata.
\subsection{User e Kernel Thread}
La gestione dei thread può avvenire :\begin{itemize}
    \item A livello del Kernel, che gestisce i così detti \textbf{Kernel thread}.
    \item A livello utente, gestiti nello spazio utente da delle \textbf{thread library}, senza la necessità che intervenga l'OS.
\end{itemize} 
Un \textbf{Kernel thread} è la più piccola unità schedulabile dall'OS, e quest'ultimo è responsabile di gestirli, ad 
ogni processo è associato un \textit{Process Control Block} (PCB), e ad ogni thread un \textit{Thread Control Block} (TCB). 
Il sistema operativo mette a disposizione dell'utente una serie di chiamate di sistema per creare e gestire i thread.\begin{itemize}
    \item \textbf{Pro} : Il kernel ha piena conoscenza di questi thread, lo scheduler quindi, sapendo quanti thread sono associati 
    ad un processo, può riservargli più tempo sulla CPU. Sono ottimi nelle applicazioni che prevedono svariate interruzioni 
    ed eseguire lo switch fra i thread risulta più rapide che eseguire lo switch fra processi. 
    \item \textbf{Contro} : La complessità del Kernel aumenta significativamente, inoltre, invocare troppe 
    volte il kernel per 
    la gestione di questi thread è inefficente. Anche se il context-switch è più rapido, richiede sempre l'avvento del 
    kernel.
\end{itemize}
Gli \textbf{user thread} sono gestiti totalmente durante l'esecuzione da delle librerie a livello utente. Il Kernel, non 
sa dell'esistenza di questi thread, e li tratta/vede come se fossero dei processi a single-thread. Idealmente, le operazioni fra 
thread di uno stesso processo dovrebbero essere rapide come delle chiamate di funzione.\begin{itemize}
    \item \textbf{Pro} : Molto veloci e leggeri, e le politiche di scheduling sono flessibili. Possono essere 
    implementati in sistemi che non supportano nativamente il multi-threading, e non richiedono chiamate di sistema, ma 
    solo chiamate di funzione, non avvengono dei veri content-switch. 
    \item \textbf{Contro} : Non avviene una vera concorrenza fra i vari thread, e le possibili decisioni che può 
    prendere lo scheduler sono limitate, il Kernel non sa nulla di essi, quindi potrebbe far competere per un quanto 
    di tempo un processo con 100 thread con uno disposto di un singolo thread, richiede delle chiamate di sistema non 
    bloccanti, d'altro canto gli altri thread del processo devono comunque attendere.
\end{itemize}
\subsubsection{Modelli di Multi-Threading}
Quando si vuole implementare un sistema multi-threading, bisogna decidere come gli user thread verranno 
"mappati" ai Kernel thread, per essere schedulati sulla CPU, vi sono diversi modi.\acc
\textbf{Many-to-One} : Più user thread vengono mappati in un solo Kernel thread, quindi il processo, vedrà i suoi 
thread venire eseguiti uno alla volta, dato che un solo kernel thread schedulato sulla CPU vi è associato, quindi non 
possono essere suddivisi su diversi core, dato che su ogni core può lavorare un solo kernel thread. Una chiamata di sistema 
può bloccare l'intero processo, anche se i thread potrebbero idealmente continuare.
\begin{figure}[h]
    \centering{
    \includegraphics[width=0.61\textwidth ]{images/manyToOne.eps}
    }
\end{figure}
\\\textbf{One-to-One} : 
Ogni singolo user thread viene mappato su un Kernel thread, non vi sono più limitazioni sulle 
chiamate bloccanti e si può dividere un processo su più CPU. Gestire però un sistema di questo tipo può 
è più complesso e può rallentare l'esecuzione, molte implementazioni di questo tipo, prevedono delle 
restrizioni sul numero di thread che possono coesistere simultaneamente.
\begin{figure}[h]
    \centering{
    \includegraphics[width=0.3\textwidth ]{images/OneToOne.eps}
    }
\end{figure}
\\\textbf{Many-to-Many} : 
Un qualsiasi numero di user thread può essere mappato in un qualsiasi numero di Kernel thread, l'utente non ha 
alcun tipo di restrizione sul numero di thread che possono essere creati, i processi possono essere divisi 
su più CPU e le chiamate bloccanti non interrompono l'intero processo.\begin{figure}[h]
    \centering{
    \includegraphics[width=0.3\textwidth ]{images/manyToMany.eps}
    }
\end{figure}
\\\textbf{Two-Level} : È una variante del modello \textit{Many-to-Many}, in cui un singolo user thread, ha la 
possibilità di essere mappato su un singolo Kernel thread, ciò aumenta la flessibilità delle politiche di scheduling.\begin{figure}[h]
    \centering{
    \includegraphics[width=0.3\textwidth ]{images/TwoLevel.eps}
    }
\end{figure}
\subsubsection{Le Librerie per la Gestione dei Thread}
Esistono delle librerie che forniscono all'utente delle API per creare e gestire i thread, e ci sono due 
 modi principali per implementarle.\begin{itemize}
    \item Con delle funzioni interamente definite da delle API all'interno dello 
    spazio utente.
    \item Implementate nel Kernel, con delle system call (è necessario 
    che il Kernel supporti la programmazione multi-thread).
 \end{itemize}
 Vediamo come funziona una delle thread library più in uso tutt'oggi, 
 ossia la \textit{Java Thread Library}. I thread, in questa libreria, son 
 detti \textit{Java thread}, e possono essere creati in due modi, estendendo 
 la classe \code{Thread}, oppure implementando l'interfaccia 
 \code{Runnable}, per entrambe le soluzioni, è richiesto che l'utente 
 esegua l'override del metodo \code{run()}. Essendo che ogni classe 
 può estendere solo una classe, è prefiribile implementare l'interfaccia 
 \code{Runnable}.
\subsubsection{Thread Pools}
Nell'esempio della Java Thread Library, abbiamo visto come, gestire una certa richiesta 
creando più thread, piùttosto che più processi, è più efficente. Nonostante ciò, 
possono occorrere dei problemi, il numero dei possibili thread coesistenti 
non è limitato, e può diventare pesante creare troppi thread se ci sono 
troppe richieste.\acc 
Vediamo cos'è una \textit{thread pool}, quando un processo viene creato, 
si genera un numero definito di thread, tali thread sono "vuoti", e vengono messi 
in attesa che arrivi una richiesta da svolgere. Quando occorre una richiesta, uno dei thread 
dalla pool verrà selezionato ed adoperato a dovere, una volta terminata tale richiesta, 
il thread sarà di nuovo in attesa, disponibile per essere selezionato. Se non ci sono 
thread disponibili,  la richiesta dovrà attendere.\acc 
L'utilizzo delle thread pool porta svariati benefici : \begin{itemize}
    \item Assegnare una richiesta ad un thread già esistente, è più 
    efficente piuttosto che crearne uno ogni volta.
    \item la pool, limita il possibile numero di thread che possono 
    esistere contemporaneamente.
    \item Separare il compito da svolgere, dal meccanismo che si occupa di 
    eseguirlo, permette di usare differenti strategie per eseguire un compito(
        ad \textit{esempio}, potremmo far si che un'operazione venga 
        eseguita dopo un certo delay temporale definito, oppure 
        periodicamente).
\end{itemize}
Che succede su viene eseguito un \code{fork()} su un thread? Viene duplicato 
esclusivamente il thread o l'intero processo? Tale decisione, dipende dalle 
specifiche del sistema operativo, se il nuovo processo chiama immediatamente 
un \code{exec()}, non c'è bisogno di copiare tutti i thread associati a quel 
processo. Molte versioni di UNIX, implementando differenti tipi della chiamata
\code{fork()} a seconda della situazione.\acc 
Se un processo multi-thread riceve un segnale, quale dei thread associati dovrà 
riceverlo? Ci sono 4 possibili modi di gestire i segnali : \begin{enumerate}
    \item Inviare il segnale allo specifico thread che lo richiede. 
    \item Inviare il segnale a tutti i thread del processo.
    \item Inviare il segnale ad un certo gruppo di thread.
    \item Definire un thread specifico, che avrà lo scopo di 
    ricevere e gestire tutti i segnali.
\end{enumerate}

\end{document}