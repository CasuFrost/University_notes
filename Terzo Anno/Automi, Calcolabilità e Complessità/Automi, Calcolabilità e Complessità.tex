\documentclass[10pt, letterpaper]{report}
% !TeX program = xelatex
%==================PREAMBOLO=======================%
\input{../../preamble/preamble.tex}
 %TOGLI COMMENTO SE USI XELATEX
%\usepackage{fontspec}
\title{Automi, Calcolabilità e Complessità} %========TITOLO========%
\author{Marco Casu}
\date{\vspace{-5ex}}
\begin{document}
\usetikzlibrary{automata, arrows.meta, positioning}
%==================coPERTINA=======================%
\begin{titlepage}
    
\begin{center}
    %TOGLI COMMENTO SE USI XELATEX
   %\setmainfont{Palace Script MT}
   \HUGE Marco Casu\acc
    %\setmainfont{Grand Casino}
     %TOGLI COMMENTO SE USI XELATEX
    %\setmainfont{h Halfroad}
    \Huge \decothreeleft\hphantom{ }{\selectfont Automi, Calcolabilità e Complessità}\hphantom{ }\decothreeright
     %TOGLI COMMENTO SE USI XELATEX
   % \setmainfont{Times New Roman}
\end{center}
\thispagestyle{empty}
\begin{figure}[h]
    \centering{
        %l'immagine deve avere una risoluzione 2048x2048
        \includegraphics[width=1\textwidth ]{images/copertina2.jpeg}
    }
\end{figure}
\vfill 
\centering \includegraphics[width=0.4\textwidth ]{../../preamble/Stemma_sapienza.png} \acc
\centering \Large \color{sapienza}Facoltà di Ingegneria dell'Informazione,
Informatica e Statistica\\
Dipartimento di Informatica
\end{titlepage}

%===================FINE coPERTINA======================%
\newpage
\pagecolor{cartaRiciclata}%\setmainfont{Algerian}
\Large
Questo documento è distribuito sotto la licenza 
\color{blue}\href{https://www.gnu.org/licenses/fdl-1.3.txt}{GNU}\color{black},  
è un resoconto degli appunti (eventualmente integrati con libri di testo) tratti dalle lezioni del corso di Automi, Calcolabilità e Complessità
\hphantom{a}per la laurea 
triennale in Informatica. Se dovessi notare errori, ti prego di segnalarmeli.
\vfill
\begin{figure}[h!]
    \raggedright
    \includegraphics[width=0.4\textwidth,right ]{../../preamble/tomodachi.pdf} 
\end{figure}
\newpage %\setmainfont{Times New Roman}
\normalsize
\tableofcontents 
\newpage

%==================FOOTER e HEADER=======================%
\fancyhf{}
\fancyhead[L]{\nouppercase{\leftmark}}
\fancyhead[R]{Sezione \thesection}
\fancyfoot[C]{\thepage}
\fancyfoot[L]{Appunti di Automi, Calcolabilità e Complessità}
\fancyfoot[R]{ Marco Casu}
%\fancyfoot[R]{\setmainfont{Palace Script MT}\huge Marco Casu \setmainfont{Times New Roman}}
%==================FOOTER e HEADER=======================%

%Ricorda del comando \flowerLine per separare le sottosezioni. Le sezioni si separano nelle diverse pagine

%==================INIZIO======================%
\chapter{Automi}
\section{Linguaggi Regolari}
Un \textit{automa a 
stati finiti} è, seppure limitato nella memoria e nella gestione 
dell'input, il più 
semplice modello di computazione. Un automa può interagire con l'input 
esclusivamente "scorrendolo" in maniera sequenziale.\acc 
\textbf{Esempio} : Si vuole modellare una semplice porta con sensore, che 
si apre quando qualcuno si trova nelle vicinanze.
\begin{center}
    \includegraphics[width=0.6\textwidth ]{images/porta.eps}
\end{center}
Un automa che modella il problema è il seguente :\begin{center}
    
    
    \begin{tikzpicture} [node distance = 6cm, on grid, auto]
        \node (aperta)[state, left] {aperta};
        \node (chiusa)[state, right = of aperta] {chiusa};
        \path [-stealth, thick]
        (aperta) edge [loop left]  node {rileva persone}()
        (chiusa) edge [loop right]  node {nessuno}()
        (aperta) edge [bend left] node {nessuno}   (chiusa)
        (chiusa) edge [bend left] node {rileva persone}   (aperta);
    \end{tikzpicture}
\end{center} 
Un automa ha alcuni stati speciali, come quello iniziale, indicato 
con un apposita freccia, e degli stati detti \textit{di accettazione}, 
ossia stati in cui deve necessariamente terminare la computazione per 
essere definita valida, vengono rappresentati con un doppio cerchio.
\acc 
Il modello di calcolo degli automi è riconducibile al concetto di 
\textit{linguaggio regolare}, che verrà formalizzato in seguito, 
segue ora una definizione formale di automa.\acc 
\defi{(DFA)}  Un DFA (Deterministic Finite Automa) è una 
5-tupla, $(Q,\Sigma,\delta, q_0, F)$ di cui\begin{itemize}
    \item $Q$ è l'insieme degli stati possibili 
    \item $\Sigma$ è l'alfabeto che compone le stringhe in input 
    \item $\delta$ è una mappa $Q\times \Sigma \rightarrow Q$ detta 
    \textit{funzione di transizione}. 
    \item $q_0\in Q$ è lo stato iniziale. 
    \item $F\subseteq Q$ è l'insieme degli stati di accettazione.
\end{itemize}
\begin{figure}[h!]
    \centering
    \begin{tikzpicture} [node distance = 3cm, on grid, auto]

        \node (q0) [state, initial, accepting, initial text = {}] {$q_1$};
        \node (q1) [state, above right = of q0] {$q_2$};
        \node (q2) [state, below right = of q1] {$q_3$};
        
        \path [-stealth, thick]
            (q0) edge node {0,1}   (q1)
            (q1) edge node {1}   (q2)
            (q1) edge [loop above]  node {0}()
            (q2) edge node {1,0} (q0);
        \end{tikzpicture}
        \caption{semplice automa}
        \label{fig:automaSemplice}
\end{figure}
Nell'esempio in figura \ref{fig:automaSemplice}, si ha che\begin{itemize}
    \item $Q=\{q_1,q_2,q_3\}$
    \item $\Sigma = \{0,1\}$
    \item $F=\{q_1\}$
    \item $q_0=q_1$
    \item $\delta =\;\;\; $ \begin{tabular}{c|c|c}
        & 0     & 1     \\ \hline
  $q_1$ & $q_2$ & $q_2$ \\ \hline
  $q_2$ & $q_2$ & $q_3$ \\ \hline
  $q_3$ & $q_1$ & $q_1$
  \end{tabular}
\end{itemize}
Sia $D$ un DFA, chiamiamo \textbf{linguaggio dell'automa}, e denotiamo 
$L(D)$, l'insieme delle stringhe che date in input a $D$ fanno si che 
$D$ termini su uno stato di accettazione.
\begin{figure}[h!]
    \centering
    \begin{tikzpicture} [node distance = 3cm, on grid, auto]
        \node (q0)[state, left, initial] {$q_1$};
        \node (q1)[state, right = of q0, accepting] {$q_2$};
        \path [-stealth, thick]
        (q0) edge [loop above]  node {0}()
        (q1) edge [loop above]  node {1}()
        (q0) edge [bend left] node {1}   (q1)
        (q1) edge [bend left] node {0}   (q0);
        \end{tikzpicture}
        \caption{il linguaggio di tale automa 
        risulta essere composto dalle stringhe che 
        terminano con $1$}
        \label{fig:automaSemplice2}
\end{figure}
Per definire formalmente un linguaggio di un automa, è 
necessario introdurre la \textbf{funzione di transizione estesa}:
$$ \delta^*(q,\epsilon)=\delta(q,\epsilon)$$
$$\delta^*(q,ax)=\delta^*(\delta(q,a),x)$$
dove
$$ a\in \Sigma,\;\;x\in \Sigma^*,\;\;\epsilon = \text{ stringa vuota}$$
$\Sigma^*$ è l'insieme di tutte le stringhe formate dall'alfabeto 
$\Sigma$. Passiamo ora alla definizione di \textbf{configurazione}, essa 
rappresenta lo stato dell'automa ad un certo punto della 
computazione,  è formata da una coppia 
$$ Q\times \Sigma^*$$
Rappresentante uno stato, ed una stringa di input rimanente da 
computare.\acc 
Un \textbf{passo della computazione} in un automa rappresenta 
una transizione da una configurazione ad un altra, è una 
relazione binaria $\vdash_D : Q\times \Sigma^*$ tale che 
$$ 
(p,ax)\vdash_D (q,x)\iff \delta(p,a)=q \;\;\text{ dove }\;\;
p,q\in Q,\;\;a\in\Sigma,\;\;x\in\Sigma^*$$
Si può estendere la definizione di passo di computazione, 
considerando la sua \textit{chiusura transitiva} 
$\vdash_D^*$. Essa si ottiene aggiungendo a $\vdash_D$ tutte 
le coppie in $ Q\times \Sigma^*$ che rendono 
la relazione chiusa rispetto la riflessività e rispetto 
la transitività.
$$ \begin{matrix}
    (q,aby)\vdash_D (p,by) \;\land\;  
    (p,by)\vdash_D (r,y) \implies 
    (q,aby)\vdash_D^*(r,y)
\end{matrix}$$
Ad esempio, nell'automa in figura \ref{fig:automaSemplice2}, 
risulta chiaro che 
$$\begin{cases}
    (q_1,011)\vdash_D (q_1,11)\\
    (q_1,11) \vdash_D (q_2,1)\\
    (q_2,1) \vdash_D (q_2,\epsilon)
\end{cases} \implies
(q_1,011) \vdash_D^* (q_2,\epsilon)$$
Inoltre$$ \begin{matrix}
    \delta^*(q_1,011) = \\ 
    \delta^*(q_1,11)  = \\ 
    \delta^*(q_2,1) = \\ 
    \delta^*(q_2,\epsilon) = q_2
\end{matrix}$$
Se non specificato diversamente, con $\epsilon$ verrà indicata 
la stringa vuota. Utilizzando le precedenti definizioni, è possibile definire formalmente 
quali sono gli input accettati da un DFA.\acc 
\defi{}  Sia $D=(Q,\Sigma,\delta, q_0, F)$ un DFA, e sia $x\in \Sigma^*$ una stringa, essa 
è \textbf{accettata} da $D$ se $$ \delta^*(q_0,x)\in F$$
Il \textbf{linguaggio riconosciuto} da $D$ è \eqImportante{$ L(D)=\{x\in \Sigma^*|\delta^*(q_0,x)\in F\}$}
\defi{(Linguaggi Regolari)}  L'insieme dei linguaggi regolari, denotato $REG$, contiene tutti 
i linguaggi, tali che esiste un DFA che li ha come linguaggi riconosciuti.
\eqImportante{$ REG = \{L\;\;|\;\;\exists D=(Q,\Sigma,\delta, q_0, F) \text{ t.c. } L\subseteq \Sigma^* \land L(D)=L\}$}
Uno fra gli scopi di questo corso riguarda il capire come progettare automi, e capire se, 
ogni linguaggio è regolare, o ce ne sono alcuni che non possono essere riconosciuti da 
alcun possibile DFA. 
\subsection{Esempi di DFA}
Vediamo in questa sezione alcuni semplici esempi di DFA.\acc 
\textbf{Esempio 1)} Si vuole progettare un DFA che accetti il seguente linguaggio 
$$ \{x\in\{0,1\}^* \;|\;w_h(x)\ge 3 \}$$
Si ricordi come 
$$ w_h(x) = \text{ occorrenze di 1 in }x$$
\begin{figure}[h!]
    \centering
    \begin{tikzpicture} [node distance = 2cm, on grid, auto]
        \node (q0)[state, left, initial] {$q_0$};
        \node (q1)[state, right = of q0] {$q_1$};
        \node (q2)[state, right = of q1] {$q_2$};
        \node (q3)[state, right = of q2, accepting] {$q_3$};
        \path [-stealth, thick]
        (q0) edge [loop above]  node {0}()
        (q1) edge [loop above]  node {0}()
        (q2) edge [loop above]  node {0}()
        (q3) edge [loop right]  node {1,0}()
        (q0) edge [bend left] node {1}   (q1)
        (q1) edge [bend left] node {1}   (q2)
        (q2) edge [bend left] node {1}   (q3);
        \end{tikzpicture}
        \caption{Esempio (1) di DFA}
        \label{fig:DFAExample1}
\end{figure}
Una volta progettato il DFA, è anche importante dimostrarne la correttezza, ossia dare una prova 
matematica che l'automa in questione accetti il linguaggio.\begin{itemize}
    \item Se $x\in L(D)$ allora $D$ accetta $x$
    \item Se $D$ accetta $x$ allora $w_h(x)\ge 3$
\end{itemize}
In questo, e nei seguenti casi, essendo i DFA estremamente semplici, risulta ovvio che 
accettino il dato linguaggio, in casi più avanzati, sarà necessario fornire una dimostrazione 
rigorosa.\acc 
\textbf{Esempio 2)} Si vuole progettare un DFA che accetti il seguente linguaggio 
$$ \{x\in\{0,1\}^* \;|\;x=1y \land y \in  \{0,1\}^*\}$$
Appunto sulla notazione : Se $a\in \Sigma^*$ e $b\in \Sigma^*$, allora con $ab$ si 
denota la concatenazione di stringhe.\acc
\begin{figure}[h!]
    \centering
    \begin{tikzpicture} [node distance = 2cm, on grid, auto]
        \node (q0)[state, left, initial] {$q_0$};
        \node (q1)[state, below = of q0] {$q_1$};
        \node (q2)[state, right = of q0, accepting] {$q_2$};
        \path [-stealth, thick]
        (q1) edge [loop left]  node {0,1}()
        (q2) edge [loop above]  node {0,1}()
        (q0) edge [bend left] node {0}   (q1)
        (q0) edge [bend left] node {1}   (q2);
        \end{tikzpicture}
        \caption{Esempio (2) di DFA}
        \label{fig:DFAExample2}
\end{figure}\acc 
Nell'esempio (2), quando dallo stato $q_0$ il DFA riceve in input $0$, la computazione cade su uno stato 
"buco nero", dalla quale non si può uscire a prescindere dall'input, l'operazione che fa 
cadere in questo stato è da considerarsi "non definita" in quanto non porterà mai la computazione 
a terminare su uno stato accettabile, è quindi comodo rimuovere tale stato dal diagramma.
\begin{figure}[h!]
    \centering
    \begin{tikzpicture} [node distance = 3cm, on grid, auto]
        \node (q0)[state, left, initial] {$q_0$};
        \node (q2)[state, right = of q0, accepting] {$q_2$};
        \path [-stealth, thick]
        (q2) edge [loop above]  node {0,1}()
        (q0) edge [bend left] node {1}   (q2);
        \end{tikzpicture}
        \caption{Esempio (2.1) di DFA}
        \label{fig:DFAExample2.1}
\end{figure}\acc 
Anche in questo caso la dimostrazione della correttezza risulta banale.\acc 
\textbf{Esempio 3)} Si vuole progettare un DFA che accetti il seguente linguaggio 
$$ \{x\in\{0,1\}^* \;|\;x=0^n1,\;\;n\in \N\}$$
Con $0^n1$ si intende una stringa che sia composta esclusivamente da $0$, ma con un $1$ come 
ultimo termine, ad esempio : $$0000000000000001$$\newpage
\begin{figure}[h!]
    \centering
    \begin{tikzpicture} [node distance = 3cm, on grid, auto]
        \node (q0)[state, left, initial] {$q_0$};
        \node (q1)[state, right = of q0] {$q_1$};
        \node (q2)[state, right = of q1, accepting] {$q_2$};
        \path [-stealth, thick]
        (q1) edge [loop above]  node {0}()
        (q0) edge  node {0}   (q1)
        (q1) edge  node {1}   (q2);
        \end{tikzpicture}
        \caption{Esempio (3) di DFA}
        \label{fig:DFAExampl3}
\end{figure}
\flowerLine 
\section{Operazioni sui Linguaggi}
Lo studio delle proprietà dei linguaggi regolari può fornire opportune accortezze
utili nella progettazione di automi, siccome i linguaggi sono insiemi di stringhe costruiti 
su un alfabeto $\Sigma$, essi godono delle operazioni insiemistiche.\acc
Risulta utile definire formalmente la concatenazione fra stringhe, siano 
$$ x = a_1,a_2\dots,a_n\;\;\;\;\;\;\;\;\;\;\;\;y = b_1,b_2\dots,b_n $$
due stringhe, esse possono essere concatenate 
$$xy=  a_1,a_2\dots,a_n,b_1,b_2\dots,b_n $$
L'operazione di concatenazione non è commutativa, può essere definita ricorsivamente in tal 
modo : 
$$ x(ya) = (xy)a$$ 
dove 
$$x,y\in \Sigma^*\;\;\;\;\;\;\;\;\;a\in \Sigma$$
Siano $L_1,L_2$ due 
linguaggi regolari in $REG$ (per semplicità, definiti su uno stesso alfabeto $\Sigma$), e 
sia $n$ un numero naturale, sono 
definite su di essi le seguenti operazioni : \begin{itemize}
    \item \textbf{unione} : $L_1\cup L_2 = \{x\in\Sigma^*\;|\;x\in L_1\lor x\in L_2\}$
    \item \textbf{intersezione} : $L_1\cap L_2 = \{x\in\Sigma^*\;|\;x\in L_1\land x\in L_2\}$
    \item \textbf{complemento} : $\lnot L_1=\{x\in\Sigma^*\;|\;x\notin L_1\}$
    \item \textbf{concatenazione} : $L_1 \circ L_2 = \{xy\;|\;x\in L_1\land y\in L_2\}$
    \item \textbf{potenza} : $L_1^n = \smash{\underbrace{L_1\circ L_1\circ L_1,\dots\circ L_1}_{n\ \text{volte}}}\vphantom{1}$\acc
    \item \textbf{star} : $L_1^* = \{x_1,x_2\dots,x_k\;|\; k\in \Z^+ \land x_i \in L_1\}$ \\ 
    Si può definire anche diversamente $$L_1^* = \bigcup_{k=0}^\infty L_1^k $$
\end{itemize}
\textit{Esempio di concatenazione e potenza} : 
$$\Sigma= \{a,b\}\;\;\;\; L_1 = \{a,ab,ba\}\;\;\;\; L_2 = \{ab,b\}\;\;\;\; L = \{a,ab,ba\}$$
$$L_1\circ L_2 = \{aab,ab,abab,abb,baab,bab\} $$
$$ L^2=\{aa,aab,aba,abab,abba,baa,baba\}$$
\teo{(Chiusura di $REG$)} La classe dei linguaggi regolari $REG$, è chiusa rispetto a tutte le 
operazioni appena elencate, siano $L_1$ ed $L_2$ due linguaggi regolari, allora :
$$\begin{matrix}
    L_1\cup L_2 \in REG && L_1\circ L_2 \in REG &&
    L_1\cap L_2 \in REG \\ \\ L_1^n \in REG &&
    \lnot L_1 \in REG && L_1^* \in REG
\end{matrix}$$
\dimo{(unione ed intersezione)} Siano $L_1$ ed $L_2$ due linguaggi regolari, considero due DFA, per semplicità, con lo stesso alfabeto
$$ 
D_1 = (Q_1,\Sigma,\delta,q_1,F_1)
$$
$$ 
D_2 = (Q_2,\Sigma,\delta,q_2,F_2)
$$
tali che 
$$ L(D_1)=L_1 \ \land \ L(D_2)=L_2$$
Si costruisce un DFA che simula contemporaneamente l'esecuzione di $D_1$ e $D_2$, in cui gli stati possibili saranno 
le possibili combinazioni di coppie di stati. Si definisce $D=(Q,\Sigma,\delta,q_0,F)$ tale che 
\begin{itemize}
    \item $Q=Q_1\times Q_2=\{(r_1,r_2) \|\ r_1\in Q_1 \land r_2\in Q_2\}$
    \item $\delta((r_1,r_2),a)=(\delta(r_1,a),\delta(r_2,a))$ dove $a\in\Sigma$ e $(r_1,r_2)\in Q$
    \item $q_0=(q_1,q_2)$
    \item $F=(F_1\times Q_2)\cup (Q_1\times F_2)=\{(r_1,r_2)\|\ r_1\in F_1\lor r_2\in F_2\}$
\end{itemize}
Nel caso si dovesse dimostrare la proprietà dell'intersezione, si avrebbe che \begin{itemize}
    \item $F=F_1\times F_2$
\end{itemize}
A questo punto risulta chiaro che 
\begin{enumerate}[(i)] 
    \item $x\in L_1\cup L_2\implies x\in L(D)$
    \item $x\in L(D)\implies x\in L_1\cup L_2\;\;\blacksquare$ 
\end{enumerate}
\dimo{(complemento)} : Sia $L$ un linguaggio regolare, e $D$ un automa che lo accetta $L(D)=L$. Si vuole dimostrare 
che esiste un automa che accetti $L^C = \{w\in \Sigma^* \ |\ w\notin L\}$. Essendo 
$$ D=(Q,\Sigma,\delta,q_0,F)$$
considero 
$$ D'=(Q,\Sigma,\delta,q_0,F^C)$$
dove  $F^C=Q\backslash F$. Supponiamo che $w\in L^C$, allora sicuramente, se data come input a $D'$, 
la computazione terminerà in uno stato che non è in $F$, dato che, per definizione, se terminasse in $F$, sarebbe 
accettato da $D$, ma $L^C$ contiene tutte le stringhe che non sono accettate da $D$, quindi\begin{itemize}
    \item $w\in L^C$
    \item  la computazione termina in uno stato $q\in F^C$ 
    \item $D'$ accetta $w$ 
    \item $L^C$ è un linguaggio regolare. $\blacksquare$ 
\end{itemize} 
Per dimostrare la proprietà di concatenazione, è necessario introdurre un nuovo concetto. 
\flowerLine 
\section{Non Determinismo}
Si può generalizzare il modello di DFA, in modo tale che la lettura di un input non scaturisca il passaggio da 
uno stato ad un'altro, ma da uno stato ad un insieme di stati, tale generalizzazione è 
detta \textit{Non-Deterministic Finite Automa}.\acc 
\defi{(NFA)}  Un NFA è una tupla $N=(Q,\Sigma,\delta,q_0,F)$ tale che \begin{itemize}
    \item $Q,\Sigma,q_0,F$ condividono la definizione con i loro corrispettivi nel DFA 
    \item $\delta : Q\times \Sigma_\epsilon\rightarrow\mathcal{P}(Q)$ \begin{itemize}
        \item $\Sigma_\epsilon = \Sigma \cup \{\epsilon\}$
        \item $\mathcal{P}(Q)$ è l'insieme delle parti di $Q$
    \end{itemize}
\end{itemize}
Una computazione in un NFA è paragonabile ad una computazione parallela, in cui un 
input può risultare in diversi \textit{rami} di computazione. Una funzione di transizione di un 
$NFA$ inoltre accetta la stringa vuota $\epsilon$, se la computazione finisce in uno stato in cui è 
presente un arco di questo tipo, verrà considerata anche una diramazione verso quell'arco a prescindere 
dall'input. \acc 
Una computazione si può rappresentare graficamente con un albero, se una delle diramazioni possibili termina in 
uno stato accettabile, allora la stringa in input è accettata. 
\begin{figure}[h!]
    \centering
    \begin{tikzpicture} [node distance = 2cm, on grid, auto]
        \node (q0)[state, left, initial] {$q_1$};
        \node (q1)[state, right = of q0] {$q_2$};
        \node (q2)[state, right = of q1] {$q_3$};
        \node (q3)[state, right = of q2, accepting] {$q_4$};
        \path [-stealth, thick]
        (q0) edge [loop above]  node {0,1}()
        (q0) edge [bend left] node {1}   (q1)
        (q1) edge [bend left] node {0,1}   (q2)
        (q2) edge [bend left] node {0,1}   (q3);
    \end{tikzpicture}
    \caption{Esempio di NFA}
    \label{fig:NFAExample1}
\end{figure}\acc
Sia $N$ l'NFA mostrato in figura \ref{fig:NFAExample1}, si ha 
$$ L(N)=\{x\in\{0,1\}^*\|\ x\text{ ha $1$ come terz'ultimo valore }\}$$
Si può visualizzare il seguente albero di computazione data come input una stringa $w$ :
$$w=10110$$\begin{center}
    \includegraphics[width=0.7\textwidth ]{images/alberoCalc.eps}
\end{center}
Essendo che la traccia di sinistra accetta $w$, allora $N$ accetta $w$.\acc 
È necessario estendere il concetto di \textit{configurazione} per gli NFA, essa, rappresentante 
uno stato della computazione, sarà una coppia 
$$ (q,x)\in Q\times \Sigma_\epsilon$$
E diremo che 
$$ (p,ax)\vdash_N (q,x)\iff q\in \delta(p,a)$$
dove 
$$p,q\in Q \ \ \ \ a\in \Sigma_\epsilon\ \ \ \ x\in \Sigma_\epsilon^*$$
Si considera ora la chiusura transitiva di $\vdash_N$, denotata $\vdash_N^*$, se $w$ è una stringa ed 
$N$ un NFA, si ha che 
$$ w\in L(N)\iff \exists q\in F\text{ tale che } (q_0,w)\vdash_N^* (q,\epsilon)$$
Consideriamo adesso l'unione di due NFA, risulta particolarmente semplice da definire, siano $N_1$ e $N_2$ due 
NFA, che per semplicità, condividono l'alfabeto 
$$ 
N_1=\{Q_1,\Sigma_\epsilon,\delta_1,q_1,F_1\}
$$$$ 
N_2=\{Q_2,\Sigma_\epsilon,\delta_2,q_2,F_2\}
$$
Definisco un nuovo NFA $N=(Q,\Sigma_\epsilon,\delta,q_0,F)$ tale che \begin{itemize}
    \item $Q=Q_1\cup Q_2$
    \item $F=F_1\cup F_2$
    \item Siano $q\in Q$ e $a\in\Sigma_\epsilon$ : 
    $$ 
        \delta(q,a)=\begin{cases}
            \delta_1(q,a) \text{ se }q\in Q_1\\
            \delta_2(q,a) \text{ se }q\in Q_2\\
            \{q_1,q_2\} \text{ se }q=q_0  \ \land\ a=\epsilon \\
            \emptyset \text{ se }q=q_0\ \land \ a\ne \epsilon 
        \end{cases}
    $$
\end{itemize}
Si avrà che $L(N)=L(N_1)\cup L(N_2)$\begin{center}
    \begin{figure}[h!]
        \centering 
        \includegraphics[width=0.6\textwidth ]{images/UnioneNFA.eps}
        \caption{Unione di due NFA}
        \label{fig:NFAUnion}
    \end{figure}
\end{center}
Denotiamo $\mathcal{L}(DFA)$ l'insieme dei linguaggi accettati da un qualsiasi DFA, che per definizione è $REG$, e denotiamo, in maniera analoga 
$\mathcal{L}(NFA)$. \acc 
\teo{}  L'insieme dei linguaggi accettati da un qualsiasi DFA, e quello dei linguaggi accettati 
da un qualsiasi NFA coincidono $$\mathcal{L}(DFA)=\mathcal{L}(NFA)=REG$$
\dimo{} Il caso $\mathcal{L}(DFA)\subseteq\mathcal{L}(NFA)$ è banale e non verrà dimostrato. Si vuole dimostrare che 
se $L$ è accettato da un generico NFA, allora esiste un DFA che accetta $L$, l'idea è quella di "simulare" un NFA 
tramite un DFA che rappresenti ogni possibile stato di computazione.



Sia $N=(Q_N,\Sigma,\delta_N,q_{0_N},F_N)$ un NFA, e sia $L=L(N)$. Si considera un DFA 
$D=(Q_D,\Sigma,\delta_D,q_{0_D},F_D)$ tale che \begin{itemize}
    \item $Q_D=\mathcal{P}(Q_N)$
    \item $q_{0_D}=\{q_{0_N}\}$
    \item $F_D=\{R\in Q_D | R\cap F_N\ne \emptyset\}$, ovvero, $D$ accetta tutti gli insiemi in cui compare almeno 
    un elemento accettato da $N$.
    \item Sia $R\in Q_D$ e $a\in\Sigma$, si definisce $\displaystyle\delta_D(R,a)=\bigcup_{r\in R}\delta_N(r,a)$
\end{itemize}
Questo caso non tiene conto di un NFA in cui sono presenti degli $\epsilon$-archi. Supponiamo che vi siano,  sia 
$R\in Q_D$, si definisce la funzione estesa $E$ definita come segue 
$$ E(R)=\{q\in Q_N \|\ q\text{ può essere raggiunto da un qualsiasi stato  }r\in R\text{ attraverso zero o 
più }\epsilon\text{-archi }\}$$
Cambia la definizione del DFA utilizzato per la dimostrazione \begin{itemize}
    \item $q_{0_D}=E(\{q_{0_N})\}$
    \item $\displaystyle\delta_D(R,a)=\bigcup_{r\in R}E(\delta_N(r,a))$
\end{itemize}
È chiaro che $D$ tiene traccia di tutte le possibili computazioni di $N$, ed accetta $L$, ossia $L(D)=L(N)$. $\blacksquare$\acc 
\textbf{Esempio} : Si consideri l'NFA $N$ definito come segue
\begin{figure}[h!]
    \centering
    \begin{tikzpicture} [node distance = 2cm, on grid, auto]
        \node (q0)[state,accepting, left, initial] {$q_1$};
        \node (q1)[state, below = of q0] {$q_2$};
        \node (q2)[state, right = of q1] {$q_3$};
        \path [-stealth, thick]
        (q1) edge [loop left]  node {$a$}()
        (q0) edge [bend right] node {$b$}   (q1)
        (q0) edge [bend right] node {$\epsilon$}   (q2)
        (q2) edge [bend right] node {$a$}   (q0)
        (q1) edge [bend right] node {$a,b$}   (q2);
    \end{tikzpicture}
    \caption{$N=(Q_N,\Sigma,\delta_N,q_{0_N},F_N)$}
    \label{fig:NFAExample2}
\end{figure}\acc
Si costruisce un DFA $D=(Q_D,\Sigma,\delta_D,q_{0_D},F_D)$ con le seguenti specifiche (per comodità, l'elemento $\{q_i,q_j\dots,q_k\}$ verrà 
denotato $p_{ij\dots k}$), mostrato in figura \ref{fig:DFAgenerated}
\begin{figure}[h!]
    \centering
    \begin{tikzpicture} [node distance = 3.5cm, on grid, auto]
        \node (p13)[state,accepting, initial above] {$p_{13}$};
        \node (p3)[state,left = of p13] {$p_{3}$};
        \node (p0)[state,above = of p3] {$p_{0}$};
        \node (p1)[state,above = of p13,accepting] {$p_{1}$};
        \node (p2)[state,right = of p1] {$p_{2}$};
        \node (p23)[state,right = of p13] {$p_{23}$};
        \node (p12)[state,right = of p2,accepting] {$p_{12}$};
        \node (p123)[state,right = of p23,accepting] {$p_{123}$};
        \path [-stealth, thick]
        (p0) edge [loop left]  node {$a,b$}()
        (p13) edge [loop left]  node {$a$}()
        (p123) edge [loop right]  node {$a$}()
        (p1) edge [bend left] node {$b$}   (p2)
        (p13) edge  node {$b$}   (p2)
        (p2) edge  node {$b$}   (p3)
        (p2) edge  node {$a$}   (p23)
        (p123) edge [bend left] node {$b$}   (p23)
        (p12) edge  node {$a,b$}   (p23)
        (p23) edge [bend left, below] node {$b$}   (p3)
        (p23) edge  node {$a$}   (p123)
        (p3) edge [bend left] node {$b$}   (p0)
        (p3) edge [bend right] node {$a$}   (p13)
        (p1) edge [bend right] node {$a$}   (p0);
    \end{tikzpicture}
    \caption{grafico di $D$}
    \label{fig:DFAgenerated}
\end{figure}
\begin{itemize}
    \item $Q_D=\{p_0,p_1,p_2,p_3,p_{12},p_{13},p_{23},p_{123}\}$
    \item $E(q_{0_N})=\{q_1,q_3\}\implies q_{0_D}=p_{13}$
    \item $F_D=\{p_1,p_{12},p_{13},p_{123}\}$
    \item la funzione $\delta_D$ si definisce osservando il grafico di $N$\begin{itemize}
        \item $\delta_N(q_2,a)=\{q_2,q_3\}\implies \delta_D(p_2,a)=p_{23}$
        \item $\delta_N(q_2,b)=\{q_3\}\implies \delta_D(p_2,a)=p_{3}$
        \item $\delta_N(q_1,a)=\emptyset\implies \delta_D(p_1,a)=p_0$
        \item $\delta_N(q_1,b)=\{q_2\}\implies \delta_D(p_1,b)=p_{2}$
        \item etc...
    \end{itemize}
\end{itemize}

L'introduzione degli automi non deterministici è stata necessaria in principio per la dimostrazione della chiusura 
di $REG$ rispetto le operazioni di concatenazione e star.\acc 
\teo{} $REG$ è chiusa per concatenazione.\acc 
\dimo{} Siano $L_1$ ed $L_2$ due linguaggi regolari, esistono quindi due NFA 
$$ N_1=(Q_1,\Sigma_\epsilon,\delta_1,q_0^1,F_1)\ \ \ \ \ \  
N_2=(Q_2,\Sigma_\epsilon,\delta_2,q_0^2,F_2)$$
tali che $L(N_1)=L_1 \ \land \ L(N_2)=L_2$. Si costruisce un NFA $N=(Q,\Sigma_\epsilon,\delta,q_0,F)$, l'idea è quella 
di concatenare le ramificazioni di $N_1$ ad $N_2$.
\begin{center}
    \begin{figure}[h!]
        \centering 
        \includegraphics[width=1\textwidth ]{images/concatenazioneChiusa.eps}
        \caption{schema di $N$}
        \label{fig:concatenazioneChiusa}
    \end{figure}
\end{center}
Un NFA di questo tipo computerà una stringa in $L_1$, se finirà in uno stato di $F_1$, andrà nello stato iniziale 
di $N_2$, è chiaro che l'automa accetta solamente una concatenazione di stringhe fra $L_1$ ed $L_2$. \begin{itemize}
    \item $Q=Q_1\cup Q_2$
    \item $q_0=q_0^1$
    \item $F=F_2$
    \item per $a\in\Sigma_\epsilon$ e $q\in Q$ si ha $\delta(q,a)=\begin{cases}
        \delta_1(q,a) \text{ se }q\in Q_1\land q\notin F_1\\ 
        \delta_1(q,a)\text{ se }q\in F_1\land a\ne \epsilon\\ 
        \delta_1(q,a)\cup\{q_0^2\}\text{ se }q\in F_1\land a= \epsilon\\ 
        \delta_2(q,a)\text{ se }q\in Q_2\\
    \end{cases}$
\end{itemize}
Si ha quindi che $L(N)=L_1\circ L_2$. $\blacksquare$\acc 
\teo{} $REG$ è chiusa per star.\acc 
\dimo{} Sia $L\in REG$ e sia $N=(Q,\Sigma_\epsilon,\delta,q_{0},F)$ un NFA tale che $L(N)=L$. Considero un NFA $N^*=(Q^*,\Sigma_\epsilon,\delta^*,q_{0}^*,F^*)$,
 identico ad $N$, con opportune 
modifiche, lo stato $q_0$ iniziale di $N$ non è iniziale in $N^*$, 
ed ogni stato finale ha una $\epsilon$-arco verso $q_0$.\begin{itemize}
    \item $Q^*=Q\cup\{q_0^*\}$
    \item $q_0^*$ è un nuovo stato 
    \item $F^*=F\cup\{q_0^*\}$ questo perché in $L^*$ è presente la stringa vuota
    \item per $a\in\Sigma_\epsilon$ e $q\in Q^*$ si ha $\delta^*(q,a)=\begin{cases}
        \delta(q,a) \text{ se }q\in Q\land q\notin F\\ 
        \delta(q,a) \text{ se }q\in F\land a\ne \epsilon\\
        \delta(q,a)\cup\{q_0\} \text{ se }q\in F\land a= \epsilon\\
        \{q_0\}\text{ se }q=q_0^*\land a =\epsilon\\ 
        \emptyset \text{ se }q=q_0^*\land a\ne \epsilon
    \end{cases}$
\end{itemize}
\begin{center}
    \begin{figure}[h!]
        \centering 
        \includegraphics[width=1\textwidth ]{images/starChiusa.eps}
        \caption{schema di $N^*$}
        \label{fig:starChiusa}
    \end{figure}
\end{center}
\flowerLine 
\section{Espressioni Regolari}
 Un espressione regolare è simile ad un espressione algebrica ma opera sulle stringhe, dato un alfabeto, un'espressione 
 su tale alfabeto rappresenta un insieme di stringhe, un esempio è 
 $$(0|1)0^*$$ 
 Dove $(0|1)\equiv \{0\}\cup \{1\}=\{0,1\}$ e $0^*\equiv\{0\}^*$ quindi $(0|1)0^*\equiv \{0,1\}\circ \{0\}^*$.\acc 
 \defi{(espressione regolare)}  Sia $\Sigma$ un alfabeto, un espressione regolare $r$ su $\Sigma$, denotata 
 $r\in re(\Sigma)$, è definita per induzione \\ 
 \textbf{Caso base}\begin{itemize}
    \item $r=\emptyset\in re(\Sigma)$
    \item $r=\epsilon\in re(\Sigma)$
    \item $r=a\in re(\Sigma)$ dove $a\in\Sigma$
 \end{itemize}
 \textbf{Caso induttivo}\begin{itemize}
    \item $r=r_1\cup r_2$ dove $r_1,r_2\in re(\Sigma)$
    \item $r=r_1\circ r_2$ dove $r_1,r_2\in re(\Sigma)$
    \item $r=r_1^*$ dove $r_1\in re(\Sigma)$
 \end{itemize}
L'insieme delle stringhe definite da  $r\in re(\Sigma)$ è il \textit{linguaggio} di $r$ ed è denotato $L(r)$.\acc 
\textbf{Esempio : } Sia $\Sigma = \{0,1\}$\begin{itemize}
    \item $0^*10^*=\{w\ | \ w \text{ ha esattamente un }1\}$
    \item $\Sigma^*1\Sigma^*=\{w\ | \ w \text{ ha almeno un }1\}$
    \item $\Sigma^*001\Sigma^*=\{w\ | \ w \text{ ha la sottostringa }001\}$
\end{itemize}
Per convenzione si definisce $$1^*\emptyset = \emptyset \ \ \ \ \ \ \emptyset^*=\epsilon $$
\teo{ Fondamentale} Sia $\mathcal{L}(DFA)=REG$ l'insieme dei linguaggi accettati da un qualsiasi DFA, e sia 
$\mathcal{L}(re)$   l'insieme dei linguaggi accettati da una qualsiasi espressione regolare, è vero che 
$$\mathcal{L}(re)=\mathcal{L}(DFA)=REG $$
\dimo{} è necessario dimostrare due direzioni\acc
\boxedMath{$\mathcal{L}(re)\subseteq\mathcal{L}(DFA)$} Sia $r$ un espressione regolare, si considera un DFA 
$D_r$ definito come segue, a seconda dei casi\acc 
\textbf{Caso base }\begin{itemize}
    \item $r=\emptyset\implies D_r$ non accetta alcuna stringa 
    \item $r=\epsilon\implies D_r$  accetta la stringa vuota
    \item $r=a\implies D_r$  accetta  $a\in\Sigma$
\end{itemize}
\textbf{Caso induttivo}\begin{itemize}
    \item $r=r_1\cup r_2$, esistono due automi $D_{r_1}$ e  $D_{r_2}$ che accettano rispettivamente $L(r_1)$ 
    e $L(r_2)$, ma allora esiste necessariamente un automa $D_r$ che accetta $L(r_1)\cup L(r_2)$.
    \item $r=r_1\circ r_2$,  esistono due automi $D_{r_1}$ e  $D_{r_2}$ che accettano rispettivamente $L(r_1)$ 
    e $L(r_2)$, ma allora esiste necessariamente un automa $D_r$ che accetta $L(r_1)\circ L(r_2)$.
    \item $r=r_1^*$, esiste un automa $D_{r_1}$  che accetta
     $L(r_1^*)$, ma allora esiste necessariamente un automa $D_r$ che accetta $L(r_1)$.
\end{itemize}
Tali tesi sono vere dato che la classe dei linguaggi regolari è chiusa per le operazioni di star, concatenazione 
ed unione. $\square $\acc 
\boxedMath{$\mathcal{L}(DFA)\subseteq\mathcal{L}(re)$} Sia $L$ un linguaggio regolare, e sia $N$ l'NFA tale che 
$L(N)=L$. Si costruisce un nuovo tipo di NFA che sarà equivalente ad $N$. Tale automa è detto \textit{GNFA}, 
dove la G sta per "Generalizzato", tale automa ha una \textit{forma canonica}, ossia, rispetta le seguenti 
proprietà \begin{itemize}
    \item Lo stato iniziale, ha solo archi uscenti 
    \item Vi è un singolo stato finale, ed ha solo archi entranti
    \item Per ogni coppia di stati (non necessariamente distinti), c'è esattamente un arco.
    \item Ogni arco è etichettato da un espressione regolare.
\end{itemize}
\begin{center}
    \begin{figure}[h!]
        \centering 
        \includegraphics[width=0.4\textwidth ]{images/GNFA.eps}
        \caption{forma di un GNFA}
        \label{fig:GNFA}
    \end{figure}
\end{center}
Più precisamente, sia $G$ un GNFA definito $G=(Q,\Sigma,\delta,q_{start},q_{acc})$ dove 
$$ \delta : Q\backslash \{q_{acc}\} \times Q\backslash \{q_{start}\}\rightarrow re(\Sigma)$$
Dato un generico NFA, è possibile trasformarlo in un GNFA aggiungendo al più due stati (iniziale e 
finale), ed utilizzando gli $\epsilon$-archi per riempire le coppie di stati che non sono collegate.\acc 
La funzione $Convert : GNFA \rightarrow GNFA$ modifica un GNFA restituendone uno equivalente, ma con uno stato in meno. 
Tale funzione è definita in tal modo, sia $k$ il numero di stati/nodi di $G$, si esegue $Convert(G)$ \begin{itemize}
    \item Se $k=2$, allora esiste un solo arco fra questi etichettato con un espressione regolare $r$, la funzione 
    restituirà $r$. 
    \item Se $k> 2$, viene selezionato un qualsiasi nodo in $Q\backslash\{q_{start},q_{acc}\}$, sia questo $q_{rip}$, 
    si avrà $Convert(G)=G'=(Q\backslash\{Q_{rip}\},\Sigma,\delta',q_{start},q_{acc})$ dove 
    $$ \delta' : Q\backslash \{q_{acc},q_{rip}\} \times Q\backslash \{q_{start},q_{rip}\}\rightarrow re(\Sigma)$$
    Inoltre ogni etichetta di $G'$ viene aggiornata secondo la seguente procedura, siano $q_i\in Q\backslash \{q_{acc},q_{rip}\}$
    e  $q_j\in Q\backslash \{q_{start},q_{rip}\}$ due stati qualsiasi 
    $$ \delta'(q_i,q_j)=(r_1r_2^*r_3)|r_4$$
    Dove \begin{itemize}
        \item $r_1=\delta(q_i,q_{rip})$
        \item $r_2=\delta(q_{rip},q_{rip})$
        \item $r_3=\delta(q_{rip},q_j)$
        \item $r_4=\delta(q_i,q_j)$
    \end{itemize}
\end{itemize}
\begin{figure}[h!]
    \centering
    \begin{tikzpicture} [node distance = 2cm, on grid, auto]
        \node (qi)[state, left] {$q_i$};
        \node (qj)[state, right = of qi] {$q_j$};
        \node (qrip)[state, below = of qi] {$q_{rip}$};
        \path [-stealth, thick]
        (qrip) edge [loop below]  node {$r_2$}()
        (qi) edge [bend left] node {$r_4$}   (qj)
        (qi) edge  node {$r_1$}   (qrip)
        (qrip) edge [bend right] node {$r_3$}   (qj);
    \end{tikzpicture}
\end{figure}
Bisogna ora dimostrare che un generico GNFA $G$ è equivalente a $Convert(G)$. Si dimostra per induzione su 
$k$ numero di stati.\acc 
  \textbf{caso base} $k=2$ : In tal caso la procedura $Convert$ restituisce l'espressione regolare 
    $r$ sull'unico arco che descrive ogni stringa accettata da $G$. $L(r)\equiv L(G)$ \acc 
    \textbf{passo induttivo} : si assume che $G$ è equivalente a $Convert(G)$ per $k-1$ stati.\begin{itemize}
        \item Se $G$ accetta $w$, allora esiste un ramo di computazione $C=\{q_{start},q_1\dots, q_{accept}\}$, se 
        $q_{rip}$ che è stato rimosso in $G'=Convert(G)$ non appartiene a $C$, allora la computazione non è 
        alterata e $G'$ accetta $w$, altrimenti ci sarà una differente sequenza di stati, ma gli stati $q_i,q_j$ adiacenti 
        a $q_{rip}$ sono ora uniti da un arco etichettato da un espressione regolare che comprende le stringhe per 
        andare da $q_i$ a $q_j$ passando per $q_{rip}$.
        \item Se $G'$ accetta $w$, anche $G$ lo accetta dato che per ogni coppia di stati in $C$ si è aggiornata 
        l'etichetta tenendo conto della transazione che porta da uno stato all'altro passando per $q_{rip}$.
    \end{itemize}
Quindi $Convert$ restituisce un automa equivalente con $k-1$ stati, quindi l'asserto è vero. $\blacksquare$\newpage
\subsection{Esempi} 
\textbf{Esempio 1)} Si trasformi $r=(ab|a)^*$ in un NFA.\begin{center}
    \begin{figure}[h!]
        \centering 
        \includegraphics[width=1\textwidth ]{images/esempio1Regex.eps}
        \caption{Esempio 1}
        \label{fig:es1Regex}
    \end{figure}
\end{center}\newpage
\textbf{Esempio 2)} Dato il seguente automa, si trovi l'espressione regolare associata 
\begin{center}
    \begin{figure}[h!]
        \centering 
        \includegraphics[width=1\textwidth ]{images/esempio2Regex.eps}
        \caption{Esempio 2}
        \label{fig:es2Regex}
    \end{figure}
\end{center}\flowerLine \newpage 
\section{Linguaggi non regolari}
\subsection{Il Pumping Lemma per i Linguaggi Regolari}
A questo punto della lettura, è naturale porsi una domanda : Tutti i linguaggi sono regolari? Esistono linguaggi che 
non possono essere accettati da alcun DFA? Nel caso solamente un sottoinsieme dei linguaggi fosse regolare, quali proprietà 
soddisfa? Si consideri il seguente linguaggio 
$$ L=\{0^n1^n \ | \ n\ge 0\}$$
Si può provare a disegnare un automa che accetti $L$, rendendosi ben presto conto che è \textit{impossibile}, $L$ non è 
regolare, esistono quindi dei linguaggi che non sono regolari. L'automa a stati finiti è un modello semplice, non può 
"ricordare" quanti caratteri di un certo tipo sono stati letti. \acc 
Essendo che solamente alcune stringhe possono essere accettate da un qualsiasi automa, è importante caratterizzare tali 
stringhe e definirne le proprietà in tal merito. \acc 
Appunto sulla notazione : Se $w$ è una stringa, allora $|w|$ è il numero dei suoi caratteri.\acc
\textbf{Osservzione} : Se un DFA con $n$ stati legge una stringa di $k>n$ caratteri, allora ci sarà almeno uno stato 
che verrà considerato due volte durante la computazione. \acc 
\teo{(Pumping Lemma)} Sia $L$ un linguaggio regolare, sia $D$ l'automa tale che $L(D)=L$, si considera una stringa 
$w\in L(D)$, ed una sua decomposizione in 3 stringhe concatenate $w=xyz$. Esiste un intero $p\le|w|$, denotato \textit{pumping}
 tale che \begin{enumerate}
    \item $\forall i\ge0$, $xy^iz\in L(D)$
    \item $|y|>0$
    \item $|xy|\le p$
\end{enumerate}
\dimo{} Sia $D=(Q,\Sigma,\delta,q_{start},F)$ un automa, e sia $p=|Q|$. Sia $w$ una stringa su $\Sigma$ di $n\ge p$ caratteri definita
$w=w_1w_2\dots w_n$. Sia $\{r_1,r_2\dots,r_{n+1}\}$ la sequenza di stati che $D$ computa su input $w$, ossia 
$$ \delta(r_i,w_i)=r_{i+1}$$
Tale sequenza è lunga $n+1\ge p+1$ stati, fra i primi $p+1$ elementi c'è necessariamente uno stato ripetuto, sia 
 $r_j$ la prima occorrenza di tale stato, e sia $r_l$ la seconda.\acc 
 Siccome la ripetizione avviene fra le prime $p+1$ computazioni, si ha che $l\le p+1$. Si consideri la seguente 
 scomposizione di $w$\begin{itemize}
    \item $x=w_1,w_2\dots,w_{j-1}$
    \item $y=w_j,w_{j+1}\dots,w_{l-1}$
    \item $z=w_l,w_{l+1}\dots,w_n$
 \end{itemize}\begin{enumerate}
    \item $xy^iz\in L(D)$ perché $x$ parte da $r_1=q_{start}$ e arriva a $r_j$, $y^i$ parte da $r_j$ e ritorna su $r_l$, che è 
    lo stesso stato, e $z$ porta da $r_l$ allo stato finale di accettazione.
    \item Essendo che $j<l$, allora la dimensione minima di $y$ è 1, in quanto il valore minimo che può assumere $l$ è $j+1$, ne consegue che $|y|>0$. 
    \item $l\le p+1$ ovvero $l-1=|xy|\le p$.
 \end{enumerate}
 I tre punti sono dimostrati. $\blacksquare$    \acc
 \prop{} Se $L$ è un linguaggio regolare, ed $L^*$ un sottoinsieme di 
 $L$, allora $L^*$ non è necessariamente regolare.\acc 
 Alcuni esercizi al seguente link : 
 \color{blue}\href{https://github.com/CasuFrost/University_notes/blob/main/Terzo%20Anno/Automi%2C%20Calcolabilit%C3%A0%20e%20Complessit%C3%A0/EsLinguaggiRegolari.pdf}{Esercizi Linguaggi Regolari}\color{black}.
 \flowerLine 
 \section{Grammatiche Acontestuali}
 Lo scopo delle grammatiche acontestuali è quello di estendere  
 l'automa a stati finiti per ottenere un modello di computazione più 
 potente. Tale automa al quale corrispondono le gramamtiche è 
 detto \textit{PDA}. Le grammatiche hanno applicazioni fondamentali, 
 nei linguaggi di programmazione, precisamente, nel funzionamento 
 dei compilatori.\acc 
 Una definizione informale può essere la seguente : Una grammatica è 
composta da un insieme di \textit{regole} su un alfabeto e delle variabili, tali 
regole sono annotate come segue $$\begin{cases}
    A\longrightarrow 0A1\\ 
    A \longrightarrow B \\
    B \longrightarrow \#
\end{cases}\ \ \ \ \Sigma = \{0,1,\#\} $$
Ciascuna regola contiene una variabile alla quale viene associata una stringa, 
composta da variabili e \textit{terminali}, ossia i caratteri dell'alfabeto $\Sigma$.
Una variabile è considerata iniziale, e per convenzione, è sempre quella 
presente nella prima regola.\acc 
Precisamente, una grammatica può generare stringhe\begin{enumerate}
    \item Si scrive la variabile iniziale 
    \item Si sostituisce applicando una delle regole 
    \item Si ripete ricorsivamente il procedimento finché la stringa contiene 
    solo terminali.
\end{enumerate}
\textbf{Esempio} : Considerando la grammatica con le regole prima elencate :\begin{enumerate}
    \item $A$ si applica $A\longrightarrow 0A1$
    \item $0A1$ si applica $A\longrightarrow 0A1$ 
    \item $00A11$ si applica $A\longrightarrow 0A1$ 
    \item $000A111$ si applica $A\longrightarrow B$
    \item $000B111$ si applica $B\longrightarrow \#$
    \item $000\#111$
\end{enumerate}
Applicando le regole secondo un ordine arbitrario, è possibile 
generare qualsiasi stringa del tipo $0^n\#1^n$, ad una grammatica quindi 
corrisponde un linguaggio. Una 'computazione' di una grammatica può 
essere rappresentata con un albero sintattico, mostrato in figura \ref{fig:alsint}.\acc
\begin{figure}[h!]
    \centering 
    \includegraphics[width=0.2\textwidth ]{images/alberoSintattico.eps}
    \caption{Albero Sintattico}
    \label{fig:alsint}
\end{figure}\acc
Con una grammatica è anche possibile rappresentare una qualsiasi 
espressione algebrica 
$$ 
\begin{cases}
    E \longrightarrow E + E \\ 
    E \longrightarrow  E \times E \\ 
    E \longrightarrow (E)\\ 
    E \longrightarrow 0 \lor 1 \lor 2 \dots, \lor \ 9 
\end{cases} \ \ \ \ \Sigma=\{0,1,2,3\dots,9\}
$$
\defi{(Grammatica Acontestuale)} Una \textbf{CFG} (Context Free Grammar) 
è una tupla $G=(V,\Sigma, R, S)$ dove\begin{itemize}
    \item $V$ è un insieme di simboli dette variabili 
    \item $\Sigma$ è un insieme di simboli detti terminali 
    \item $V\cap \Sigma = \emptyset$
    \item $S\in V$ è la variabile iniziale 
    \item $R$ è un insieme di regole
\end{itemize}
Le regole $R$ possono essere rappresentate come una funzione 
$R : V \rightarrow (V\cup \Sigma)^*$, associa ad ogni variabile una 
stringa composta da terminali, variabili, o entrambe.\acc 
Sia $uAv$ una stringa tale che $A\in V, \ \ u,v\in \Sigma \cup V$, e sia 
$w\in \Sigma \cup V$ diremo che 
$uAv$ \textbf{produce} $uwv$, e denoteremo $uAv \Rightarrow  uwv$ se e solo se
 $$ A\longrightarrow w \in R$$
 Il simbolo $ \Rightarrow $ rappresenta una relazione su $\Sigma \cup V$.
 Si può considerare la sua chiusura transitiva $\Rightarrow^*$

 $$u\Rightarrow^* v \iff \exists \{u_1,u_2\dots,u_k\} \ | \  
 u\Rightarrow u_1 \Rightarrow u_2 \dots \Rightarrow u_k \Rightarrow v$$ 
Sia $G=(V,\Sigma, R, S)$ una CFG, $L(G)$ è il \textbf{linguaggio della grammatica}, 
definito come segue 
$$L(G)= \{w\in\Sigma^*\ | \ S\Rightarrow^* w\}$$
\subsubsection{Unione}
Si consideri il seguente insieme di grammatiche 
$$ \{G_i=(V_i,\Sigma_i,R_i,S_i)\}\ \ \ \ \ i\in\{1,2,3\dots,n\}$$
È possibile considerare \textit{l'unione delle grammatiche}, definita come segue
$$G=(\bigcup_i\{V_i\}\cup \{S\},\bigcup_i\Sigma_i,R,S) $$
Dove\begin{itemize}
    \item $S$ è una nuova variabile 
    \item $R=\{\bigcup_iR_i\}\cup\{S\rightarrow S_1\lor S\rightarrow S_2 \dots\lor S\rightarrow S_n\}$
\end{itemize}
\prop{} $L(G)=\bigcup_i L(G_i)$
\subsubsection{Ambiguità}
Una CFG soddisfa la proprietà di ambiguità se, una stessa stringa può essere generata seguendo 
sequenze di regole differenti. L'ambiguità di una grammatica può risultare problematica nelle applicazioni.\acc 
\defi{} Una stringa di una CFG ha una \textit{derivazione a sinistra} se può essere ottenuta 
sostituendo ad ogni passo di produzione (applicazione delle regole) la variabile che si trova più a sinistra. Una 
stringa è \textit{derivata ambiguamente} se ha 2 o più derivazioni a sinistra. Una CFG è ambigua se ha almeno una 
stringa derivata ambiguamente. 
\subsection{Forma Normale}
In questa sezione verrà definita una forma canonica per le grammatiche, tale forma è fondamentale, soprattutto nelle applicazioni, 
se una grammatica è in tale forma, è possibile stabilire un tetto minimo di operazioni da eseguire per ottenere una 
determinata stringa.\acc 
\defi{} Una CFG $G=(V,\Sigma, R, S)$ è in \textbf{forma normale Chomsky} ( o più comunemente, in forma normale) se ogni sua 
regola è della forma $$\begin{matrix}
    A\longrightarrow BC \\ 
    A \longrightarrow a
\end{matrix} $$
Dove \begin{itemize}
    \item $a\in\Sigma$
    \item $A,B,C \in V$
    \item $B\ne S, \ \ \ C\ne S$
    \item La regola $S\longrightarrow\epsilon$ è permessa.
\end{itemize}
La variabile iniziale $S$ non può mai essere nel termine destro di una regola.\acc 
\teo{} Per ogni CFG, esiste una CFG equivalente (che genera lo stesso linguaggio) in forma normale.\acc 
\dimo{} La dimostrazione consiste in una procedura, in cui vengono applicate delle trasformazioni alle regole di una 
generica CFG in modo che soddisfino la forma normale. Sia $G=(V,\Sigma, R, S)$ una CFG, non 
necessariamente in forma normale\begin{itemize}
    \item Si definisce una nuova variabile $S_0$, essa sarà considerata la nuova variabile iniziale della grammatica 
    $G$, e verrà aggiunta la regola $S_0\rightarrow S$, ciò garantisce che la variabile iniziale non compare mai 
    come termine destro. 
    \item Data una $\epsilon$-regola, ossia le regole della forma 
    $$ A\longrightarrow \epsilon \ \ \ \ \ \ A\in V$$
    Si considera ogni occorrenza di $A$ nel termine destro di una regola, e si definisce una nuova regola 
    identica, dove $A$ è assente. Infine, $A\longrightarrow \epsilon$ viene rimossa dalle regole 
    $$ \begin{cases}
        A\longrightarrow \epsilon \\ 
        B\longrightarrow xAyA
    \end{cases} \Longrightarrow  \text{(diventa)}\begin{cases}
        B\longrightarrow xAyA\\ 
        B\longrightarrow xyA \text{ (aggiunta)}\\ 
        B\longrightarrow xAy \text{ (aggiunta)}
    \end{cases}\ \ \ \ \ \ \begin{matrix}
        B\in V\\ x,y\in (\Sigma\cup V)^*
    \end{matrix}$$
    \item Si considerano poi tutte le regole unitarie, ossia del tipo $$ A\longrightarrow B$$ con $B\in V$, una 
    volta rimossa, per ogni altra regola $B\rightarrow u$, si aggiunge la regola $A\rightarrow u$, con 
    $u\in(\Sigma\cup V)^*$, ripetendo ricorsivamente il procedimento. 
    \item Infine si considerano le regole del tipo 
    $$A\longrightarrow u_1u_2\dots,u_k \ \ \ \ \begin{matrix}u_i\in(\Sigma\cup V)\\k\ge 3 \end{matrix}$$
    Tale regola viene rimossa, e vengono spezzate in un set di regole come segue $$\begin{cases}
        A\longrightarrow u_1A_1 \ \ \ \ \text{ nuova variabile $A_1$ aggiunta a $V$}\\ 
        A_1\longrightarrow u_2A_2 \ \ \ \ \text{ nuova variabile $A_2$ aggiunta a $V$}\\ 
        A_2\longrightarrow u_3A_3 \ \ \ \ \text{ nuova variabile $A_3$ aggiunta a $V$}\\ 
        \vdots \\ 
        A_{k-2}\longrightarrow u_{k-1}u_k \ \ \ \ \text{ nuova variabile $A_{k-2}$ aggiunta a $V$}\\ 
    \end{cases} $$
    Una volta fatto ciò, per ogni regola $A_i\longrightarrow u_jA_j$ in cui $u_j$ è un 
    terminale, si definisce una nuova variabile $U_j$ ed una 
    nuova regola $U_j\longrightarrow u_j$. Ad esempio, se $u_1$ è un terminale, viene rimossa $A\longrightarrow u_1A_1$, 
    e vengono definite le nuove regole $$\begin{cases}
        A\longrightarrow U_1A_1\\ 
        U_1\longrightarrow u_1
    \end{cases}$$ 
\end{itemize}
Una volta eseguite le procedure, la nuova CFG sarà equivalente a quella originale, e sarà in forma normale Chomsky. $\blacksquare$ 
\subsubsection{Esempio}
Si consideri la seguente grammatica $G$ $$\begin{cases}
    S\longrightarrow ASA\lor aB \\ 
    A \longrightarrow A\lor S  \\ 
    B \longrightarrow b\lor \epsilon
\end{cases} \text{ si può scrivere anche  } \ \begin{cases}
    S\longrightarrow ASA | aB \\ 
    A \longrightarrow B| S  \\ 
    B \longrightarrow b| \epsilon
\end{cases} $$\begin{itemize}
    \item Passo 1 : variabile iniziale
    $$ \begin{cases}
        S\longrightarrow ASA | aB \\ 
        A \longrightarrow B| S  \\ 
        B \longrightarrow b| \epsilon
    \end{cases} \Longrightarrow \begin{cases}
        S_0 \longrightarrow S\\
        S\longrightarrow ASA | aB \\ 
        A \longrightarrow B| S  \\ 
        B \longrightarrow b| \epsilon
    \end{cases} $$
    \item Passo 2 : $\epsilon$-regole $$ 
    \begin{cases}
        S_0 \longrightarrow S\\
        S\longrightarrow ASA | aB \\ 
        A \longrightarrow B| S  \\ 
        B \longrightarrow b| \epsilon
    \end{cases}\Longrightarrow
    \begin{cases}
        S_0 \longrightarrow S\\
        S\longrightarrow ASA |  aB  \\ 
        A \longrightarrow B| S  | \epsilon \\ 
        B \longrightarrow b
    \end{cases}
    $$
    $$\begin{cases}
        S_0 \longrightarrow S\\
        S\longrightarrow ASA |  aB  \\ 
        A \longrightarrow B| S  | \epsilon \\ 
        B \longrightarrow b
    \end{cases}\Longrightarrow 
    \begin{cases}
        S_0 \longrightarrow S\\
        S\longrightarrow ASA |  aB | SA |AS | S  \\ 
        A \longrightarrow B| S \\ 
        B \longrightarrow b
    \end{cases}$$
\item passo 3 : regole unitarie
\\ si rimuove $S_0\longrightarrow S$
$$\begin{cases}
    S_0 \longrightarrow S\\
    S\longrightarrow ASA |  aB | SA |AS | S  \\ 
    A \longrightarrow B| S \\ 
    B \longrightarrow b
\end{cases}\Longrightarrow 
\begin{cases}
    S_0 \longrightarrow ASA |  aB | SA |AS \\
    S\longrightarrow ASA |  aB | SA |AS   \\ 
    A \longrightarrow B| S \\ 
    B \longrightarrow b
\end{cases}$$
si rimuovono $A\longrightarrow B$ e $A\longrightarrow S$
$$ 
\begin{cases}
    S_0 \longrightarrow ASA |  aB | SA |AS \\
    S\longrightarrow ASA |  aB | SA |AS   \\ 
    A \longrightarrow B| S \\ 
    B \longrightarrow b
\end{cases}
\Longrightarrow 
\begin{cases}
    S_0 \longrightarrow ASA |  aB | SA |AS \\
    S\longrightarrow ASA |  aB | SA |AS   \\ 
    A \longrightarrow b \\
    A\longrightarrow ASA |  aB | SA |AS \\ 
    B \longrightarrow b
\end{cases}
$$
\item passo 4 : convertire le ultime regole nella forma corretta $$
\begin{cases}
    S_0 \longrightarrow ASA |  aB | SA |AS \\
    S\longrightarrow ASA |  aB | SA |AS   \\ 
    A \longrightarrow b \\
    A\longrightarrow ASA |  aB | SA |AS \\ 
    B \longrightarrow b
\end{cases}\Longrightarrow 
\begin{cases}
    U\longrightarrow a\\
    S_0 \longrightarrow AA_1 |  UB | SA |AS \\
    A_1 \longrightarrow SA \\
    S\longrightarrow AA_1 |  UB | SA |AS   \\ 
    A \longrightarrow b \\
    A\longrightarrow AA_1 |  UB | SA |AS \\ 
    B \longrightarrow b
\end{cases} \ \ \ \ \ \text{variabili aggiunte : }A_1,\ U
$$
\end{itemize}
\flowerLine 
\section{Push Down Automata}
Si considera adesso un nuovo modello di computazione che estende il concetto di automa, vedremo in seguito 
che, i linguaggi regolari stanno agli NFA, come le grammatiche acontestuali stanno ai PDA. Introdurremo alcune 
caratteristiche dei PDA in modo informale, per poi darne la definizione.\acc 
Innanzitutto un PDA è sempre un automa, non deterministico, in particolare si differenzia dagli NFA/DFA per la 
presenza di una \textbf{pila} (di dimensione potenzialmente infinita) detta anche \textit{stack}, il cui contenuto evolve dinamicamente 
durante la computazione, tale pila permette all'automa di \textit{memorizzare}. L'accesso ad essa è limitato, 
esclusivamente LIFO, è possibile scrivere sulla pila una serie di caratteri appartenenti ad un determinato insieme, 
su di essa è possibile eseguire operazioni di \begin{itemize}
    \item pop 
    \item top 
    \item push
\end{itemize}
Se un NFA ha un solo alfabeto $\Sigma$, un PDA avrà due alfabeti, $\Sigma$ e $\Gamma$, denotati \textit{input} e 
\textit{pila}. Anche la funzione $\delta$ sarà differente, infatti gli archi saranno marcati nel seguente modo 
\begin{center}
    \begin{tikzpicture} [node distance = 6cm, on grid, auto]
        \node (q1)[state, left] {$q_1$};
        \node (q2)[state, right = of aperta] {$q_2$};
        \path [-stealth, thick]
        (q1) edge  node {$a;b\rightarrow c$}   (q2);
    \end{tikzpicture}
\end{center} 
Dove $b$ e $c$ sono elementi di $\Gamma_{\epsilon}=\Gamma\cup\{\epsilon\}$, ed $a$ è un elemento di $\Sigma_{\epsilon}$.
In particolare, la freccia in figura indica che : \begin{quote}
    Se l'automa è nello stato $q_1$, legge in input $a$, ed in cima alla pila 
    si trova $b$, allora si sposterà nello stato $q_2$, verrà rimosso $b$ dalla pila 
    e verrà aggiunto $c$
\end{quote}
Il dominio della funzione di transizione sarà quindi $Q\times\Sigma_\epsilon\times \Gamma_\epsilon$, mentre il codominio 
sarà l'insieme delle parti di $Q\times\Gamma_\epsilon$. Essendo non deterministico, ad ogni ramo di computazione 
sarà associata una differente evoluzione della pila.\acc 
\defi{(PDA)} Un \textit{Push Down Automata} è una tupla $P=(Q,\Sigma,\Gamma,\delta,q_0,F)$ dove \begin{itemize}
    \item $Q,\Sigma,q_0,F$ sono definiti identicamente agli NFA/DFA 
    \item $\Gamma$ è un alfabeto finito rappresentante gli elementi che possono essere nello stack 
    \item $\delta : Q\times\Gamma_\epsilon\times\Sigma_\epsilon\rightarrow\mathcal{P}(Q\times\Gamma_\epsilon)$
\end{itemize}
Considerato un PDA, supponiamo che 
$$ (q,c)\in\delta(p,a,b) \ \ \ \text{ con } \ \ \  
\begin{cases}
    q,p\in Q \\ a\in \Sigma_\epsilon \\ c,b \in \Gamma_\epsilon
\end{cases} $$
Allora, a seconda dei valori di $a,b$ e $c$, lo step di computazione assume il seguente significato\begin{itemize}
    \item $a,b,c\ne\epsilon\implies$ il PDA legge $a$, passa dallo stato $p$ allo stato $q$ e sostituisce 
    $b$ (che si trova nel top dello stack) con $c$. 
    \item $a,c\ne\epsilon\land b=\epsilon\implies $ il PDA legge $a$, passa dallo stato $p$ allo stato $q$ 
    indipendentemente dal valore del top dello stack, su cui andrà ad inserire $c$ 
    \item $a,b\ne\epsilon\land c=\epsilon\implies $ il PDA legge $a$, passa dallo stato $p$ allo stato $q$ ed 
    eseguo una pop sullo stack rimuovendo $b$.
\end{itemize}
Passiamo alla descrizione di come viene \textit{eseguita la computazione} su un PDA, si consideri una stringa $w$ in input, 
composta dai caratteri $$w=w_1\dots w_n \text{ con } w_i\in\Sigma$$ consideriamo ora gli stati che 
verranno attraversati nella computazione : $$r_0,r_1,r_2\dots, r_{n}$$
e si considerino le stringhe 
$$ s_0,s_2\dots, s_n \text{ con } s_i\in\Gamma^*$$
Lo stato $r_0=q_0$ sarà lo stato iniziale, ed $s_0=\epsilon$ rappresenterà la pila vuota, inoltre, 
$\forall i = 0,1\dots, n$ si avrà che  
$$ (r_{i+1},a)\in\delta(r_i,w_{i+1},b)$$
Dove $s_i$ rappresenterà lo stack nel momento in cui si è nello stato $r_i$, ed $s_{i+1}$
rappresenterà lo stack nel momento in cui si è nello stato $r_{i+1}$, si avrà infatti che \begin{itemize}
    \item $s_i = bT$ 
    \item $s_{i+1}= aT$ 
    \item $T\in \Gamma_\epsilon^*$
    \item In breve, nello stack è stata sostituito il simbolo $b$ con $a$
\end{itemize}
La computazione sarà andata a buon fine se e solo se $r_n\in F$, ossia lo stato finale è di accettazione.\begin{center}
    \begin{figure}[h!]
        \centering 
        \includegraphics[width=0.85\textwidth ]{images/pda1.eps}
        \caption{Lo stato verde rappresenta la computazione corrente}
        \label{fig:pdaComp}
    \end{figure}
\end{center}
Gli stati consistenti nella computazione sono in relazione 
$$ (p,ax,by)\vdash (q,x,cy)\iff (q,c)\in\delta(p,a,b)$$
$$ p,q\in Q\  \ b,c\in\Gamma_\epsilon\ \ a\in\Sigma_\epsilon\ \ x\in\Sigma^* \ \ y\in\Gamma^*$$
La chiusura transitiva $\vdash^*$ definisce la relazione di transizione estesa, utile per la seguente 
definizione.\acc 
\defi{} Sia $P=(Q,\Sigma,\Gamma,\delta,q_0,F)$ un PDA, definiamo \textbf{linguaggio di }$P$, e denotiamo $L(P)$, 
l'insieme $$L(P)=\{w\in\Sigma^* \ | \ (q_0,w,\epsilon)\vdash^* (q,\epsilon,y) \land q\in F\land y\in\Gamma^*\} $$
Si può definire in manire equivalente assumendo che la computazione termini sempre 
con lo stack vuoto 
$$L(P)=\{w\in\Sigma^* \ | \ (q_0,w,\epsilon)\vdash^* (q,\epsilon,\epsilon) \land q\in F\} $$
Il fatto che le due definizioni siano equivalenti, implica che ogni PDA può essere trasformato in un PDA equivalente 
in cui ogni stringa accettata termini la computazione con lo stack vuoto.
\subsection{Esempi}
\subsubsection{Esempio 1}
Sia $L=\{0^n1^n | n\ge 0\}$ un linguaggio, si vuole determinare un PDA $P$ tale che $L(P)=L$. \acc 
L'idea è quella di utilizzare lo stack, aggiungendo un valore ogni volta che si legge uno zero, e togliendone 
uno ogni volta che si legge un 1, se al termine la pila sarà vuota, allora la stringa letta apparterrà al 
linguaggio $L$. Definiamo $\Gamma=\{0,\$\}$, il simbolo $\$$ è utile per marcare il fondo dello stack, verrà 
aggiunto automaticamente all'inizio.
\begin{center}
    \begin{tikzpicture} [node distance = 3cm, on grid, auto]
        \node (q1)[initial, state, left] {$q_1$};
        \node (q2)[state, right = of q1] {$q_2$};
        \node (q3)[state, below = of q2] {$q_3$};
        \node (q4)[state, below = of q1, accepting] {$q_4$};
        \path [-stealth, thick]
        (q3) edge  node {$\epsilon;\$\rightarrow \epsilon$}   (q4)
        (q2) edge  node {$1;0\rightarrow \epsilon$}   (q3)
        (q2) edge [loop right]  node {$0;\epsilon\rightarrow 0$}()
        (q3) edge [loop right]  node {$1;0\rightarrow \epsilon$}()
        (q1) edge  node {$\epsilon;\epsilon\rightarrow \$$}   (q2);
    \end{tikzpicture}
\end{center} 
\subsubsection{Esempio 2}
Data una stringa $w$, definiamo $w^R$ la stringa $w$ specchiata, ad esempio $$ w=a12hgf \implies w^R=fgh21a$$
Sia $L=\{w\&w^R|w\in\{0,1\}^*\}$, ossia l'insieme di tutte le stringhe binarie palindrome, al cui centro 
è presente il carattere $\&$. 
\begin{center}
    \begin{tikzpicture} [node distance = 3cm, on grid, auto]
        \node (q1)[initial, state, left] {$q_1$};
        \node (q2)[state, right = of q1] {$q_2$};
        \node (q3)[state, below = of q2] {$q_3$};
        \node (q4)[state, below = of q1, accepting] {$q_4$};
        \path [-stealth, thick]
        (q3) edge  node {$\epsilon;\$\rightarrow \epsilon$}   (q4)
        (q2) edge  node {$\&;\epsilon\rightarrow \epsilon$}   (q3)
        (q2) edge [loop right]  node {$\begin{matrix}
            0;\epsilon\rightarrow 0\\1;\epsilon\rightarrow 1
        \end{matrix}$}()
        (q3) edge [loop right]  node {$\begin{matrix}
            0;0\rightarrow \epsilon\\1;1\rightarrow \epsilon
        \end{matrix}$}()
        (q1) edge  node {$\epsilon;\epsilon\rightarrow \$$}   (q2);
    \end{tikzpicture}
\end{center}
\subsection{PDA e Linguaggi Acontestuali}
Tale sezione si concentrerà sul seguente teorema.\acc 
\teo{} Un linguaggio è acontestuale (generato da una CFG) se e solo se esiste un PDA che lo riconosce.
Per rendere più leggibile e chiara la dimostrazione del teorema, essa verrà scomposta in due lemma, che rappresentano le due implicazioni della 
dimostrazione. \acc 
\lemma{[$\implies$]}  Se un linguaggio è acontestuale, esiste un PDA che lo riconosce. \acc 
\dimo{[$\implies$]} Sia $G=(V,\Sigma, R, S)$ una generica CFG, il cui linguaggio è $L(G)$. L'idea è quella di costruire un PDA  $P$
che sfrutti il non determinismo per accettare una generica stringa $w\in L(G)$ usando tutte le possibili 
derivazioni (applicazioni delle regole) di $G$.\acc 
Lo stack di $P$ conterrà le variabili ed i terminali di $G$ che verranno appositamente sostituiti con le regole.\\ 
Possono esserci differenti alberi di computazione, il comportamento del PDA può essere scritto dal seguente 
algoritmo 
\greybox{
    $\diamond $ si inserisce $\$$ nella pila \\ \\\color{blue}
    $\diamond $ while(true)\{\\\\\color{black}
    \hphantom{ident}$\diamond $ Se nel top dello stack vi è una variabile, si eseguono varie diramazioni in base a tutte 
    \\ \hphantom{ident}le regole che  comprendono tale variabile\\ \\
    \hphantom{ident}$\diamond $ Se nel top dello stack c'è un terminale, viene rimosso dallo stack e si confronta 
    con il\\\hphantom{ident} prossimo carattere in input, se identici la computazione continua, altrimenti rifiuta.\\\\ 
    \hphantom{ident}$\diamond $ Se nel top dello stack c'è $\$$ (la pila è svuotata), si accetta se e solo se 
    è stata \\\hphantom{ident}letta tutta la stringa in input.\\\\
    \color{blue}
    $\diamond $ \}\color{black}
}
\begin{figure}[h!]
    \centering 
    \includegraphics[width=0.6\textwidth ]{images/esempioCFGPDA.eps}
    \caption{Esempio di computazione}
    \label{fig:esePDACGF}
\end{figure}
\textbf{Notazione} : Per comodotià, verrà utilizzata una notazione ridotta nel diagramma del PDA in questione, 
in particolare, si introduce l'inserimento di una stringa nello stack, piuttosto che di un solo carattere, ovviamente 
un push down di questo tipo equivale a più push down, che richiedono stati intermedi che verranno omessi.
\begin{center}
    \begin{tikzpicture} [node distance = 3cm, on grid, auto]
        \node (q1)[ state, left] {$q$};
        \node (q2)[state, right = of q1] {$r$};
        \path [-stealth, thick]
        (q1) edge  node {$a;S\rightarrow xyz$}   (q2);
    \end{tikzpicture}\\equivale a\acc
    \begin{tikzpicture} [node distance = 3cm, on grid, auto]
        \node (q)[ state, left] {$q$};
        \node (q1)[state, right = of q] {};
        \node (q2)[state, right = of q1] {};
        \node (r)[state, right = of q2] {$r$};
        \path [-stealth, thick]
        (q2) edge  node {$a;\epsilon\rightarrow x$}   (r)
        (q1) edge  node {$a;\epsilon\rightarrow y$}   (q2)
        (q) edge  node {$a;S\rightarrow z$}   (q1);
    \end{tikzpicture}
\end{center}
Sia $P=(Q,\Sigma,\Gamma,\delta,q_{start},F)$ l'automa che deve accettare il linguaggio $L(G)$, l'insieme 
degli stati sarà 
$$ Q=\{q_{start},q_{loop},q_{accept}\}\cup E$$
Dove $E$ è l'insieme degli stati intermedi, ossia quelli che verranno omessi nella notazione.
Siano\begin{itemize}
    \item $a\in \Sigma$ un carattere generico della grammatica e dell'alfabeto del PDA corrispondente
    \item $A\in V$ una generica variabile della grammatica 
\end{itemize}
Si ricordi che gli elementi del codominio di $\delta$ sono insiemi, dato che il 
PDA è non deterministico. La funzione di transizione del PDA $P$ sarà definita come segue\begin{itemize}
    \item $\delta(q_{start},\epsilon,\epsilon)=\{(q_{loop},S\$)\}$ si ricordi che $S$ è la variabile iniziale 
    della grammatica e $\$$ è il simbolo per marcare il fondo dello stack
    \item $\delta(q_{loop},\epsilon,A)=\{(q_{loop},W)\ |\ A\rightarrow W \in R\}$, ossia  
    $A\longrightarrow W \in R$ è una generica regola della grammatica 
    \item $\delta(q_{loop},a,a)=\{(q_{loop},\epsilon)\}$
    \item $\delta(q_{loop},\epsilon,\$)=\{(q_{accept},\epsilon)\}$
\end{itemize}
In generale (ignorando gli stati in $E$, in notazione abbreviata), il PDA assumerà una forma canonica del tipo
\begin{center}
    \begin{tikzpicture} [node distance = 3cm, on grid, auto]
        \node (q0)[ state, left] {$q_{start}$};
        \node (q1)[state, right = of q] {$q_{loop}$};
        \node (q2)[state, right = of q1] {$q_{accept}$};
        \path [-stealth, thick]
        (q1) edge [loop below]  node {$\begin{matrix}
            0;0\rightarrow \epsilon\\1;1\rightarrow \epsilon
        \end{matrix}$}()
        (q0) edge  node {$\epsilon;A\rightarrow W\$ $}   (q1)
        (q1) edge  node {$\epsilon;a\rightarrow \epsilon$}   (q2);
    \end{tikzpicture}\\
    \hfill $\blacksquare$
\end{center}
\subsubsection{Esempio} 
Si consideri la seguente grammatica 
$$ \begin{cases}
    S\longrightarrow aTb|b\\ 
    T\longrightarrow Ta|\epsilon 
\end{cases} \ \ \ \ \ \ \ \ \ \ \begin{matrix}
    \Sigma = \{a,b\}\\ V=\{S,T\}
\end{matrix}$$
Il PDA corrispondente avrà pila $\Gamma = \{\$, S, a, T, b\}$, precisamente\begin{center}
    \includegraphics[width=0.7\textwidth ]{images/PDA2.eps}
\end{center}
Gli stati $E=\{q_1,1_2,q_3,q_4\}$ sono quelli di transizione che nella notazione abbreviata verrebbero 
omessi, sostituendo le etichette con gli archi.
\begin{center}
    \includegraphics[width=0.65\textwidth ]{images/PDA2.2.eps}
\end{center}
\lemma{[$\impliedby$]}  Dato un PDA, esiste una CGF le cui stringhe generate sono il linguaggio del PDA.\acc 
\dimo{[$\impliedby$]} Sia $P=(Q,\Sigma,\Gamma,\delta,q_0,F)$ il PDA in questione. L'idea è quella di considerare,
 per ogni coppia di stati $p,q$ in $Q$, una variabile denotata $A_{pq}$ che genera tutte le stringhe 
 che permettono al PDA di passare da $p$ a $q$ lasciando la pila vuota, come nell'esempio in figura \ref{fig:dimopda1}.
 \begin{center}
    \begin{figure}[h!]
        \centering 
        \includegraphics[width=0.8\textwidth ]{images/dimopda1.eps}
        \caption{esempio}
        \label{fig:dimopda1}
    \end{figure}
\end{center}\sapbox{
\textbf{Forma Canonica} : Durante la dimostrazione, si assumerà che $P$ soddisfi certe proprietà, ossia che assuma una 
\textit{forma canonica}, tale forma non fa perdere di generalità alla dimostrazione in quanto ogni PDA 
può essere trasformato in un PDA in tale forma lasciando invariato il linguaggio che accetta. Un PDA è nella forma canonica 
se\begin{enumerate}
    \item Inizia con la pila vuota, ed termina la computazione sempre con la pila vuota 
    \item Ha un solo stato accettante $|F|=1$
    \item Il PDA ad ogni passo computazionale, non sostituisce mai un elemento della pila con un altro, o ne elimina 
    uno, o ne aggiunge uno, la sostituzione non avviene mai in un solo passo di computazione, formalmente, cambia la 
    definizione della funzione di transizione 
    $$\delta_1 : Q\times\Gamma_\epsilon\times\Sigma_\epsilon\rightarrow\mathcal{P}(Q\times\{\epsilon\})$$
    $$\delta_2 : Q\times\{\epsilon\}\times\Sigma_\epsilon\rightarrow\mathcal{P}(Q\times\Gamma_\epsilon)$$
    $$\delta = \delta_1\cup \delta_2$$
\end{enumerate}
È chiaro che ogni PDA può essere portato in tale forma, se ci sono più stati di accettazione, si creerà un nuovo stato 
alla quale tutti rimandano, rendendolo l'unico stato di accettazione. Quest'ultimo stato inoltre si occuperà 
di svuotare la pila automaticamente grazie agli $\epsilon$-archi. Inoltre, ogni arco che esegue una sostituzione 
nello stack sarà diviso in due archi che eseguono in sequenza un pop ed un push, facendo uso di uno stato 
intermedio.\begin{center}
    \includegraphics[width=1\textwidth ]{images/pdacanonico.eps}
\end{center}}
Bisogna definire le regole dela grammatica, la variabile iniziale $S$ sarà $A_{q_0q_{accept}}$, quindi per 
definizione $S$ genererà tutte le stringhe che accetta il PDA. L'insieme delle regole $R$ della grammatica $G$ sarà 
definito come segue \begin{itemize}
    \item Siano $p,q,r,s\in Q$, $u\in\Gamma$, $a,b\in\Sigma_\epsilon$, se $$ (r,u)\in\delta(p,a,\epsilon)\land  
    (q,\epsilon)\in\delta(s,b,u)$$ allora ci sarà la regola $$A_{pq}\longrightarrow aA_{rs}b $$\begin{center}\begin{figure}[h!]
        \centering
        \includegraphics[width=0.8\textwidth ]{images/dimoPDACFG1.eps}
        \caption{situazione del PDA}
        \label{fig:dimoPDACFG1}
    \end{figure}\end{center}
    È ovvio che che si possa arrivare da $p$ a $q$ in tal modo, il carattere $a$ porta da $p$ ad $r$ (aggiungendo $u$), il 
    carattere $b$ porta da $s$ a $q$ (rimuovendo $u$), la variabile $A_{rs}$ deriva\footnote{
        con \textit{deriva}, si intende l'atto di generare tale stringa attraverso diverse produzioni, ossia la 
        chiusura transitiva della relazione di produzione, precedentemente denotata $\Rightarrow^*$
    } i caratteri che portano da $r$ ad $s$ con pila vuota.
    \item Per ogni tripla $p,q,r\in Q$ si pone la regola $A_{pq}\longrightarrow A_{pr}A_{rq}$. 
    \item Per ogni $p\in Q$, si pone la regola $A_{pp}\longrightarrow \epsilon$
\end{itemize}
La prova della dimostrazione segue dal fatto che $A_{pq}$ genera $x$ \textit{se e solo se} $x$ porta l'automa $P$ da 
$p$ a $q$ con pila vuota, ne segue che, se $S=A_{q_0q_{accept}}$ deriva $x$, allora $x\in L(P)$. Tale dimostrazione sarà 
suddivisa in due claim.\acc 
\claim{1} Se $A_{pq}$ deriva $x$, allora $x$ porta da $p$ a $q$ con pila vuota. \acc 
\dimo{ claim 1} Verrà dimostrato per induzione sul numero di produzioni di $x$ in $G$.\begin{itemize}
    \item \textbf{caso base} : $x$ è generata da una regola di $G$ per $k=1$ produzione, allora l'unica regola 
    che può generare $x$ è del tipo $A_{pp}\longrightarrow \epsilon$, allora $x=\epsilon$, e la stringa vuota porta 
    l'automa $P$ da $p$ a $p$ con pila vuota. 
    \item \textbf{ipotesi induttiva} : Per ogni $x$ generata da una regola di $G$ per $k>1$ produzioni, tale stringa 
    congiunge i due stati in questione lasciando la pila vuota. 
    \item \textbf{passo induttivo} : Supponiamo che $A_{pq}$ produca $x$ in $k+1$ produzioni, allora, data la definizione 
    delle regole di $G$, la regola in questione può essere di due tipi : \begin{itemize}
        \item La regola in questione è $A_{pq}\longrightarrow aA_{rs}b$, allora $x$ ha la forma $ayb$, ne consegue che 
        $A_{rs}$ deriva $y$ in al più $k$ passi, quindi, per ipotesi, la stringa $y$ porta l'automa $P$ dallo stato
        $r$ allo stato $s$ lasciando la pila vuota. Inoltre, per definizione delle regole è vero che 
        $$ (r,u)\in\delta(p,a,\epsilon)\land (q,\epsilon)\in\delta(s,b,u) \ \ \text{ per qualche }u\in\Gamma$$
        Quindi è chiaro che $x$ porti $P$ da $p$ a $q$ lasciando la pila vuota. 
        \item La regola in questione è $A_{pq}\longrightarrow A_{pr}A_{rq}$,, allora la stringa $x$ sarà del tipo 
        $yz$, quindi $A_{pr}$ deriva $y$, mentre $A_{rq}$ deriva $z$. Se $A_{pq}$ deriva $x$ in $k+1$ produzioni, allora 
        le derivazioni di $y$ e $z$ avvengono in al più $k$ produzioni, ma allora per ipotesi$$\begin{cases}
            y\text{ porta $P$ da $p$ a $r$ lasciando la pila vuota}\\ 
            z\text{ porta $P$ da $r$ a $q$ lasciando la pila vuota}
        \end{cases}\implies x=yz\text{ porta $P$ da $p$ a $q$ lasciando la pila vuota} $$
    \end{itemize}
    \hfill $\square$
\end{itemize}
\claim{ 2 } Se $x$ porta da $p$ a $q$ con pila vuota, allora $A_{pq}$ deriva $x$.\acc
\dimo{ claim 2} Verrà dimostrato per induzione sul numero di passi di computazione dell'automa $P$.\begin{itemize}
    \item \textbf{caso base} : L'automa compie 0 passi, la computazione inizia e finisce in $p$, l'input $x$ è quindi 
    la stringa vuota, siccome per definizione delle regole, $G$ ha la regola $A_{pp}\longrightarrow\epsilon$, allora 
    $A_{pp}$ deriva $x$. 
    \item \textbf{ipotesi induttiva} : Si ipotizza che per ogni computazione  di $k>0$ passi (che lasci vuota 
    la pila) da $p$ a $q$ con stringa in input 
    $x$, la regola $A_{pq}$ deriva $x$.  
    \item \textbf{passo induttivo} : Supponiamo che $x$ porti da $p$ a $q$ lasciando la pila vuota in $k+1$ passi di 
    computazione, precisamente, ci sono due possibili casi \begin{itemize}
        \item La pila è vuota solo negli stati $p$ e $q$, negli stati intermedi è stata riempita dal primo passo, e verrà 
        poi svuotata all'ultimo passo. Sia $u$, tale carattere inserito all'inizio e rimosso al termine, e sia $a$
        il simbolo che porta $p$ allo stato successivo $r$ (primo passo di computazione), e $b$ il simbolo che porta dal 
        penultimo stato $s$ a $q$ (ultimo passo di computazione), proprio come mostrato in figura \ref{fig:dimoPDACFG1}. Per 
        definizione, la grammatica $G$ contiene la regola $A_{pq}\longrightarrow aA_{rs}b$, quindi $x$ è del tipo 
        $ayb$. La variabile $A_{rs}$ deriva $y$, quindi $y$ porta da $r$ a $s$ lasciando la pila vuota con al più 
        $k-1$ passi. Ricapitolando\begin{itemize}
            \item $a$ porta da $p$ ad $r$ aggiungendo $u$
            \item $y$ porta da $r$ ad $s$ senza fare operazioni sulla pila 
            \item $b$ porta da $s$ a $q$ rimuovendo $u$
            \item $x=ayb$ porta da $p$ a $q$ lasciando la pila vuota 
            \item $A_{pq}$ deriva $x$.
        \end{itemize}
        \item La pila viene svuotata negli stati intermedi fra $p$ e $q$, sia $r$ lo stato in cui la pila 
        si svuota, essendo che la computazione da $p$ a $q$ richiede $k+1$ passi, allora le computazioni da 
        $p$ ad $r$ e da $r$ a $q$ richiederanno al più $k$ passi. Sia $x=yz$, $y$ porta da  $p$ ad $r$, e 
        $z$ porta da $r$ a $q$, per ipotesi $A_{pr}$ deriva $y$ e $A_{rq}$ deriva $z$, 
        ma allora, essendo che esiste la regola $A_{pq}\longrightarrow A_{pr}A_{rq}$ ne consegue che 
        $A_{pq}$ deriva $x=yz$.\begin{center}
            \includegraphics[width=0.8\textwidth ]{images/dimoPDACFG2.eps}
        \end{center}
    \end{itemize}
    \hfill $\square$
\end{itemize}
I due claim dimostrano il lemma $\impliedby$.\hfill $\blacksquare$\\
Essendo entrambi i lati dimostrati, il teorema sull'equivalenza fra PDA e CFG è dimostrato.
\subsection{Il Pumping Lemma per le Grammatiche Acontestuali} 
Il teorema presentato in questa sezione mostra che esistono anche dei linguaggi non acontestuali, ossia, per i quali non 
esiste alcuna grammatica che li genera. Un esempio di linguaggio acontestuale è il seguente 
$$ L=\{0^n1^n2^n\ | \ n\ge 1\}$$
Prima di presentare il teorema, è necessario introdurre il seguente fatto.\acc
\claim{} Sia $G$ una CFG in forma normale Chomsky, data la forma delle regole, ogni 
albero di derivazione di $G$ è binario, da ciò ne deriva che, preso un qualsiasi albero, se il cammino 
più lungo di tale albero è lungo $i$, allora  allora la stringa generata sarà 
lunga al più $2^{i-1}$.\acc 
\dimo{ claim } Si dimostra per induzione su $i$\begin{itemize}
    \item \textbf{caso base} : $i=1$, allora la derivazione è composta da una sola produzione, la stringa 
    è lunga 1, infatti $2^{i-1}=2^{1-1}=2^0=1$. 
    \item \textbf{ipotesi induttiva} : si assume sia vero per un generico $i>1$. 
    \item \textbf{passo induttivo} : si consideri un cammino lungo $k=i+1 =  (i>1)+1 \ge 3$, la prima regola applicata 
    deve essere necessariamente del tipo $$S\longrightarrow BC$$ i sotto alberi generati da $B$ e $C$ hanno il 
    cammino più lungo (denominato da ora in poi \textit{altezza}) grande al più $i$, generano quindi stringhe 
    lunghe al più $2^{-1}$, quindi $S$ genera stringhe lunghe al più $2\cdot 2^{i-1}=2^i=2^{k-1}$.\hfill$\blacksquare$
\end{itemize}
\teo{( Pumping Lemma per le CFG )}  Sia $L$ un linguaggio acontestuale, allora esiste un numero $p$ tale che, presa 
una qualsiasi stringa $w$ lunga almeno $p$, ($|w|\ge p$), esiste una sua suddivisione $$w=uvxyz$$ tale che\begin{enumerate}
    \item $\forall i\ge 0 \ \ \ uv^ixy^iz\in L$
    \item $|vy|>0$
    \item $|vxy|\ge p$
\end{enumerate}
\dimo{( Pumping Lemma per le CFG )} Sia $G=(V,\Sigma, R, S)$ la CFG associata ad $L$, ossia $L(G)=L$, si assume che $G$ sia in forma 
normale Chomsky (senza perdita di generalità), quindi ogni suo albero di derivazione sarà binario. Sia $m=|V|$ il numero 
di variabili distinte di $G$, identifichiamo come \textit{pumping} il numero $p=2^m$. Sia $w$ una generica stringa tale 
che $|w|\ge p = 2^m$, allora $w$ avrà un albero di derivazione di altezza almeno $m+1$, ed il numero di nodi 
nell'albero sarà almeno $m+2$, di cui 1 nodo è necessariamente un terminale, e gli altri $m+1$ sono variabili.\acc 
Sia $w=uvxyz$, siccome le variabili distinte sono $m$, esiste una variabile $A_i$ che si ripete nell'albero, in particolare \begin{itemize}
    \item L'albero con radice $A_i$ genera $vxy$
    \item L'albero con radice $A_j=A_i$ genera $x$
\end{itemize}
Mentre $u$ e $z$ sono generate nella derivazione da $S$ ad $A_i$.\begin{center}
    \includegraphics[width=0.4\textwidth ]{images/alberoDer1.eps}
\end{center}
Siccome $G$ è in forma normale, un sotto albero contiene, o un singolo terminale, o due variabili. Quindi, essendo che il 
sotto albero di $A_i$ non può essere un terminale, avendo $A_j$, sarà composto da due variabili. 
$$A_i\longrightarrow BC\in R $$
Una delle due variabili fra $B$ e $C$ genererà $A_j$ che a sua volta deriverà $x$. L'altra variabile deriverà 
$vy$, quindi $vxy\ne x$, ciò implica che necessariamente $|vy|>0$ (punto $(ii)$ dimostrato).\acc 
Essendo che il cammino più lungo nell'albero ha lunghezza $m+1$, le stringhe generate avranno lunghezza 
minore o uguale a $2^{(m+1)-1}=2^m=p$, quindi $|vxy|\ge p$  (punto $(iii)$ dimostrato).\acc 
Le variabili $A_i$ e $A_j$ possono essere sostituite nell'albero di computazione.
\begin{center}
    \includegraphics[width=0.85\textwidth ]{images/alberoDer2.eps}
\end{center}
In generale, è chiaro come $\forall i \ uv^ixy^iz\in L$. (punto $(i)$ dimostrato), la dimostrazione del 
teorema è completa.\hfill$\blacksquare$
\subsection{Esercizi ed Ultime Proprietà sulle CFG}
In questa sezione verrà utilizzato il pumping lemma per dimostrare che alcuni linguaggi non sono acontestuali.
\subsubsection{Esercizio 1}
Si consideri il linguaggio 
$$ L=\{0^n1^n2^n \ | \ n \ge 1 \}$$
Tale linguaggio non è acontestuale, non esiste nessuna CFG che lo genera. Per assurdo, si assuma che 
$L$ sia acontestuale, allora $\exists p$ tale che, per ogni stringa $w\in L$ di lunghezza al più 
$p$, valgono le condizioni del pumping lemma. \acc 
Si prende in esame la stringa $w=0^p1^p2^p$, bisogna considerare tutte le scomposizioni di $w$ del tipo\begin{itemize}
    \item $w=uvxyz$ con  
    \item $|vy|>0$
    \item $|vxy|\le p$
\end{itemize}
$$ w=\smash{\underbrace{00\dots 0}_{p\ \text{volte}}} \
\smash{\underbrace{11\dots 1}_{p\ \text{volte}}} \
\smash{\underbrace{22\dots 2}_{p\ \text{volte}}}$$
\\Se la stringa $vxy$ è lunga al più $p$ conterrà 1 oppure 2 caratteri distinti, dato che, se contenesse 3 caratteri distinti (sia 0 che 1 che 2 ) allora 
per costruzione di $w$ sarebbe lunga più di $p$. Si consideri il punto $(1)$ del pumping lemma, preso $i=0$ la stringa 
$$ \hat w=uv^0xy^0z$$
Deve essere contenuta in $L$. Siccome la stringa $vxy$ per definizione non era vuota, la stringa $\hat w = uxz$ 
avrà un numero di elementi minore di $w$ dato che $v$ ed $y$ sono state rimosse ed insieme erano composte da 
almeno 1 carattere. 
\acc 
La rimozione di $vy$ comporta il cambio del numero di occorrenze di 1 (oppure 2) simboli in $\hat w$ rispetto a 
$w$, quindi $\hat w$ avrà 1 (oppure 2) simboli le cui occorrenze differiscono dal/dai restante/restanti, ma allora 
non è del tipo $0^n1^n2^n, \ \ n \ge 1$, quindi non è in $L$, ma allora non è vero che 
$L$ è acontestuale.
\subsubsection{Esercizio 2}\label{es2}
Si consideri il linguaggio 
$$ L=\{ww \ | \  w\in\{0,1\}^* \}$$
Tale linguaggio non è acontestuale, non esiste nessuna CFG che lo genera. Per assurdo, si assuma che $L$ sia 
acontestuale, allora $\exists p$ tale che, per ogni stringa $w\in L$ di lunghezza al più 
$p$, valgono le condizioni del pumping lemma. \acc Si considera la stringa $$ w=0^p1^p0^p1^p$$ di cardinalità $|w|=4p$, 
bisogna considerare ogni possibile modo di scomporre la stringa, come prima cosa, si individuano le seguenti sezioni 
nella stringa\begin{center}
    \includegraphics[width=0.6\textwidth ]{images/sezioniStringa4p.eps}
\end{center}
La stringa può essere scomposta in diversi modi\begin{itemize}
    \item \textit{caso 1} : $vxy$ non si trova a cavallo fra i confini e non si trova a cavallo nel mezzo, è 
    quindi confinata fra una delle 4 metà della stringa, è quindi composta di soli 0 oppure di soli 1. Secondo il punto 
 $(1)$ del pumping lemma, preso $i=2$ si ha la stringa $$ \hat w = uv^2xy^2z$$
ma allora $\hat w$ non appartiene ad $L$. 
\item \textit{caso 2} : $vxy$ si trova a cavallo fra uno dei due confini, ma non tocca il mezzo. Si considera $i=0$ : 
$$ \hat w = uv02xy^0z=uxz$$
eliminando $vy$, si crea un "buco" rispetto alla stringa originale, tale buco ha dimensione 
al più $p$, essendo che una delle due metà della stringa cambia, il mezzo si sposta, supponiamo che $vxy$ si trovava sul confine 
destro (la dimostrazione è analoga al sinistro), allora il centro si sposterà verso destra di $\ge p$ posizioni, e la metà destra della 
stringa terminerà necessariamente con un 1, ma la metà sinistra terminerà necessariamente con uno zero, quindi le due metà 
non sono uguali e $\hat w\notin L$. 
\item \textit{caso 3} : $vxy$ si trova a cavallo sul mezzo ma non oltrepassa alcun confine. Si considera, preso $i=0$
$$ \hat w = uxz$$ 
In tal modo, la seconda e la terza sezione di $w$ verrà modificata, ed una delle due parti avrà meno di $p$ caratteri 
$$  w = \smash{\underbrace{00\dots 0}_{p\ \text{volte}}} \
\smash{\underbrace{11\dots 1}_{p\ \text{volte}}} \
\smash{\underbrace{00\dots 0}_{p\ \text{volte}}} \
\smash{\underbrace{11\dots 1}_{p\ \text{volte}}} $$ 
\\
$$ \hat w = \smash{\underbrace{00\dots 0}_{p\ \text{volte}}} \
\smash{\underbrace{11\dots 1}_{k\ \text{volte}}} \
\smash{\underbrace{00\dots 0}_{j\ \text{volte}}} \
\smash{\underbrace{11\dots 1}_{p\ \text{volte}}} $$\\ $$k\ne p\lor j\ne p $$
\\Ma allora $\hat w \notin L$.
\end{itemize}
Durante dimostrazioni di questo tipo può risultare utile l'applicazione di una proprietà delle CFG, ossia la loro 
\textbf{chiusura per concatenazione}, siano $G_1=(V_1,\Sigma, R_1, S_1)$ e $G_2=(V_2,\Sigma, R_2, S_2)$ due CFG, la grammatica 
$G=(V_1\cup V_2,\Sigma, R, S)$ dove \begin{itemize}
    \item $S$ è un nuovo stato 
    \item $R=R_1\cup R_2\cup\{S\rightarrow S_1S_2\}$
\end{itemize}
é ancora una CFG.
\acc 
Differentemente, le CFG non sono chiuse per intersezione, basta dare un singolo contro esempio per dimostrare la tesi, si 
considerino i seguenti linguaggi 
$$ \begin{matrix}
    L_1=\{0^n1^n2^i \ | \ n\ge 0\land i\ge 0\}\\ 
    L_2=\{0^k1^n2^n \ | \ n\ge 0\land k\ge 0\}
\end{matrix}$$
Entrambi i linguaggi sono acontestuali, infatti sono generati dalle regole 
$$ R_1=\begin{cases}
    S\longrightarrow S_1S_2\\ 
    S_1\longrightarrow0S_1 1|\epsilon\\ 
    S_2 \longrightarrow 2S_2|\epsilon
\end{cases} \ \ \ \ \ \ \ \ R_2=\begin{cases}
    S\longrightarrow S_1S_2\\ 
    S_1\longrightarrow0S_1 |\epsilon\\ 
    S_2 \longrightarrow 1S_22|\epsilon
\end{cases}$$
L'intersezione dei due linguaggi $L=L_1\cup L_2 = \{0^n1^n2^n \ | \ n\ge 0\}$ sappiamo essere un linguaggio acontestuale, 
quindi le CFG \textbf{non sono chiuse per intersezione}.
\subsubsection{Esercizio 4}
Si consideri il seguente linguaggio $$ L=\{a,b\}^* \ \backslash \ \{ww\ | \ w\in\{a,b\}^*\}$$
Comprende tutte le stringhe che non possono essere scritte come una stessa stringa ripetuta due volte. Nell'esercizio 
\ref{es2} si è visto 
che il complementare di $L$, non è un linguaggio acontestuale, si vuole dimostrare che invece $L$ lo è. Sicuramente, 
tutte le stringhe di cardinalità dispari saranno in $L$. Le regole che generano il linguaggio sono le seguenti$$ 
R=\begin{cases}
    S\longrightarrow A|B|AB|BA\\ 
    A\longrightarrow a|aAa|aAb|bAa|bAb 
    B\longrightarrow b|aBa|aBb|bBa|bBb 
\end{cases}
$$\begin{itemize}
    \item $A$ genera le stringhe dispari con una $a$ al centro
    \item $B$ genera le stringhe dispari con una $b$ al centro
\end{itemize}
\dimo{} : Si vuole dimostrare che le stringhe di lunghezza pari generate da $R$ sono in $L$. Si dimostreranno entrambi i 
versi.\acc 
\boxedMath{$\implies$} Se $x\in L$, allora $S\Rightarrow^* x$. Se $x\in L$, allora esiste almeno una lettera che si differenzia fra le due sotto stringhe di eguale 
lunghezza che compongono $x$, sia $n=|x|$
$$\exists \ i \ | \ x_i\ne x_{\nicefrac{n}{2}+1} $$
$x$ può essere scritta come unione di due 
stringhe di lunghezza dispari $x=uv$. Si pongono \begin{itemize}
    \item $u=x_1x_2\dots x_{2i-1}$
    \item $v=x_{2i}x_{2i+1}\dots x_n$
\end{itemize}
$S\longrightarrow AB$ e $A\longrightarrow u \land B\longrightarrow v \implies S\Rightarrow^* x$.\acc 
\boxedMath{$\impliedby$} Se $S\Rightarrow^* x$ allora $x\in L$. 
Siccome $|x|=n=0\mod 2$, può essere generata a partire da $S\longrightarrow AB$ oppure $S\longrightarrow BA$, si suppone che 
sia generata dalla prima (il procedimento è analogo), 
allora $x=uv$ con $u\ne v$ e \begin{itemize}
    \item $A\Rightarrow^* u$
    \item $B\Rightarrow^* v$
\end{itemize}
Sia $|u|=l$ e $|v|=n-l$, le lettere centrali di $u$ e $v$ sono \begin{itemize}
    \item $u_{\frac{l+1}{2}}$
    \item $v_{\frac{n-2+1}{2}}$
    \item dove $u_{\frac{l+1}{2}}\ne v_{\frac{n-2+1}{2}}$
\end{itemize}
Scrivendo le lettere in funzione di $x$ si ha 
$$ u_{\frac{l+1}{2}}=x_{\frac{l+1}{2}} \ \ \ \  \  v_{\frac{n-2+1}{2}}=x_{\frac{n+l+1}{2}}
$$
\redText{dimostrazione non completa}
\chapter{Calcolabilità}
\section{Macchina di Turing}
Esistono linguaggi che nessuna CFG può accettare, si vuole estendere il modello di calcolo ad uno più 
potente. Negli anni 30' del ventesimo secolo fu introdotta la \textbf{Macchina di Turing} (alla quale ci 
riferiremo come TM), estendendo gli automi dandogli una memoria illimitata, tale modello è una semplice 
astrazione dei calcolatori odierni, e corrisponde alla nozione di algoritmo.\subsubsection{Caratteristiche di una TM}\begin{itemize}
    \item Una TM possiede un \textit{nastro di lavoro}, rappresenta una memoria (illimitata) sulla quale il modello 
    può scrivere dei caratteri. 
    \item Una \textit{testina di lettura} che identifica la precisa posizione attuale sul nastro, che può spostarsi 
    a destra o a sinistra. 
    \item Vi sono poi degli \textit{stati di accettazione} (la stringa in input è accettata dalla TM) e degli 
    \textit{stati di rifiuto} (la stringa in input è rifiutata dalla TM). Quando la TM passa su uno stato di rifiuto, 
    la computazione è immediatamente terminata. 
    \item È possibile che per alcuni input una TM entri in uno stato di \textit{loop} in cui non termina.
\end{itemize}
\defi{(Turing Machine)} Una TM $T=(Q,\Sigma,\Gamma,\delta,q_0,q_{acc},q_{rej})$ è una tupla tale che \begin{itemize}
    \item $Q$ è l'insieme degli stati 
    \item $\Sigma$ è l'alfabeto delle stringhe in input 
    \item $\Gamma$ è l'insieme dei caratteri che possono essere scritti sul nastro, solitamente $\Sigma\subseteq \Gamma$. Inoltre 
    in $\Gamma$ vi è sempre un carattere speciale $\blank$ (denominato "blank") che rappresenta il carattere vuoto.
    Inoltre $\blank\notin\Sigma$.
    $\delta$ è la funzione di transizione definita $$ \delta : Q\times \Gamma \rightarrow Q\times \Gamma \times \{L,R\}$$
    L'insieme $\{L,R\}$ rappresenta i possibili spostamenti della testina a sinistra o a destra
    \item $q_0$ è lo stato iniziale 
    \item  $q_{acc}$ è lo stato (unico) di accettazione 
    \item $q_{rej}$ è lo stato (unico) di rifiuto 
\end{itemize}
Canonicamente, la configurazione iniziale di una TM prevede l'intera stringa in input contenuta nel nastro, seguita 
dal carattere $\blank$.\begin{center}
    \includegraphics[width=0.7\textwidth ]{images/turingMachine.eps}
\end{center}
Una TM computa la stringa in input seguendo le regole definite dalla $\delta$, come per gli automi, per una 
TM è defintia la configurazione ad un certo passo nella configurazione, essa determina il contenuto del nastro, la 
posizione della testina, e lo stato attuale. Una configurazione si denota $$ uqav$$ dove \begin{enumerate}
    \item $u,v\in\Gamma^*$
    \item $q\in Q$
    \item $uav$ è il contenuto del nastro, $a\in\Gamma$ è il carattere su cui si trova la testina.
\end{enumerate}
Data in input una stringa $w$, la configurazione iniziale sarà $q_0w$. Si può rappresentare graficamente una 
configurazione come segue\begin{center}\begin{figure}[h!]
    \centering 
    \includegraphics[width=0.8\textwidth ]{images/confTM.eps}
    \caption{la testina è sul carattere $w_i$ e lo stato attuale è $q$}
\end{figure}
\end{center}
Per definire il concetto di accettazione, bisogna stabilire la relazione di \textit{produzione} :
 $$\begin{matrix}
    uaq_ibv \text{ produce }uq_jacv \\ \text{se e solo se}\\ 
    \delta(q_i,b)=(q_j,c,L)
 \end{matrix} $$
Vuol dire che la TM, nello stato $q_i$, leggendo con la testina il carattere $b$ si sposta a sinistra. Può 
essere analogamente definito per lo spostamento a destra.
$$\begin{matrix}
    uaq_ibv \text{ produce }uacq_jv \\ \text{se e solo se}\\ 
    \delta(q_i,b)=(q_j,c,R)
 \end{matrix} $$
 Diremo che una TM \textbf{accetta} $w$ se e solo se esiste una sequenza di configurazioni 
 $$ C_1\rightarrow C_2\rightarrow C_3\dots\rightarrow C_k$$
 dove \begin{itemize}
    \item $C_1=q_0w$ 
    \item $\forall i \ \ \ C_i$ produce $C_j$
    \item lo stato della configurazione $C_k$ è lo stato accettante $C_k=uq_{acc}av$
 \end{itemize}
 \defi{(Riconoscibilità)} Un linguaggio $L$ è \textbf{turing riconoscibile} se esiste una TM $M$ che 
 \textit{accetta} ogni sua stringa, si dice che $L$ è il linguaggio di $M$.\acc 
 Se una TM deve computare una stringa che non è nel suo linguaggio, può \begin{itemize}
    \item rifiutare 
    \item andare in loop
 \end{itemize}
 Una TM $M$ per cui, data una qualsiasi stringa, non vai mai in loop, viene detta \textbf{decisore}.\acc 
 \defi{(Decidibilità)} Un linguaggio $L$ è \textbf{turing decidibile} se esiste una TM $M$ che è un 
 decisore per $L$, ossia, per ogni sua stringa, non va mai in loop. Si dice che $M$ \textit{decide} $L$.\acc 
Data una TM $M$ si hanno gli insiemi \begin{itemize}
    \item $L(M)=\{w\in\Sigma^* \ | \ M\text{ accetta }w \}$
    \item $R(M)=\{w\in\Sigma^* \ | \ M\text{ rifiuta }w \}$
    \item Generalmente $L(M)\cup R(M)\subseteq \Sigma^*$
    \item Se $L(M)\cup R(M)= \Sigma^*$ allora $M$ è un decisore.
 \end{itemize}
 Un linguaggio che non ha decisori \textit{non è decidibile}, la definizione di decidibilità stabilisce i limiti della 
 computabilità, esistono infatti dei linguaggi (astrando, dei problemi) che non possono essere decisi (non possono essere 
 risolti), ciò si lega inevitabilmente con i \textit{teoremi di incompletezza} di Gödel, esisteranno sempre 
 delle proposioni per cui è \textit{impossibile} stabilire se sono vere o false. 
 \subsection{Esempi di TM}
 Le TM verranno rappresentato in maniera compatta sottoforma di grafi proprio come per gli automi.
 \begin{center}
	\begin{tabular}{>{\centering\arraybackslash}m{3in}>{\arraybackslash}m{3in}}
        \begin{tikzpicture} [node distance = 5cm, on grid, auto]
            \node (q1)[state, left] {$q_1$};
            \node (q2)[state, right = of aperta] {$q_2$};
            \path [-stealth, thick]
            (q1) edge  node {$a\rightarrow b,R$}   (q2);
        \end{tikzpicture} & Il grafo rappresentato in figura descrive la seguente situazione : Se la TM si trova 
        nello stato $q_1$, e la testina si trova sul carattere $a$, allora si sostituisce il carattere 
        $a$ nel nastro con il carattere $b$, si sposta la testina a destra (se al posto di $R$ ci fosse 
        stato $L$ si sarebbe andati a sinistra) e la TM si sposta nello stato $q_2$.
		\\
	\end{tabular}
\end{center}
\subsubsection{Esempio 1}
Si consideri il seguente linguaggio $L=\{01^*0\}$. La TM che decide $L$ è la seguente\begin{center}
    \begin{tikzpicture} [node distance = 4cm, on grid, auto]
        \node (q0)[state, initial, left] {$q_0$};
        \node (B)[state, right = of q0] {$B$};
        \node (C)[state, right = of B] {$C$};
        \node (rej)[state, below = of B] {$q_{rej}$};
        \node (acc)[state,accepting, below = of C] {$q_{acc}$};
        \path [-stealth, thick]
        (B) edge [loop above]  node {$1\rightarrow y, R$}()
        (q0) edge  node {$0\rightarrow x,R$}  (B)
        (q0) edge [bend right]  node[left] {$\begin{matrix}\\1\rightarrow 1,R \ \ \\\blank\rightarrow \blank,R\ \ \end{matrix}$}  (rej)
        (B) edge  node {$0\rightarrow x,R$}  (C)
        (C) edge  node {$\blank\rightarrow \blank,R$}  (acc)
        (C) edge  node {$\begin{matrix}1\rightarrow 1,R\\0\rightarrow 0,R\end{matrix}$}  (rej)
        (B) edge  node {$\blank\rightarrow \blank,R$}  (rej);
    \end{tikzpicture}
\end{center}
Esempio di computazione su una stringa di $L$:$$ 
\begin{matrix}
    \bar 0 1 1 1 0 \blank & & x\bar 1 1 1 0 \blank&& xy\bar 1 1 0 \blank\\ xyy\bar 10\blank & & xyyy\bar 0\blank && xyyyx\bar \blank
\end{matrix} 
$$
\subsubsection{Esempio 2}\label{esempioTM2}
Si consideri il seguente linguaggio $L=\{0^n1^n \ | \ n\ge 0\}$. La TM che decide $L$ è la seguente (stato di 
rifiuto omesso)\begin{center}
    \begin{tikzpicture} [node distance = 3cm, on grid, auto]
        \node (A)[state, initial, left] {$A$};
        \node (B)[state, right = of A] {$B$};
        \node (C)[state, right = of B] {$C$};
        \node (D)[state, below = of A] {$D$};
        \node (acc)[state,accepting, below = of D] {$q_{acc}$};
        \path [-stealth, thick]
        (B) edge [loop below]  node {$\begin{matrix}y\rightarrow y,R\\0\rightarrow 0,R\end{matrix}$}()
        (D) edge [loop right]  node {$y\rightarrow y,R$}()
        (C) edge [loop right]  node {$\begin{matrix}y\rightarrow y,L\\0\rightarrow 0,L\end{matrix}$}()
        (A) edge  node[below] {$0\rightarrow x,R$}  (B)
        (A) edge [bend right] node[left] {$\blank\rightarrow \blank,R$}  (acc)
        (A) edge  node {$y\rightarrow y,R$}  (D)
        (D) edge  node {$\blank\rightarrow \blank,L$}  (acc)
        (B) edge  node[below] {$1\rightarrow y,L$}  (C)
        (C) edge [bend right] node[above] {$x\rightarrow x,R$}  (A);
    \end{tikzpicture}
\end{center}
Nel capitolo precedente si è visto come il linguaggio $L=\{0^n1^n2^n \ | \ n\ge 0\}$  non è acontestuale, 
è possibile computarlo tramite le TM, de facto, basterà unire 2 TM che si comportino come 
quella vista nell'esempio \ref{esempioTM2}, infatti nella prima computazione si occuperà di controllare 
che la stringa in input abbia i primi $2n$ caratteri del tipo $0^n1^n$.$$
00\dots 0 \  \ 11\dots 1 \ \ 22\dots 2  \ \ \text{ viene trasformata } \ \ xx\dots x \ \ yy\dots y \ \ 22\dots 2$$
La seconda TM si occuperà di controllare se gli ultimi $2n$ caratteri sono del tipo $y^n2^n$. $$
xx \dots x \ \ yy\dots y\  \ 22\dots 2 \ \ \text{ viene trasformata }  \  \ xx\dots x \ \  xx \dots x  \ \ yy \dots y
$$
Senza perdita di generalità, è possibile astrarre le TM considerano due nuovi modelli equivalenti\begin{itemize}
    \item \textbf{TM multinastro}
    \item \textbf{TM non deterministica}
\end{itemize}
Prima di introdurli, si consideri il seguente esempio di 
astrazione di una TM, ossia di una macchina che, ad ogni passo di 
computazione, piuttosto che spostarsi necessariamente a destra o 
a sinistra, può rimanere ferma, la denominiamo STM. \acc 
La definiamo $M'$, e la sua funzione di transizione sarà del tipo 
$$ \delta':Q\times\Gamma \rightarrow Q\times\Gamma\times \{L,R,S\}$$
Dove $S$ (che sta per 'stay') indica l'azione del restare fermi senza 
muovere la testina.\acc 
\prop{} Per ogni STM esiste una TM classica equivalente. \acc 
\dimo{} ,Sia $M'$ la STM, l'idea è quella di considerare una TM $M$ 
che gestisca le transizioni di stato in cui la testina 
non si muove, a tal scopo, basta simulare tale azione tramite 
una sequenza di 2 movimenti che fanno uso di uno stato ausiliario $q_s$. 
\begin{center}
    $ \delta'(q,a)=(p,b,S)$
    \\ è equivalente alle azioni \\ 
    $\begin{cases}
        \delta(q,a)=(p,q_s,L)\\ 
        \delta(q_s,*)=(p,*,R)
        
    \end{cases}$
\end{center}
dove $*$ rappresenta un qualsiasi elemento di $\Gamma$.
\subsection{TM multi nastro}\label{TMmultinastro}
Introduciamo la macchina di Turing con più nastri, che verrà 
denotata MTM, 
sia $k$ il numero di nastri, ogni nastro avrà 
una testina proprietaria, uno stato sarà quindi rappresentato 
dalle stringhe scritte su tutti e $k$ i nastri, e la relativa 
posizione delle testine. \acc 
La funzione di transizione prenderà le decisioni valutando le posizioni 
ed il valore di tutte le testine 
$$ \delta : Q\times\Gamma^k\rightarrow Q\times\Gamma^k\times \{L,R\}^k$$
\teo{(MTM$\equiv$TM)} Per ogni MTM esiste una TM equivalente.\acc 
\dimo{} La dimostrazione, piuttosto che formale, mostrerà la costruzione di una 
TM che si comporta come la generica MTM.\acc 
Sia $MM$ la macchina multi nastro, ed $M$ la macchina classica 
 che deve simularla, si introduce un nuovo carattere $\#\notin \Gamma$
 che verrà utilizzato nel nastro di $M$ come separatore per i 
differenti nastri originari di $MM$.\begin{center}
    \includegraphics[width=\textwidth ]{images/multinastro.eps}
\end{center}
Per simulare le $k$ testine, si implementa la possibilità per $M$ 
di \textit{marcare} i caratteri (in tal caso, con un punto come 
apice). I caratteri marcati indicano che la testina è posta su di essi. Sia $\Gamma'$ l'alfabeto dei nastri di $MM$, e 
$\Gamma$ l'alfabeto del nastro di $M$.
$$\forall a\in\Gamma', \ \ \ \exists \dot{a} \in\Gamma$$
Si descrive ora la computazione di $M$, data una stringa 
in input $w$ la configurazione iniziale è la seguente: 
$$\#\dot{w_1}w_2w_3\dots,w_n\# 
\smash{\underbrace{\dot{\blank}\#\dot{\blank}\dots \#\dot{\blank}}_{k-1\ \text{volte}}}
$$
\\ 
Nel \textit{passo di computazione}, 
si scansiona una prima volta il nastro 
partendo dal primo simbolo $\#$, leggendo tutti i 
caratteri marcati, considerando poi la funzione di 
transizione, si esegue una seconda scansione aggiornando i 
valori delle testine (le marcature dei caratteri) ed il contenuto 
dei $k$ caratteri in questione. Se la testina su uno dei nastri deve 
spostarsi su $\#$, allora $M$ sposta il contenuto dell'intero nastro a destra di 1 posizione.\acc 
\textbf{Corollario} : $L$ è turing riconoscibile/decidibile se e solo se esiste una MTM che lo riconosce/decide.
\subsection{TM non deterministiche}
Si arricchisce il modello della macchina di turing introducendo il non determinismo, denotandolo NTM, la funzione di transizione cambia definizione 
$$ \delta : Q\times\Gamma \rightarrow \mathcal{P}(Q\times \Gamma \times \{L,R\})$$
Per ogni input si diramano più vie di computazione, una NTM accetta una stringa in input se almeno un ramo di computazione è accettante.\acc 
\teo{(NTM$\equiv$TM)} Per ogni NTM esiste una TM equivalente.\acc 
\dimo{} La dimostrazione, piuttosto che formale, mostrerà la costruzione di una 
TM che si comporta come la generica NTM.\acc Sia $N$ la NTM in questione, e sia $M$ la TM classica che la vuole 
simulare. $M$, per essere equivalente, dovrà esplorare ogni ramo di computazione, precisamente, deve esplorarlo in altezza. L'esplorazione in profondità in presenza di un loop su un ramo di computazione, comporterà il loop su $M$, anche se l'originaria $N$ aveva uno stato accettante su un differente ramo. \begin{center}
    \includegraphics[width=0.3\textwidth ]{images/loopEAccettante.eps} 
\end{center}
\textbf{Idea algoritmica} : $M$ utilizzerà 3 nastri\begin{enumerate}
    \item il primo nastro conterrà la stringa in input che non verrà modificata (il riferimento va mantenuto)
    \item il secondo nastro sarà il nastro di lavoro che verrà modificato per esplorare un cammino emulando la NTM.
    \item il terzo nastro conterrà l'indirizzo del nodo 
    dell'albero che si sta esplorando.
\end{enumerate}
Nell'$i$-esimo step, identificato da una tripla 
$$ (M,w,i)$$
usando l'input del primo nastro si percorre il ramo simulando $N$, se viene trovato uno stato accettante, 
$M$ accetta, altrimenti si incremente l'indice $i$
 e si ripercorre.
 \begin{itemize}
    \item Se un ramo di $N$ è accettante, $M$ accetta 
    \item Se ogni ramo di $N$ rifiuta, $M$ rifiuta 
    \item Se almeno un ramo di $N$ va in loop, e nessun 
    ramo accetta, $M$ va in loop.
 \end{itemize}
 \begin{quote}
    Se una NTM è un decisore, tutti i cammini hanno lunghezza finita
 \end{quote}
 \hfill$\blacksquare$
 \subsection{L'Enumeratore}
 L'enumeratore è un modello (in particolare, una variante di una TM) capace di generare un preciso insieme di stringhe, in un qualsiasi ordine e con eventuali ripetizioni, in particolare, per ogni linguaggio turing-riconoscibile, esiste un enumeratore che lo genera.\acc
Un enumeratore è una TM che "stampa" delle stringhe, eventualmente, infinite.\acc 
\teo{} Un linguaggio è turing riconoscibile se e solo se esiste un enumeratore che lo genera.\acc 
\dimo{} Si dimostrano separatamente le due direzioni.\acc 
\boxedMath{$\impliedby$} Dato un enumeratore $E$, definiamo una TM $M$ come segue (sia $w$ l'input)\begin{itemize}
    \item Si esegue $E$, se stampa una stringa, si confronta con $w$
    \item Se $w$ è uguale ad una stringa stampata da $E$, allora $M$ accetta.
\end{itemize}
\boxedMath{$\implies$} Sia $M$ una TM, si considera un enumeratore $E$ che deve 
stampare $L(M)$. Sia $\Sigma$ l'alfabeto di 
$L(M)$, identifichiamo come segue 
$$ \Sigma^*=\{\epsilon,s_1,s_2\dots\}$$
la lista di tutte le possibile stringhe su tale alfabeto. $E$ sarà definito come segue 
\begin{itemize}
    \item Si ripetono i passi per $i=1,2\dots$\begin{itemize}
        \item Si esegue $M$ per $i$ passi su ogni input $s_1,s_2\dots s_i$
        \item Se una qualsiasi computazione su $s_j$ accetta, $E$ stampa 
        $s_j$
    \end{itemize}
\hfill$\blacksquare$\end{itemize}\flowerLine 
\section{Decidibilità dei Linguaggi}
Come esistono linguaggi non regolari e linguaggi non acontestuali, risulta naturale porsi la domanda : Esistono dei linguaggi non turing riconosibili? Il modello della TM corrisponde al concetto di algoritmo, un linguaggio non riconoscibile corrisponde ad un problema che \textit{nessun} algoritmo può risolvere.\acc 
Una TM può ricevere in input un qualsiasi oggetto matematico, come un polinomio, un DFA, o direttamente un'altra TM, è importante che tale oggetto $O$, venga \textbf{codificato} in binario, denotandolo $<O>$ prima di essere computato dalla TM.\acc 
Vediamo come prima cosa un esempio di linguaggio decidibile, si consideri 
$$A_{DFA}=\{<D,w>\ |\ D \in DFA \land w\in L(d)\}$$
con $D\in DFA$ si indica che $D$ è un DFA. \acc 
\prop{} $A_{DFA}$ è decidibile. \acc 
\dimo{} Si vuole costruire un decisore per $A_{DFA}$, la TM in questione avrà il seguente comportamento\begin{enumerate}
    \item Su input $<D,w>$ interpreta $D$ come un DFA e $w$ come stringa, se non riconosce l'input, rifiuta 
    \item Simula l'esecuzione di $D$ con input $w$
    \item Se $D$ accetta $w$, allora la TM accetta, altrimenti rifiuta
\end{enumerate}
\hfill $\blacksquare$\acc 
In maniera simile, si può dimostrare che anche  $A_{NFA}=\{<N,w>\ |\ D \in NFA \land w\in L(d)\}$ è decidibile, utilizzando una TM non deterministica, oppure, si potrebbe trasformare l'input in un DFA ed eseguire l'algoritmo appena trattato. \acc  
Si consideri il linguaggio 
$$ E_{DFA}=\{<D>\ | \ D\in DFA \land L(D)=\emptyset\}$$
Ossia di tutte le codifiche di DFA che hanno linguaggio vuoto, ossia che non accettano nessuna stringa. \acc 
\prop{} $E_{DFA}$ è decidibile. \acc 
\dimo{} La TM che deciderà il linguaggio codificherà il DFA come un grafo, in gli stati sono i nodi, e vi è un arco da un nodo ad un altro solo se esiste una transizione fra i due stati. In particolare la TM seguirà il seguente comportamento\begin{enumerate}
    \item marca lo stato iniziale, ed inizia a fare una passeggiata sul grafo 
    \item marca ogni stato raggiungibile 
    \item se almeno uno stato di accettazione è marcato, la TM rifiuta, altrimenti accetta\hfill $\blacksquare$
\end{enumerate}
Un altro esempio interessante è il seguente 
$$ EQ_{DFA}=\{<D_1,D_2>\ | \ D_1,D_2\in DFA\land L(D_1)=L(D_2)\}$$
L'insieme delle coppie (codificate) di DFA che accettano lo stesso linguaggio.\acc 
\prop{} $EQ_{DFA}$ è decidibile. \acc 
\dimo{} Dati due DFA si definisce l'operatore di  differenza simmetrica $\Delta$ come segue 
$$ L(D_1)\Delta L(D_2)= (L(D_1)\cap \overline{L(D_2)})\cup (L(D_2)\cap \overline{L(D_1)})$$
Fatto : $L(D_1)=L(D_2)\iff L(D_1)\Delta L(D_2)=\emptyset$. Dati $D_1$ e $D_2$ si definisce $D$ tale che 
$L(D)=L(D_1)\Delta L(D_2)$ e si esegue l'algoritmo per decidere $E_{DFA}$ su $D$. \hfill$\blacksquare$\acc 
La seguente proposizione sarà necessaria a dimostrare la decidibilità di un linguaggio. \acc 
\prop{} Se $G$ è una CFG in forma normale Chomsky, e $w\in L(G)$, se $|w|=n$ per produrre $w$ saranno necessarie $2n-1$ derivazioni. \acc 
\dimo{} Il caso base per $n=1$ è banale. Se $n\ge 2$ la regola di partenza sarà $$ S\rightarrow AB$$ 
$A$ produrrà $u$ e $B$ produrrà $v$, con $w=uv$. Allora 
$$ |u|= k \ \ \ |v|=n-k$$
Quindi per ipotesi induttiva per generare $w$ saranno necessari 
$$ 1+2k-1+2(n-k)-1=1+2k-1+2n-2k-1=2n-1$$
passi.\hfill$\blacksquare$\acc 
Si consideri 
$$ A_{CFG}=\{<G,w>\ |\ G\in CFG \land G \text{ produce }w\}$$
\prop{} $A_{CFG}$ è decidibile. \acc 
\dimo{} Si può assumere che $G$ sia in forma normale Chomsky. Si costruisce una TM che considera tutte le derivazioni lunghe $2n-1$ di $G$. Se trova $w$ accetta, altrimenti rifiuta.\hfill$\blacksquare$\acc 
Anche il linguaggio 
$$ E_{CFG}=\{<G>\ | \ G\in CFG \land L(G)=\emptyset\}$$
è decidibile, se ne da un \textit{idea di dimostrazione}, la TM può procedere nel seguente modo\begin{enumerate}
    \item data $G$ marca ogni terminale 
    \item marca ogni variabile $A$ se esiste la regola $A\longrightarrow U_1U_2\dots U_k$
    e se esiste un $U_i$ marcato. 
    \item accetta se e solo se $S$ non è marcata, altrimenti rifiuta.
\end{enumerate}
\flowerLine 
\section{Linguaggi non Decidibili}
Un linguaggio può essere regolare, acontestuale, decidibile e riconoscibile, in generale vi è una relazione di inclusione fra gli insiemi delle classi dei linguaggi.\begin{itemize}
    \item regolari $\subset$ acontestuali  
    \item acontestuali $\subset$ decidibili 
    \item decidibili $\subset$ riconoscibili 
\end{itemize}
\begin{center}
    \includegraphics[width=0.7\textwidth ]{images/insiemiLinguaggi.pdf}
\end{center}
\subsection{Macchina di Turing Universale}
Vedremo che il linguaggio contenente le codifiche delle macchine di turing ed una qualsiasi stringa che accettano, è riconoscibile ma non decidibile.
\eqImportante{$ A_{TM}=\{<M,w> \ | \ M\in TM\land w\in L(M)\}$}
\teo{} $A_{TM}$ è turing-riconoscibile.\acc 
\dimo{} Si definisce una TM nota detta \textit{macchina di Turing universale} denotata $U$, sarà quella che riconoscerà $A_{TM}$. Avrà 2 nastri, nel primo conterrà la codifica dell'input $<M,w>$, nel secondo conterrà la configurazione corrente $<(a,q,b)>$ dell'esecuzione simulata di $M$.\acc 
È necessario definire la codifica di $M,w$. Essendo 
$$ M=(Q,\Sigma,\Gamma,\delta,q_0,q_{acc},q_{rej})$$
Siano 
$$ n=|\Sigma| \ \ m=|\Gamma| \ \ s=|Q|$$
la codifica sarà 
$$ (n)_2,(m)_2,(s)_2,<\delta>$$
dove $,$ è un carattere speciale, $(k)_2$ è la codifica binaria di $k\in\Z$ e $<\delta>$ è la codifica binaria delle transizioni 
$$ <\delta>=R_1;R_2\dots;R_j$$
Le regole sono del tipo 
$$ R=((q,a),(r,b,z)) \ \ \ \ z\in\{L,R\}$$
I passi di esecuzione sono i seguenti\begin{enumerate}
    \item Sul nastro 2 viene scritta la configurazione iniziale 
    \item al passo $t$-esimo, la configurazione sarà del tipo $$ <(a,q,b)>$$\begin{enumerate}
        \item si estrae $q$ dalla configurazione 
        \item si scansiona il nastro 1 in cerca di una regola del tipo $$ <(q,x),(r,y,z)>$$
        se $x\ne b[1]$\footnote{primo carattere di $b$}, si controlla la regola successiva, se $x=b[1]$
        si aggiorna la configurazione sul nastro 2 in accordo con la regola letta. 
        \item se il nastro 2 contiene una configurazione con uno stato fra $q_{acc},q_{rej}$, accetta o rifiuta di conseguenza. 
    \end{enumerate}
    \item Se $w\in L(M)$, $U$ accetta, altrimenti rifiuta. Se $M$ va in loop, $U$ andrà in loop di conseguenza. 
\end{enumerate}
$U$ è quindi un riconoscitore per $A_{TM}$.\hfill$\blacksquare$\acc 
\subsection{Diagonalizzazione}
La diagonalizzazione è una tecnica utilizzata per dimostrare alcune semplici proprietà degli insiemi. Sappiamo un insieme generico $A$ è numerabile se esiste una biezione da $\N$ a $A$. Ad esempio, l'insieme dei numeri pari $E$ è numerabile perché esiste $$ f:E\rightarrow \N\ \ \ | \ \ \  f(n)=2n$$ che è una biezione.\acc 
\teo{} $\R$ non è numerabile.\acc 
\dimo{} è necessario dimostrare che un sottoinsieme di $\R$ non sia numerabile. Si pone per assurdo che esiste una biezione da $\N$ a $[0,1]$. Sia $f$ tale biezione, ad ogni valore di $\N$ associa un numero reale fra zero ed uno.$$ \begin{matrix}
    f(1)=0.52423\dots\\ 
    f(2)=0.08362\dots\\ 
    f(3)=0.92841\dots\\ \vdots
\end{matrix}$$
Si definisce un numero $d$ tale che, la parte intera di $d$ è 0, e l'$i$-esima cifra decimale di $d$ è diversa dall'$i$-esima cifra decimale di $f(i)$.\begin{center}
    \includegraphics[width=0.7\textwidth ]{images/realiNonNumerabili..pdf}
\end{center}
Essendo $f$ biettiva per ipotesi, $\exists k \in \N | f(k)=d$, ma in questo modo la $k$-esima cifra di $f(k)$ è identica alla $k$-esima cifra di $d$, dato che $d=f(k)$. Ma allora $d$ non è contenuto nell'immagine di $f$, quindi $f$ non è biettiva. \hfill$\blacksquare$\acc 
\prop{} L'insieme delle stringhe binarie (o non) di lunghezza infinita non è numerabile.\acc 
La dimostrazione sarà omessa, ma si può dimostrare con la diagonalizzazione. Si presenteranno ora due proposizioni che saranno utili a dimostrare un risultato fondamentale.\acc 
\prop{1} L'insieme $TM$ di tutte le macchine di turing è numerabile.\acc 
\dimo{1} Si consideri $\Sigma^*$, ossia l'insieme di tutte le stringhe finite (su un generico alfabeto $\Sigma$). Ogni macchina di turing $M$ può essere codificata con una stringa finita. Quindi 
$$ \{<M>\ |\ M\in TM\}\subseteq \Sigma^*$$
Si definisce una relazione d'ordine $<_l$ che rappresenta l'ordinamento lessicografico per tutte le 
stringhe di $l$ caratteri. Si estende poi  a tutte le stringhe con la relazione $<^*$ definita 
$$ x<^*y\iff 
    |x|<|y| \lor (|x|=|y|\land x<_l y)
$$
$<^*$ è una relazione d'ordine totale. Si definisce $f:\N\rightarrow \Sigma^*$ 
$$ f(i)=i\text{-esimo elemento di $\Sigma^*$ secondo l'ordinamento }<^*$$
$f$ è una biezione dato che per ogni $i\in\N$ si può associare un elemento di $\Sigma^*$. Quindi $\Sigma^*$ è numerabile, e di conseguenza $\{<M>\ |\ M\in TM\}$ è numerabile. \hfill$\blacksquare$\acc 
\prop{2} L'insieme dei linguaggi non è numerabile.\acc 
\dimo{2} Sia $\mathcal{L}$ l'insieme di tutti i linguaggi le cui stringhe sono binarie. Un generico linguaggio $L$ è in $\Sigma^*$ dove $\Sigma=\{0,1\}$. 
Definisco per ogni linguaggio $L$ una stringa binaria $\mathcal{X}_L$ di lunghezza infinita tale che 
\begin{center}
    l'$i$-esimo carattere di $\mathcal{X}_L$ è 1 se l'$i$-esimo elemento di $\Sigma^*$ (secondo un arbitrario ordinamento) è in $L$. Altrimenti è 0. 
\end{center}
Questa applicazione $f:\mathcal{L}\rightarrow\mathcal{B}$ è biettiva, $\mathcal{B}$ è l'insieme delle stringhe di lunghezza infinita, che sappiamo non essere numerabile, ma allora anche $\mathcal{L}$, ossia un sotto-insieme del totale dei linguaggi non è numerabile, implica che l'insieme dei linguaggi non è numerabile. \hfill$\blacksquare$\acc 
\teo{ fondamentale } Esistono linguaggi che non sono turing riconoscibili. \acc 
\dimo{} Ogni macchina di turing ha ad esso associato un solo linguaggio. Essendo l'insieme dei linguaggi non numerabile, ed essendo l'insieme delle TM numerabile, esiste almeno un linguaggio che non si può associare a nessuna TM $\implies$ esistono linguaggi non riconoscibili.\hfill$\blacksquare$\acc 
\teo{} $A_{TM}$ \textbf{non} è turing-decidibile.\acc 
\dimo{} Si utilizzerà la diagonalizzazione. Si pone per assurdo che esiste una TM $H$ che è un decisore per $A_{TM}$, ovvero 
$$ H(<M,w>)=\begin{cases}
    \text{accetta se $M$ accetta $w$}\\ 
    \text{rifiuta se $M$ rifiuta $w$}
\end{cases}$$
Si definisce una nuova TM $D$ che prende come input una TM e si comporta nel seguente modo \begin{enumerate}
    \item Su input $<M>$, esegue $H(<M,<M>>)$
    \item Se $H$ accetta, $D$ rifiuta. Se $H$ rifiuta, $D$ accetta.
\end{enumerate}
Ricordando il comportamento di $H$, si ha la seguente situazione "transitiva"
\begin{center}
    \includegraphics[width=0.6\textwidth ]{images/ATMnonDec.pdf}
\end{center}
Se alla TM $D$ si da in input $<D>$ si ricade nella seguente situazione.
\begin{center}
    \includegraphics[width=0.6\textwidth ]{images/ATMnonDec2.pdf}
\end{center}
Ma allora $D$ accetta $<D>$ solo e soltanto se $D$ rifiuta $<D>$. È una palese contraddizione, è quindi impossibile che $H$ sia un decisore per $A_{TM}$.
\hfill$\blacksquare$\acc 
\defi{(co-turing Riconoscibilità)} $L$ è co-turing riconoscibile (o più semplicemente, co-riconoscibile) se $\overline{L}$ è turing riconoscibile.\acc 
\teo{} $L$ è decidibile se e solo se $L$ è sia riconoscibile che co-riconoscibile.\acc 
\dimo{} Si dimostrano separatamente le due implicazioni. \\ 
\boxedMath{$\implies$} Per ipotesi $L$ è decidibile, allora anche $\overline{L}$ lo è, quindi sia $L$ che $\overline{L}$ sono riconoscibili. \\ 
\boxedMath{$\impliedby$} Per ipotesi $L$ ed $\overline{L}$ sono riconoscibili. Siano $M_1,M_2$ le TM che li riconoscono. Si costruisce una nuova TM $M$ tale che \begin{enumerate}
    \item su input $w$ esegue parallelamente $w$ su $M_1$ ed $M_2$
    \item se $M_1$ accetta, $M$ accetta. se $M_2$ accetta, $M$ rifiuta.
\end{enumerate}
$\forall w$ si ha che $w\in L \lor w\in \overline{L}$, solamente una fra $M_1,M_2$ può accettare $w$, quindi $M$ non può mai andare in loop $\implies$ $M$ è un decisore per $L$.\hfill$\blacksquare$\acc 
\textbf{Corollario} : $\overline{A_{TM}}$ non è turing riconoscibile.\flowerLine 
\section{Riducibilità}
La riducibilità è una tecnica che permette di dimostrare la non decidibilità/non riconoscibilità di molti linguaggi. Informalmente, dati due linguaggi $A$ e $B$,  diremo che $A$ \textbf{si riduce a} $B$, e denoteremo $$ A\le B$$ Se esiste un algoritmo (una TM) per $B$ che risolve (decide) anche $A$.\begin{quotation}
    trovare una soluzione per $A$ \textbf{non può essere} più difficile di trovare una soluzione per $B$
\end{quotation}
Si consideri il seguente linguaggio 
$$ HALT_{TM}=\{<M,w> \ | \ M\in TM\land M(w)\text{ termina} \}$$
Data una TM si deve stabilire se termina o continua all'infinito su un certo input.\acc 
\teo{(Halting Problem)} $HALT_{TM}$ non è decidibile.\acc 
\dimo{} Per assurdo, sia $R$ un decisore per $HALT_{TM}$. Si definisce una TM $S$ tale che \begin{enumerate}
    \item prende come input una TM ed una stringa $<M,w>\in A_{TM}$
    \item esegue $R$ su $<M,w>$
    \item se $R$ accetta (quindi si ha la certezza che $M$ termina), $S$ simula $M$ su $w$ ed accetta solamente se $M$ accetta $w$.
\end{enumerate}
Quindi $S$ è un decisore per tutte le coppie $<M,w>$ ossia $A_{TM}$. Ma $A_{TM}$ non è decidibile, è quindi impossibile che esista un decisore per $HALT_{TM}$.\hfill$\blacksquare$\acc 
Si consideri 
$$ E_{TM}=\{<M>\ | \ M\in DFA \land L(M)=\emptyset\}$$
\prop{} $E_{TM}$ non è decidibile.\acc 
\dimo{} Si pone per assurdo che $R$ decide $E_{TM}$. Si definisce una nuova TM $S$ che prende come input gli elementi di $A_{TM}$ e si comporta come segue \begin{enumerate}
    \item costruisce una nuova TM $M'$, il cui comportamento è il seguente\begin{enumerate}
        \item prende come input una stringa $x$
        \item se $x\ne w$ rifiuta 
        \item se $x=w$ esegue $M$ su input $w$
        \item $M'$ accetta se $M$ accetta
    \end{enumerate}
    \item esegue $R$ su input $<M'>$
    \item se $R$ accetta, $S$ rifiuta. Se $R$ rifiuta, $S$ accetta.
\end{enumerate}
Essendo $R$ un decisore, allora $S$ è un decisore di $A_{TM}$, ma $A_{TM}$ non è decidibile, è quindi impossibile che esista un decisore per $E_{TM}$.\hfill$\blacksquare$\acc 
Si vuole formalizzare il concetto di riduzione, intuitivamente si può dire che la riduzione è definita da una funzione, che negli obiettivi di questo corso sarà calcolata da una TM.\acc 
\defi{} Una funzione $f:\Sigma^*\rightarrow \Sigma^*$ si dice \textbf{calcolabile} se esiste una TM che su input $w$ termina scrivendo sul nastro $f(w)$. Spesso tale TM ha un nastro dedicato per l'output della funzione.\acc 
Nell'halting problem è stata definita una TM a tal scopo. La definizione di riduzione che verrà data non rappresenta il concetto in senso lato, bensì è la sua formulazione più semplice detta \textit{mapping reduction}, sufficiente per gli obiettivi di questo corso.\acc 
\defi{(Riduzione)} Siano $A$ e $B$ due generici linguaggi. Si dice che $A$ è \textbf{riducibile} a $B$, e si denota 
$$ A\le_m B$$
Se \textit{esiste} una funzione calcolabile $f:\Sigma^*\rightarrow \Sigma^*$ tale che 
$$ \forall w\in\Sigma^*, \ \ w\in A \iff f(w)\in B$$
Il seguente teorema è di fondamentale importanza quando si vuole dimostrare la non decidibilità di un linguaggio.\acc 
\teo{} Se $A\le_m B$ e $B$ è decidibile, allora  anche $A$ è decidibile.\acc 
\dimo{} Sia $M_B$ un decisore per $B$, si definisce una TM $A_M$ tale che \begin{enumerate}
    \item su input $w\in\Sigma^*$, calcola $f(w)$
    \item esegue $M_B$ su input $f(w)$ 
    \item Se $M_B(f(w))$ accetta, $M_A$ accetta, altrimenti rifiuta.
\end{enumerate}
Essendo che, per ogni $w$ si ha che $w\in A$ solo se $f(w)\in B$, $M_A$ potrà sempre o accettare o rifiutare (data l'ipotesi che $M_B$ è un decisore), quindi  $M_A$ è un decisore per $A$.
\hfill$\blacksquare$\acc 
\textbf{Corollario} : Se $A\le_m B$ e $A$ non è decidibile, allora  anche $B$ non è decidibile.
\subsection{Applicazioni della Riducibilità}
In questa sezione verranno trattati differenti esempi di come la \textit{riducibilità} può essere usata per dimostrare la non decidibilità di molti linguaggi, e verranno presentati \textit{alcuni risultati} che estendono le applicazioni, in particolare, quando si trattano linguaggi non riconoscibili. È \underline{consigliato} allo studente che deve sostenere l'esame di porre particolare attenzione a questa sezione.
\esempio{1}
Si vuole dimostrare in maniera formale l'indecidibilità di $HALT_{TM}$. La notazione $M(w)\ne\infty$ indicia che la TM $M$ su input $w$ non va in loop, e termina. 
$$ HALT_{TM}=\{<M,w> \ | \ M\in TM\land M(w)\ne\infty \}$$
Basta dimostrare che $$ A_{TM}\le_m HALT_{TM}$$
Essendo $A_{TM}$ indecidibile, ciò comporterebbe anche l'indecidibilità di $HALT_{TM}$. Si definisce una funzione $\ridFunc$ tale che 
$$ \forall <M,w>\in\Sigma^*, \ \ \ <M,w>\in A_{TM}\iff f(<M,w>)\in HALT_{TM}$$
$f$ deve essere calcolabile, quindi si definisce una TM che calcola $f$, ed esegue i seguenti passi 
\begin{enumerate}
    \item Su input $<M,w>$ costruisce una TM ausiliaria $M'$
    \item Tale $M'$ su un generico input $x$, esegue i seguenti passi\begin{enumerate}
        \item Esegue $M$ su input $x$
        \item Se $M(x)$ accetta, allora $M'$ accetta, se $M(x)$ rifiuta, entra volontariamente in uno stato di loop.
    \end{enumerate}
    \item L'output della funzione sarà $<M',w>$
\end{enumerate}
\boxedMath{$\implies$} se $<M,w>\in A_{TM}$, allora $M$ accetta $w$, pertanto $M'$ a sua volta accetterà $w$, terminando. Si ha quindi che $<M',w>\in HALT_{TM}$. La prima implicazione della riducibilità è dimostrata. \acc 
\boxedMath{$\impliedby$} se $<M,w>\notin A_{TM}$, allora $M(w)$ o rifiuta, o va in loop. \begin{itemize}
    \item Se $M(w)$ rifiuta, per definizione, $M'$ entra in uno stato di loop. 
    \item Se $M(w)$ va in loop, anche $M'$ (che la esegue) va in loop.
\end{itemize}
Si ha quindi che, se  $<M,w>\notin A_{TM}$, la TM $M'$ va in loop su $w$, quindi non termina, allora 
$$f(<M,w>)=<M',w>\notin HALT_{TM}$$La seconda implicazione della riducibilità è dimostrata. Quindi 
$ A_{TM}\le_m HALT_{TM}$. Ne consegue che $HALT_{TM}$ è indecidibile.
\esempio{2}
Si vuole dimostrare l'indecidibilità di $EQ_{TM}$. 
$$ EQ_{TM}=\{<M_1,M_2> \ | \ M_1,M_2\in TM\land L(M_1)=L(M_2) \}$$
Basta dimostrare che $$ E_{TM}\le_m  EQ_{TM}$$Si definisce una funzione $\ridFunc$ tale che 
$$ \forall <M>\in\Sigma^*, \ \ \ <M>\in E_{TM}\iff f(<M>)\in EQ_{TM}$$
$f$ deve essere calcolabile, quindi si definisce una TM che calcola $f$, ed esegue i seguenti passi 
\begin{enumerate}
    \item   Su input $<M>$, definisce una TM ausiliaria $M'$, la cui computazione è descritta come segue \begin{enumerate}
        \item $M'$ su un qualsiasi input $x$ rifiuta sempre.
    \end{enumerate}
    \item L'output della funzione sarà $<M,M'>$
\end{enumerate}
\boxedMath{$\implies$} Se $M\in E_{TM}$, allora $L(M)=\emptyset$. Essendo che $M'$ rifiuta su ogni input, anche il suo linguaggio è vuoto, quindi $L(M)=L(M')$, ne consegue che $<M,M'>\in EQ_{TM}$.
\acc \boxedMath{$\impliedby$} Se $M\notin E_{TM}$, allora $L(M)\ne \emptyset$, Essendo che $M'$ rifiuta su ogni input, il suo linguaggio è vuoto, quindi $L(M)\ne L(M') = \emptyset$, ne consegue che $<M,M'>\notin EQ_{TM}$.\acc 
Risulta naturale chiedersi: È possibile utilizzare la riducibilità per dimostrare anche la non riconoscibilità di un linguaggio? Il seguente teorema risponde a tale domanda. \acc 
\teo{} Se $A\le_m B$ e $B$ è riconoscibile, allora  anche $A$ è riconoscibile.\acc 
\textbf{Corollario} : Se $A\le_m B$ e $A$ non è riconoscibile, allora  anche $B$ non è riconoscibile.\acc 
Essendo che $A_{TM}$ è riconoscibile ma non decidibile, per il teorema sulla co-riconoscibilità è noto che il linguaggio $\overline{A_{TM}}$ non è riconoscibile. È possibile utilizzare $\overline{A_{TM}}$ e la riducibilità per dimostrare la non riconoscibilità di molti linguaggi. $\overline{A_{TM}}$  è però un linguaggio ambiguo con la quale lavorare, risulta più funzionale considerare direttamente $A_{TM}$ grazie al seguente teorema.\acc 
\teo{} Se $A\le_m B$, allora $\overline A\le_m \overline B$.\acc 
\dimo{} La funzione $f$ rimane invariata dato che $ \forall w\in\Sigma^*$ si ha che $$\begin{matrix}
    w\in A\iff f(w)\in B \\ \\ 
    w\in \overline A\iff f(w)\in \overline B 
\end{matrix} $$\hfill$\blacksquare$\acc 
Le conseguenze sono ovvie, se $A\le_m B$ e $\overline A$ non è riconoscibile, allora anche $\overline B$ non è riconoscibile. Per dimostrare la non riconoscibilità di un generico linguaggio $A$, basterà dimostrare $A_{TM}$ si riduce a $\overline A$.
\esempio{3}
Si vuole dimostrare la non riconoscibilità di ${EQ_{TM}}$. 
Basta dimostrare che $$ A_{TM}\le_m  \overline{EQ_{TM}}$$Si definisce una funzione $\ridFunc$ tale che 
$$ \forall <M,w>\in\Sigma^*, \ \ \  <M,w>\in A_{TM}\iff f( <M,w>)\in \overline{EQ_{TM}}$$
In pratica, se $M$ accetta $w$, allora $f(<M,w>)$ deve restituire due TM che hanno linguaggi differenti.  
$f$ deve essere calcolabile, quindi si definisce una TM che calcola $f$, ed esegue i seguenti passi 
\begin{enumerate}
    \item  Su input $<M,w>$, definisce due TM ausiliarie $M_1,M_2$, tali che \begin{itemize}
        \item $M_1$ rifiuta sempre 
        \item $M_2$ accetta solo se $M(w)$ accetta, altrimenti rifiuta.
    \end{itemize}
    \item l'output della funzione è $<M_1,M_2>$
\end{enumerate}
\boxedMath{$\implies$} Se $<M,w>\in A_{TM}$, essendo che $M(w)$ accetta si ha che \begin{itemize}
    \item $M_1$ rifiuta sempre 
    \item  $M_2$ accetta sempre 
\end{itemize}
Quindi $L(M_1)\ne L(M_2)\implies <M_1,M_2>\in  \overline{EQ_{TM}}$.
\acc \boxedMath{$\impliedby$} Se $<M,w>\notin A_{TM}$, essendo che $M(w)$ rifiuta si ha che \begin{itemize}
    \item $M_1$ rifiuta sempre 
    \item  $M_2$ rifiuta sempre 
\end{itemize}
Quindi $L(M_1)=L(M_2)=\emptyset \implies <M_1,M_2>\in  {EQ_{TM}}\implies <M_1,M_2>\notin  \overline{EQ_{TM}}$. Ne consegue che $EQ_{TM}$ non è riconoscibile.
\esempio{4}
Si vuole dimostrare la non decidibilità di $L$ definito come segue 
$$ L=\{<M>\ | \ M\in TM \land w\in L(M)\implies w\in \{0,1\}^*\land |w|\%2=1\}$$ 
Ossia il linguaggio di tutte le TM che accettano esclusivamente stringhe binarie di lunghezza dispari.
Basta dimostrare che $$ A_{TM}\le_m  L$$Si definisce una funzione $\ridFunc$ tale che 
$$ \forall <M,w>\in\Sigma^*, \ \ \  <M,w>\in A_{TM}\iff f( <M,w>)\in L$$
$f$ deve essere calcolabile, quindi si definisce una TM che calcola $f$, ed esegue i seguenti passi 
\begin{enumerate}
    \item  Su input $<M,w>$, definisce una TM ausiliaria $M'$, tale che:
    \item Su input $x$, se $|x|\%2=0$ rifiuta. 
    \item Altrimenti, accetta se e solo se $M$ accetta $w$.
    \item L'output sarà $M'$.
\end{enumerate}
\boxedMath{$\implies$} Se $<M,w>\in A_{TM}$, essendo che $M(w)$ accetta si ha che 
$M'$ accetterà tutte e sole le stringhe binarie di lunghezza dispari, quindi $f(<M,w>)=M'\in L$.
\acc \boxedMath{$\impliedby$} Se $<M,w>\notin A_{TM}$, $M'$ rifiuterà a prescindere, si avrà che 
$L(M')=\emptyset\implies M'\notin L$. Ne consegue che $L$ è indecidibile.
\esempio{5}
Si vuole dimostrare la non decidibilità di $L$ definito come segue 
$$ L=\{<M>\ | \ M\in TM \land L(M)=\{0^n1^n0^n,\ n\ge 0\}\}$$ 
Basta dimostrare che $$ A_{TM}\le_m  L$$Si definisce una funzione $\ridFunc$ tale che 
$$ \forall <M,w>\in\Sigma^*, \ \ \  <M,w>\in A_{TM}\iff f( <M,w>)\in L$$
$f$ deve essere calcolabile, quindi si definisce una TM che calcola $f$, ed esegue i seguenti passi 
\begin{enumerate}
    \item  Su input $<M,w>$, definisce una TM ausiliaria $M'$, tale che:
    \item Su input $x$, esegue $M(w)$. 
    \item Se $M(w)$ accetta, $M'$ accetta solo se $x\in\{0^n1^n0^n,\ n\ge 0\}$, altrimenti rifiuta.
    \item Se $M(w)$ rifiuta, $M'$ rifiuta a prescindere. 
    \item L'output sarà $M'$.
\end{enumerate}
\boxedMath{$\implies$} Se $<M,w>\in A_{TM}$, essendo che $M(w)$ accetta si ha che 
$M'$ accetterà solo se $$x\in\{0^n1^n0^n,\ n\ge 0\}$$quindi $L(M')=\{0^n1^n0^n,\ n\ge 0\} \implies f(<M,w>)=M'\in L$.
\acc \boxedMath{$\impliedby$} Se $<M,w>\notin A_{TM}$, $M'$ rifiuterà a prescindere (o andrà in loop nel caso $M(w)$ vada in loop), si avrà che 
$L(M')=\emptyset\ne \{0^n1^n0^n,\ n\ge 0\} \implies M'\notin L$. Ne consegue che $L$ è indecidibile.
\esempio{6}
Si vuole dimostrare la non decidibilità di $L$ definito come segue 
$$ L=\{<M>\ | \ M\in TM \land L(M)\supseteq \{w\ | \ w\text{ inizia con uno }0\}\}$$ 
Ossia il linguaggio di tutte le TM il cui linguaggio contiene ogni stringa che inizia con 0.
Basta dimostrare che $$ A_{TM}\le_m  L$$Si definisce una funzione $\ridFunc$ tale che 
$$ \forall <M,w>\in\Sigma^*, \ \ \  <M,w>\in A_{TM}\iff f( <M,w>)\in L$$
$f$ deve essere calcolabile, quindi si definisce una TM che calcola $f$, ed esegue i seguenti passi 
\begin{enumerate}
    \item  Su input $<M,w>$, definisce una TM ausiliaria $M'$, tale che:
    \item su ogni input $x$ esegue $M(w)$, e se questa accetta, $M'$ accetta, altrimenti rifiuta.
    \item L'output sarà $M'$.
\end{enumerate}
\boxedMath{$\implies$} Se $<M,w>\in A_{TM}$, essendo che $M(w)$ accetta si ha che 
$M'$ accetterà ogni input, quindi $L(M')=\Sigma^*$ ed essendo l'insieme di ogni stringa, conterrà anche ogni stringa che inizia con zero, allora $<M'>\in L$.
\acc \boxedMath{$\impliedby$} Se $<M,w>\notin A_{TM}$, $M'$ rifiuterà  (o andrà in loop nel caso $M(w)$ vada in loop), si avrà che 
$L(M')=\emptyset$, quindi non conterrà nemmeno una stringa che inizia con 0, allora  $<M'>\notin L$.
Ne consegue che $L$ è indecidibile.
\esempio{7}
Si vuole dimostrare la non decidibilità di $U$ definito come segue 
$$ U=\{<T,T'> \| \ T,T'\in TM\land L(T)\cup L(T')=\Sigma^*\}$$ 
Ossia il linguaggio di tutte le coppie di TM la cui unione dei linguaggi equivale all'insieme di tutte le stringhe.
Basta dimostrare che $$ A_{TM}\le_m  U$$Si definisce una funzione $\ridFunc$ tale che 
$$ \forall <M,w>\in\Sigma^*, \ \ \  <M,w>\in A_{TM}\iff f( <M,w>)\in U$$
$f$ deve essere calcolabile, quindi si definisce una TM che calcola $f$, ed esegue i seguenti passi 
\begin{enumerate}
    \item  Su input $<M,w>$, definisce una TM ausiliaria $T$ che rifiuta per ogni input $x$. 
    \item Definisce poi una TM $T'$ che accetta solo se $M(w)$ accetta. 
    \item L'output è $<T,T'>$
\end{enumerate}
\boxedMath{$\implies$} Se $<M,w>\in A_{TM}$, essendo che $M(w)$ accetta si ha che
$T'$ accetterà ogni stringa ed il suo linguaggio sarà uguale a tutte le stringhe. $T$ invece avrà linguaggio vuoto. $L(T)\cup L(T')=\emptyset\cup \Sigma^*=\Sigma^*\implies <T,T'>\in U$.
\acc \boxedMath{$\impliedby$} Se $<M,w>\notin A_{TM}$, $M'$ rifiuterà  (o andrà in loop nel caso $M(w)$ vada in loop), si avrà che 
$L(T')=\emptyset$, inoltre anche $T$ ha linguaggio vuoto, quindi $L(T)\cup L(T')=\emptyset\ne \Sigma^*\implies <T,T'>\notin U$. Ne consegue che $U$ è indecidibile.
\section{I Teoremi di Incompletezza}
I risultati riguardanti i teoremi di Gödel possono essere ottenuti utilizzando le TM. Negli anni è sempre stata presente l'esigenza di poter dimostrare le proposizioni matematiche, già nel 300 a.c. Euclide stipulò gli assiomi della geometria piana.\acc 
Nel XIX secolo i matematici si sono interrogati riguardo la formalità degli assiomi di Euclide, in particolare, è iniziata la ricerca verso una formulazione più rigorosa della matematica che ha anche sancito la nascita della logica, che fornisce una serie di connettivi, con i quali si possono derivare gli assiomi ed ottenere nuove proposizioni.\acc 
Si definisce \textbf{sistema di prova} un insieme di assiomi uniti alla logica del primo ordine, con la quale è possibile dimostrare ogni proposizione derivante da essi, anche in maniera automatica (calcolabile da un algoritmo).\acc 
Denoteremo $\Pi$ un generico sistema di prova, in particolare si può definire in funzione della calcolabilità. Riguardo $\Pi$ si ha che \begin{itemize}
    \item Per ogni affermazione $x$, esiste una codifica $<x>$ sottoforma di stringa. 
    \item Per ogni dimostrazione $w$, esiste una codifica $<w>$ sottoforma di stringa.  
    \item Esiste una TM $V$ che è un decisore, per cui $V(<x,w>)$ accetta se e solo se $w$ è una dimostrazione di $x$.
\end{itemize}
\defi{} un'affermazione $x$ è \textbf{dimostrabile} se $\exists w$ tale che $V(<x,w>)$ accetta.\acc 
\defi{} un'affermazione $x$ è \textbf{indipendente} se, ne $x$ ne $\overline x$ sono dimostrabili.\acc 
Un sistema di prova $\Pi$ può godere di differenti proprietà\begin{itemize}
    \item $\Pi$ è \textbf{consistente} se, per ogni affermazione $x$, al massimo 1 fra $x$ e $\overline x$ è vera, ossia, non esiste un affermazione vera, la cui negata è a sua volta vera. 
    \item $\Pi$ è \textbf{valido} se ogni affermazione dimostrabile è vera. 
    \item $\Pi$ è \textbf{completo} se, per ogni affermazione $x$, almeno 1 fra $x$ e $\overline x$ è vera. 
    \item $\Pi$ è \textbf{incompleto} se esiste un affermazione $x$ indipendente.
\end{itemize} 
\textbf{Osservazione} : Se $\Pi$ è valido, allora è anche consistente. Se $\Pi$ è valido e completo, per ogni affermazione $x$, esattamente una fra $x$ e $\overline x$ è vera, e ciò che è dimostrabile è anche vero.\acc 
I teoremi di incompletezza riguardano tali proprietà, il primo, afferma che un sistema di prova consistente è necessariamente incompleto, ossia che esiste almeno un'affermazione indipendente, che non si può dimostrare. Il secondo, afferma, per un sistema di prova $\Pi$, che l'affermazione 
\begin{quote}
    $x=$ "$\Pi$ è consistente"
\end{quote}
Non è dimostrabile. Prima di dimostrare i due teoremi, è necessario introdurre due TM relative ad un generico sistema di prova $\Pi$. \acc 
\defi{} Dato un sistema $\Pi$, si definisce una TM $P$, il cui comportamento è descritto come segue\begin{enumerate}
    \item Data in input un affermazione $<x>$
    \item Controlla per ogni $k\in 1,2,3\dots $\begin{itemize}
        \item Per ogni possibile stringa $w$ tale che $|w|=k$
        \item Se $V(<x,w>)$ accetta, $P$ scrive sul nastro di output $<w>$.
    \end{itemize}
\end{enumerate}
si definisce poi una TM $R$, il cui comportamento è descritto come segue\begin{enumerate}
    \item Data in input un affermazione $<x>$
    \item Controlla per ogni $k\in 1,2,3\dots $\begin{itemize}
        \item Per ogni possibile stringa $w$ tale che $|w|=k$
        \item Se $V(<x,w>)$ accetta, $R$ accetta, altrimenti rifiuta.
    \end{itemize}
\end{enumerate}
Si definisce il linguaggio 
$$ L_{provable}=\{<x>\ | \ x \text{ è dimostrabile }\}$$
Alcune osservazioni
\begin{itemize}
    \item Se $\Pi$ è valido, l'output di $P$ è vero, anche se non è assicurato che $R$ termini
    \item Se $\Pi$ è completo, allora $R$ è un decisore per $ L_{provable}$. In particolare, per ogni $<x>$, se $x$ è vera, $R(<x>)$ accetta, altrimenti $R(<x>)$ rifiuta
\end{itemize}
Verrà considerato adesso un caso speciale del primo teorema di incompletezza, abbiamo visto come un sistema valido è anche consistente. Il primo teorema di Gödel riguarda i sistemi consistenti, il caso speciale che verrà trattato riguarda i sistemi validi. \acc 
\teo{(caso speciale)} Sia $\Pi$ un generico sistema di prova. Se $\Pi$ è valido, allora non è completo.\acc 
\dimo{} Si pone per assurdo che $\Pi$ è valido e completo, si definisce una TM $D$ per il linguaggio 
$$ HALT_{TM}=\{<M,w> \ | \ M\in TM\land M(w)\ne \infty \}$$
Essendo $\Pi$ completo, la TM $R$ è un decisore per $ L_{provable}$, quindi, data una generica TM $M$ ed una generica stringa $w$, l'affermazione  
$$ <x>="M(w)\ne \infty"$$
È decisa da $R$. La TM $D$ è definita come segue 
$$ D(<M,w>)=R(<"M(w)\ne \infty">)$$
Ma allora $D$ è chiaramente un decisore per $HALT_{TM}$, ma questo linguaggio è indecidibile, è quindi impossibile che $R$ sia un decisore per $ L_{provable}$, allora $\Pi$ non può essere completo.
\hfill$\blacksquare$\acc 
È necessario sapere che ZFC è il sistema di prova per la teoria degli insiemi, comprende gli assiomi standard della teoria assiomatica degli insiemi su cui, insieme con l'assioma di scelta, si basa tutta la matematica ordinaria secondo formulazioni moderne. In passato, si pensava che ogni affermazione in ogni branca della matematica potesse essere dimostrata a partire da tali assiomi.\acc 
In particolare, se $M$ è una generica TM, e $w$ è una stringa, se $M(w)$ termina, la proposizione \begin{quote}
    "$M$ termina con input $w$"
\end{quote}
È dimostrabile \textit{canonicamente} tramite gli assiomi del sistema ZFC, dato che basta controllare la traccia di esecuzione di $M$ su $w$, e constatare che ad un certo punto la TM si ferma. Ovviamente ciò è possibile se si da come ipotesi che $M$ non vada in loop su $w$.
\acc 
Si consideri la TM mostrata in figura \begin{center}
    \begin{tikzpicture} [node distance = 4cm, on grid, auto]
        \node (q0)[state, initial, left] {$q_0$};
        \node (B)[state, right = of q0] {$B$};
        \node (C)[state, right = of B] {$C$};
        \node (rej)[state, below = of B] {$q_{rej}$};
        \node (acc)[state,accepting, below = of C] {$q_{acc}$};
        \path [-stealth, thick]
        (B) edge [loop above]  node {$1\rightarrow y, R$}()
        (q0) edge  node {$0\rightarrow x,R$}  (B)
        (q0) edge [bend right]  node[left] {$\begin{matrix}\\1\rightarrow 1,R \ \ \\\blank\rightarrow \blank,R\ \ \end{matrix}$}  (rej)
        (B) edge  node {$0\rightarrow x,R$}  (C)
        (C) edge  node {$\blank\rightarrow \blank,R$}  (acc)
        (C) edge  node {$\begin{matrix}1\rightarrow 1,R\\0\rightarrow 0,R\end{matrix}$}  (rej)
        (B) edge  node {$\blank\rightarrow \blank,R$}  (rej);
    \end{tikzpicture}
\end{center}
È chiaro che su input $w=0 \ 0\ \blank$ la TM termina, per dimostrarlo è necessario mostrare la traccia di esecuzione della macchina sulla stringa data.\acc 
\color{sapienza}\textbf{Lemma} : \color{black} Sia $M$ una generica TM e $w$ una stringa, se $M$ termina su $w$, l'affermazione $<M(w)\ne \infty>$ è dimostrabile.\acc 
\teo{di Incompletezza I} Sia $\Pi$ un generico sistema di prova. Se $\Pi$ è consistente, allora non è completo.\acc 
\dimo{} Si definisce un'apposita TM $D$ il cui comportamento è descritto come segue \begin{enumerate}
    \item su input $<M>$, dove $M$ è una macchina di Turing 
    \item si controlla per ogni $k\in1,2,3,\dots$\begin{itemize}
        \item per ogni stringa $w$ tale che $|w|=k$
        \item Se $w$ è una prova per l'affermazione "$M(<M>)\ne \infty$", allora $D$ si sposta volontariamente in uno stato $q_L$ di loop  
        \item Se $w$ è una prova per l'affermazione "$M(<M>)=\infty$", allora $D$ termina. 
    \end{itemize}
\end{enumerate}
Si applica la diagonalizzazione e si considera il caso particolare $D(<D>)$. Essendo $\Pi$ per ipotesi consistente, si ha che al massimo un'affermazione  fra $$ \begin{cases}
    D(<D>)=\infty\\ 
    D(<D>)\ne \infty
\end{cases}$$
è dimostrabile. Si considerano i due casi separatamente\begin{itemize}
    \item Se "$D(<D>)=\infty$" è dimostrabile, allora, per costruzione di $D$, essa stessa con input se stessa, ossia $D(<D>)$ termina, ossia $$D(<D>)\ne\infty$$ allora per il \textit{Lemma} esiste una dimostrazione per $D(<D>)\ne\infty$, quindi $D(<D>)\ne\infty$ è dimostrabile. Si ha che sia 
    $$ \begin{cases}
    D(<D>)=\infty\\ 
    D(<D>)\ne \infty
\end{cases}$$
sono entrambe dimostrabili, quindi $\Pi$ non è consistente.
    \item Se "$D(<D>)\ne\infty$" è dimostrabile, allora per costruzione $D(<D>)$ durante l'esecuzione troverà tale dimostrazione $w$ ed entrerà nello stato di loop $q_L$, ma allora, tale traccia di computazione è proprio una dimostrazione dell'affermazione 
    $$D(<D>)=\infty $$
    Si ha che sia 
    $$ \begin{cases} 
    D(<D>)=\infty\\ 
    D(<D>)\ne \infty
\end{cases}$$
sono entrambe dimostrabili, quindi $\Pi$ non è consistente.  
\end{itemize}
In conclusione, si è dimostrato che il generico sistema di prova $\Pi$ non può essere sia completo che consistente.\hfill$\blacksquare$\acc 
\teo{di Incompletezza II} Sia $\Pi$ un sistema di prova \textit{consistente}, l'affermazione \begin{quote}
    "$\Pi$ è consistente"
\end{quote}
Non è dimostrabile. \acc 
\dimo{} Si consideri la TM $D$ definita nella dimostrazione precedente, si è visto come questa, non riesca a trovare una dimostrazione per nessuna delle due affermazioni 
$$ \begin{cases}
    D(<D>)=\infty\\ 
    D(<D>)\ne \infty
\end{cases}$$
Ciò, signigica che $D(<D>)$ non trovando una dimostrazione, non termina e va in loop, ma allora l'affermazione $D(<D>)=\infty$ si può codificare ed è vera. Ne consegue che 
$$ \Pi \text{ consistente }\implies D(<D>)=\infty$$
Ma come si è mostrato in precedenza, $D(<D>)=\infty$ è un affermazione non dimostrabile, quindi è necessariamente vero che anche l'affermazione che la implica, ossia "$\Pi \text{ consistente }$" sia indimostrabile.\hfill$\blacksquare$
\sapbox{Esistono affermazioni vere e non dimostrabili}
\chapter{Complessità Temporale}
Questo capitolo si occuperà di trattare le risorse necessarie alle TM per risolvere i problemi, con risorse, si definiscono \begin{itemize}
    \item tempo 
    \item spazio 
    \item numero di processori 
    \item randomicità 
    \item altre
\end{itemize}
Ci si occuperà della trattazione di tempo e spazio, la domanda scatenante la teoria della complessità è: "\textit{quante risorse occorrono per decidere i problemi?}".\acc 
I problemi che si affrontano sono identificabili in delle \textit{classi} che li contengono, e sono classificati in base al tempo (o spazio) necessario per risolverli. La domanda fondamentale della teoria della complessità (che verrà formalizzata in seguito) è \begin{quote}
    trovare una soluzione ad un problema, è tanto difficile quanto verificare che una soluzione data sia corretta?
\end{quote}
Un altra domanda cardine è\begin{quote}
    se è possibile risolvere un problema utilizzando poco spazio, è possibile risolverlo anche utilizzando poco tempo?
\end{quote}
\defi{(costo computazionale)} : Sia $M$ un decisore, il \textbf{tempo di esecuzione} di $M$ è una funzione $t:\N\rightarrow \N$ definita come segue 
$$ t(n)=\max_{\begin{matrix}x\in\Sigma^* \land \\ |x|=n\end{matrix}}(\text{step di computazione di }M)$$
Sia $x$ la stringa in input di lunghezza $n$ che richiede il maggior numero di passi di computazione per essere decisa. Tale numero di passi di computazione, è proprio il costo computazionale di $M$ su un input di lunghezza $n$. È centrale nella teoria della complessità lo studio di $t$ al variare di $n$.\acc 
\defi{(O-grande)} : Siano $f$ e $g$ due funzioni definite su un generico sottoinsieme di $\R$, si dice che $$ f(x)\in O(g(x))$$ 
se e solo se 
$$ \exists x_0\in \R,\ c>0 \ \text{ tale che } \ |f(x)|\le c|g(x)| \ \forall x>x_0$$
\textbf{Notazione} : Se $f(x)\in O(g(x))$, verrà scritto $f(x)=O(g(x))$.\acc 
Nei capitoli precedenti sono state considerate delle varianti della TM, che possono essere simulato e rese equivalenti ad una TM normale. Tali varianti vanno rivalutate da un punto di vista della risorsa del tempo. 
\section{Classi di Complessità}\label{classiComplessità}
\textbf{Esempio} : Si consideri la TM che ad ogni passo di computazione può rimanere ferma sul nastro, la cui funzione di transizione è definita tale che 
$$ \delta:Q\times\Gamma \rightarrow Q\times\Gamma\times \{L,R,S\}$$
Questa poteva essere resa equivalente ad una TM classica, aggiungendo un semplice stato in cui la TM simula l'azione $S$ spostandosi a destra e poi a sinistra. È chiaro che una TM classica che simula la TM che può stare ferma, dovrà compiere al più il doppio dei passi computazionali.\acc 
Ne consegue che, se $M$ è una TM che può stare ferma, il cui costo computazionale è $t(n)=O(t(n))$, allora una TM classica che la simula avrà costo computazionale $2t(n)=O(t(n))$.\acc 
\textbf{Esempio} : Si consideri la TM multi nastro, si vuole calcolare l'overhead quando si considera una sua simulazione con TM classica. \begin{center}
    \includegraphics[width=\textwidth ]{images/multinastro.eps}
\end{center}
Si può simulare con una TM classica come esposto nella sezione \ref{TMmultinastro}. Se una TM multi nastro $M$ ha costo computazionale $t(n)$, allora la lunghezza di ogni nastro sarà al massimo $n$. Sia $M'$ la TM classica che simula $M$, l'inizializzazione costa sicuramente $t(n)$ in quanto è necessario considerare tutto il nastro, in oltre, per ogni step di computazione di $M$, $M'$ dovrà considerare tutto il nastro per leggere i caratteri marcati ed eventualmente spostarne il contenuto. Ogni step di computazione su $M'$ ha costo $t(n)$. Ne consegue che la complessità di $M'$ sarà 
$$t(n)+t(n)^2 = O(t(n)^2)$$ Il numero dei nastri quindi influisce (in maniera non trascurabile) sulla complessità.\acc 
\textbf{Esempio} : Il linguaggio delle stringhe palindrome è decidibile in $O(n)$ su una macchina con due nastri, su una TM a singolo nastro può essere deciso in $O(n^2)$.\acc 
\defi{} : Sia $t:\N\rightarrow \N$ una funzione, si definisce 
$$ Dtime(t(n))=\Bigg\{L \ |\ \begin{matrix}\exists M \in TM(\text{singolo nastro}) \text{ tale che }\\ L(M)=L\land M\text{ ha complessità }O(t(n))\end{matrix}\Bigg\}$$
È l'insieme dei linguaggi decidibili da TM che hanno complessità $O(t(n))$. Il linguaggio delle stringhe palindrome appartiene all'insieme $Dtime(n^2)$.\acc 
\defi{(Classe Polinomiale)} Si definisce $P$ l'insieme 
$$ P=\bigcup_{k\in \N}Dtime(t^k)$$
Ovvero, l'insieme di tutti i linguaggi che possono essere decisi in tempo polinomiale. L'insieme delle stringhe palindrome è in $P$. \acc 
\textbf{Osservazione} : La classe $P$ è robusta rispetto le varianti di TM. Se un linguaggio è deciso in tempo polinomiale da una TM non classica, allora è deciso in tempo polinomiale anche da una TM classica.\acc 
\defi{(Classe Esponenziale)} Si definisce $EXP$ l'insieme 
$$ EXP=\bigcup_{k\in \N}Dtime(2^{n^k})$$
Il teorema di \textit{gerarchia dei tempi}, che verrà formalizzato e dimostrato in seguito, afferma che la classe $P$ è un sottoinsieme della classe $EXP$.\begin{center}
    \includegraphics[width=0.6\textwidth ]{images/EXP.pdf}
\end{center}
Esistono linguaggi che sono in $EXP$, ma che, tramite algoritmi intelligenti, si può dimostrare sono anche in $P$. Verranno considerati linguaggi riguardanti i grafi, il cui input delle TM sarà la codifica di un grafo $x=<G=(V,E)>$, la complessità sarà misurata in funzione delle cardinalità di $V$ ed $E$, se è polinomiale in queste, lo sarà anche in funzione della lunghezza della stringa di codifica.\acc 
\textbf{Esempio} : Si consideri il linguaggio 
$$ PATH = \{<G,s,t> \ | \ s,t\in V(G) \land \text{ esiste un cammino }s\rightarrow t\text{ in }G\}$$
Un cammino non è altro che una sequenza di vertici $$v_0\rightarrow V_1 \dots \rightarrow v_l$$ il numero di vertici in ogni cammino è limitato da $n=|V(G)|$, è possibile controllare ogni cammino  del grafo (sono $n^n$) e verificare che ce ne sia uno che inizia in $s$ e termina in $t$. La complessità è $O(n^n)=O(2^{n\log(n)})$, è chiaro che $PATH\in EXP$.\acc 
Si può però considerare una DFS, in particolare una TM che esegue i seguenti passi\begin{itemize}
    \item marca lo stato $s$, ed inizia a fare una passeggiata sul grafo partendo da $s$
    \item marca ogni stato raggiungibile 
    \item se alla fine $t$ è marcato, la TM accetta, altrimenti rifiuta\hfill 
\end{itemize}
Tale TM decide $PATH$ in tempo polinomiale, è chiaro che $PATH\in P$.\acc 
\textbf{Esempio} : Si consideri il linguaggio 
$$ 2COL=\{<G> \ |\ G\text{ è 2-colorabile }\}$$
Un grafo è 2-colorabile se è possibile assegnare un colore ad ogni nodo (da un insieme di due colori possibili) in modo tale che ogni arco connetta sempre e solo nodi di colori diversi.\begin{figure}[h!]
    \centering
    \includegraphics[width=0.5\textwidth ]{images/2COL.pdf}
    \caption{grafo 2-colorato}
\end{figure}
Esistono al più $2^n$ possibili modi di colorare il grafo (ad ognuno degli $n$ nodi si assegna uno fra i due colori), quindi, per ogni possibile colorazione, si può verificare che rispetti la condizione di 2-colorabilità in $O(n)$, ne consegue che esiste una TM che decide $2COL$ in $O(n2^n)$. \acc 
Si consideri il seguente algoritmo\begin{itemize}
    \item si seleziona un nodo e si colora 
    \item si selezionano tutti i nodi adiacenti, e si colorano del colore opposto 
    \item si prosegue per tutti i nodi 
    \item alla fine, quando ogni nodo è colorato, è possibile verificare la condizione di 2-colorabilità in $O(n)$
\end{itemize}
L'algoritmo descritto decide $2COL$ in tempo polinomiale, è chiaro che $2COL\in P$.\acc 
Negli esempi mostrati, dei linguaggi in $EXP$ si sono poi rivelati anche in $P$, ovviamente, non è assicurato che ciò accada. Esistono linguaggi che sono in $EXP$ ma che sicuramente non sono in $P$, ed esistono linguaggi che sono in $EXP$, ma per cui non è stato dimostrato se sono o non sono in $P$.\acc 
\textbf{Esempio} : Si consideri il linguaggio 
$$ 3COL=\{<G> \ |\ G\text{ è 3-colorabile }\}$$
Per un ragionamento analogo al precedente, $3COL\in EXP$, ma non è chiaro se è, oppure non è, in $P$. L'unico risultato dimostrato è il seguente. \acc 
\teo{} $3COL\in Dtime(1.3289^n)$\acc 
\textbf{Esempio} : Un clique in un grafo, è un sottoinsieme totalmente connesso. Si consideri 
$$ 3CLIQUE=\{<G> \ | \ G \text{ contiene un triangolo (clique da 3 nodi)}\}$$\begin{center}
    \includegraphics[width=0.8\textwidth ]{images/clique.eps}
\end{center}
È stato dimostrato che $3CLIQUE$ appartiene a $P$, in particolare 
$$ 3CLIQUE\in Dtime(n^{2.39})$$
Non è ancora stato dimostrato che $ 3CLIQUE\in Dtime(n^{2})$. In generale, non è noto se il linguaggio
$$ kCLIQUE=\{<G> \ | \ G \text{ contiene un clique da $k$ nodi}\}$$ 
Sia o no in $P$, è almeno in $Dtime(n^k)$. La sua appartenenza o no alla classe polinomiale, risulterà nota quando verrà formalizzata la dipendenza di $k$ da $n$. Se $k$ non dovesse dipendere in nessun modo da $n$, allora $kCLIQUE$ sarebbe un elemento di $P$.\flowerLine 
\section{Soddisfacibilità}
Un circuito logico, viene rappresentato con un grafo diretto ed aciclico, in cui sono presenti dei nodi input ($x_1,x_2\dots x_n$ variabili booleane), delle porte logiche (AND, OR, NOT, ed altre) e dei nodi output ($y_1,y_2\dots y_n$ variabili booleane che dipendono dagli input).
\begin{center}
    \includegraphics[width=0.7\textwidth ]{images/circLogico.pdf}
\end{center}
Un circuito logico può anche essere codificato con una stringa, si definisce \textit{formula} un circuito che presenta un solo output. Una formula si identifica quindi con una funzione $C:\{0,1\}^n\rightarrow \{0,1\}$, e denotiamo $<C>$ la sua codifica sotto forma di stringa. Data una formula $C$ ed un'assegnamento delle variabili $x_1,x_2\dots x_n$, una TM può \textit{valutare} il risultato dell'output, si definisce il linguaggio 
$$ CIRC-EVAL = \{<C,x> \ | \ C(x)=1\}$$
Dove $x$ è l'assegnamento delel variabili e $C(x)$ è il valore dell'output dato tale assegnamento. Tale linguaggio è in $P$, tipicamente, se $m$ è il numero delle porte logiche, il costo computazionale della TM che decide $CIRC-EVAL$ è in $O(m\log(m))$.\acc 
\defi{(soddisfacibilità)} Un circuito logico $C$ si dice \textbf{soddisfacibile} se esiste almeno un assegnamento delle variabili in input che renda uguale ad $1$ l'output (o tutte le variabili in output). Di conseguenza, si definisce il linguaggio 
$$ SAT = \{<C> \ | \ \exists x\in \{0,1\}^n \text{ tale che }C(x)=1\}$$
Se un circuito è soddisfacibile, piuttosto che dire che $<C>$ si trova nell'insieme $SAT$ diremo che "$C$ è SAT". Verrà trattato un sotto-insieme di tale linguaggio che considera esclusivamente i circuiti con 1 output 
$$ FORMULA-SAT = \{<C>\in SAT \ | \ C \text{ è una formula}\}$$
Questi due linguaggi sono chiaramente in $EXP$, dato che è possibile controllare ogni singola assegnazione di input (sono $2^n$), e per ognuna di queste è possibile valutarne l'output (in tempo $poly(n)$)\footnote{con $poly(n)$ si denota un costo polinomiale in funzione di $n$}. È quindi possibile definire una TM che decida $ FORMULA-SAT$ in tempo $O(2^npoly(n))$. \acc Non è noto tutt'oggi se sia possibile decidere $FORMULA-SAT$ in tempo polinomiale, è però noto il seguente risultato 
$$ SAT \in P \iff P=NP$$
Questo teorema (fondamentale) verrà trattato e dimostrato formalmente in seguito.\acc 
Definiamo \textit{clausola} una formula logica consistente in una o più variabili in OR fra loro $$ \begin{cases}
    (x_1 \lor x_2 \lor \overline{x_3}) \ \ \ \text{ è una clausola }\\ 
    (x_1 \land x_2 \land \overline{x_3})\ \ \ \text{ non è una clausola }\\ 
    (x_1 \land x_2 \lor \overline{x_3}) \ \ \ \text{ non è una clausola }\\ 
    ( x_2) \ \ \ \text{ è una clausola }\\ 
\end{cases}$$
\defi{(CNF)} una formula logica è in \textit{CNF} (forma normale congiuntiva) se consiste in una serie di clausole in AND fra loro. Ad esempio  
$$ (x_1 \lor x_2 \lor \overline{x_3}) \land (x_4 \lor x_2 \lor {x_1}) \land  
(\overline{x_5} \lor x_2)$$
\prop{} ogni formula logica può essere scritto in forma normale congiuntiva. Si può quindi assumere (senza perdita di generalità) che una formula sia in CNF.\acc 
Si definisce l'insieme 
$$ CNF-SAT=\{<C>\in FORMULA-SAT \ | \ C \text{ è in forma normale congiuntiva}\}$$
è vero che 
$$ CNF-SAT \in EXP \ \ \ \ \ CNF-SAT \in P \iff P=NP$$
Le formule in CNF godono di una proprietà, la complessità computazionale di una TM che ne valuta il risultato è in funzione del numero delle variabili e del numero di clausole.\acc 
Si definisce anche il linguaggio 
$$ K-SAT = \{<C>\in CNF-SAT \ | \ \text{ ogni clausola ha al più }K\text{ variabili } \}$$è vero che 
$$ K-SAT \in EXP \ \ \ \ \ K-SAT \in P \iff P=NP$$
Il migliore algoritmo che è stato definito per decidere $3-SAT$ opera in tempo (circa) $O(1.34^n)$. In generale il costo per decidere $K-SAT$ dipende da $K$. \acc 
\subsection{Complessità di $2-SAT$}
Si consideri il seguente sotto-insieme di $K-SAT$
$$ 2-SAT = \{<C>\in CNF-SAT \ | \ \text{ ogni clausola ha al più 2 variabili } \}$$
Si vuole costruire un \textit{grafo} (diretto) associato ad ogni formula. Una generica clausola di $2-SAT$ coinvolge due variabili ed è del tipo 
$$ (x_i \lor x_j)$$
Quest'ultima implica che 
$$ \begin{matrix}
    \overline{x_i}\rightarrow x_j\\ \overline{x_j}\rightarrow x_i
\end{matrix}$$ 
\defi{} Data una generica formula $\phi(x_1,x_2\dots , x_n)$ in $2-SAT$, si definisce il \textit{grafo associato}  $G$ come segue \begin{itemize}
    \item Vi è un nodo associato ad ogni variabile $x_i$ e al negato $\overline{x_i}$ 
    \item Per ogni clausola $(x_i \lor x_j)\equiv \overline{x_i}\rightarrow x_j$, si aggiunge un arco $(\overline{x_i},x_j)$
\end{itemize}
Ad esempio, per la formula $$(\bar x \lor y)\land (\bar y\lor z) \land (x \lor \bar z)\land (y\lor z) $$
che è equivalente a 
$$(x\rightarrow y)\land (y\rightarrow z) \land (\bar x \rightarrow \bar z )\land (\bar y \rightarrow z) $$
a sua volta equivalente a 
$$(\bar y \rightarrow \bar x)\land (\bar z\rightarrow \bar y) \land (z \rightarrow x )\land (\bar z \rightarrow y) $$
si definisce il grafo associato 
\begin{center}
    \includegraphics[width=0.7\textwidth ]{images/grafoAssociato.pdf}
\end{center}
La seguente proposizione sarà fondamentale per dimostrare che una formula $2-SAT$ può essere decisa in tempo polinomiale.\acc  
\prop{} Sia $\phi$ una formula in CNF in cui ogni clausola ha al più $2$ variabili, e sia $G$ il grafo associato, $\phi$ è soddisfacibile ($\phi\in 2-SAT$ ) \textit{se e solo se}, 
nessuna componente fortemente connessa di $G$ contiene sia una variabile che la sua negata.\acc 
\dimo{} Verranno dimostrate entrambe le direzioni del se e solo se. \acc 
\boxedMath{$\implies$} Sia $\phi$ una formula in $2-SAT$, e sia $G$ il grafo associato. Sia $(a,b)$ un arco del grafo, ne consegue in $\phi$ c'è la clausola 
$$ a\rightarrow b \equiv \bar a \lor b$$
quindi se $a=1$, anche $b=1$. Tale fatto si può estendere ad un cammino nel grafo, se esiste un cammino 
$$ x_1,x_2\dots , x_k$$
ne consegue che 
$$ x_1\rightarrow x_2 \dots \rightarrow x_k$$
quindi 
$$ x_1 \rightarrow x_k$$
Si consideri una generica variabile $x$. Se $x$ ed $\bar x$ sono in una stessa componente fortemente connessa, allora esiste un cammino da $x$ ad $\bar x$ ed un cammino da $\bar x$ ad $x$\begin{center}
    \includegraphics[width=0.5\textwidth ]{images/dimoGrafo.pdf}
\end{center}
Si ha però che 
$$ \begin{cases}
    x\rightarrow \bar x \equiv \bar x \lor \bar x\equiv \bar x   \\ 
    \bar x \rightarrow x \equiv x \lor x\equiv x 
\end{cases}$$
Ma allora in $\phi$ esiste sia la clausola $x$ che la clausola $\bar x$, essendo $\phi$ in CNF essa vedrà queste due clausole in AND 
$$ x \land \bar x $$
Ma tale formula per qualunque assegnazione di $x$ sarà sempre 0, quindi $\phi$ è insoddisfacibile, ma ciò contraddice l'ipotesi, è quindi impossibile che una componente fortemente connessa contenga sia $x$ che $\bar x$. \acc  
\boxedMath{$\impliedby$} Sia $\phi$ una formula in CNF con ogni clausola contenente al più 2 variabili, il cui grafo associato $G$ ha una componente fortemente connessa in cui sono contenute sia $x$ che $\bar x$. Sia $G'$ il grafo in cui ogni nodo $C_i$ rappresenta una componente fortemente connessa di $G$ (si applica la compressione dei nodi).
\begin{center}
    \includegraphics[width=0.9\textwidth ]{images/compressione.pdf}
\end{center}
Si consideri un ordinamento topologico di $G'$ $$ (C_1,C_2\dots C_n)$$ dove $i<j$ indica che $C_i$ viene prima di $C_j$ nell'ordinamento. Si consideri il seguente \textit{assegnamento} di variabili in input per $\phi$\begin{itemize}
    \item Per ogni variabile $x$ si ha che\begin{itemize}
        \item se $x$ è contenuta in una componente $C_i$, e $\bar x$ è contenuta in una componente $C_j$ tale che $i\le j$ ( $C_i$ non appare dopo $C_j$ nell'ordinamento), allora la variabile $x$ si assegna uguale a 0.
        \item nel caso opposto, se $x$ è contenuta in una componente $C_i$, e $\bar x$ è contenuta in una componente $C_j$ tale che $i\ge j$ ( $C_i$ appare dopo $C_j$ nell'ordinamento), allora la variabile $x$ si assegna uguale a 1.
    \end{itemize}
\end{itemize}
Dato tale assegnamento, è vero il seguente \textbf{fatto} :
\begin{quote}
per nessun arco $(a,b)$ in $G$,  può succedere che $a$ sia vero\footnote{con $a$ vero si intende che $a$ è 1. con $a$ falso si intende che $a$ è 0} e $b$ sia falso. La dimostrazione è la seguente, si pone per assurdo che il \textit{fatto} sia falso, che esiste quindi una situazione in cui\begin{itemize}
    \item 
 $a$ è vero \item  $b$ è falso\end{itemize} supponiamo che $C_i$ sia la componente in cui è contenuto $a$. Essendoci l'arco $(a,b)$ esiste la clausola $\bar a \lor b$ ed esiste quindi anche l'arco $(\bar b,\bar a)$, essendo $a$ vero, si ha che $\bar a$ è falso.
 Ma allora, il nodo $\bar a$ compare in una componente $C_j$ tale che $j<i$ \begin{center}la componente in cui è contenuta $\bar a$ precede la componente in cui è contenuta $a$ nell'ordinamento topologico\end{center}
 Tale affermazione è vera dato l'assengamento particolare scelto per le variabili. Essendo $b$ falso, ne consegue che $\bar b$ è contenuto in una componente $C_k$ che viene dopo $C_i$, quindi 
 $$ k>i>j$$
 nell'ordinamento topologico
 $$ C_k>C_i>C_j$$
 Ma allora la presenza dell'arco $(\bar b, \bar a)$ contraddice l'ordinamento topologico, quindi è impossibile che il fatto sia falso.
\end{quote} 
Tale fatto implica che $\phi$ è SAT. Quindi entrambe le direzioni della proposizione sono dimostrate.\hfill$\blacksquare$\acc 
Data tale proposizione, per decidere se una formula è o non è in $2-SAT$ basterà controllare il grafo associato trovando le componenti fortemente connesse, ciò si può fare (ad esempio, con l'algoritmo di Tarjan ) in tempo polinomiale. Ciò porta al seguente risultato:\acc
\teo{} $2-SAT\in P $
\flowerLine 
\section{La classe $NP$}
I problemi presentati, ad esempio 
\begin{itemize}
    \item $3-COL$
    \item $3-SAT$
    \item ecc...
\end{itemize}
condividono una proprietà: data una \textit{soluzione}, questa può essere verificata valida o meno in tempo polinomiale. Si vuole definire in questa sezione la classe $NP$ tramite il \textit{non determinismo}, esiste però, anche un'altra definizione altrettanto naturale.\acc 
\defi{} Dato un linguaggio $L$, una TM $V$ è detta \textbf{verificatore} se, dato un input $<x,y>$ si ha che \begin{itemize}
    \item $x\in L \iff \exists y \text{ tale che }V(<x,y>)$ accetta
\end{itemize}
$V$ data una soluzione può verificare che questa sia contenuta o no nel linguaggio $L$. Si dice che $V$ ha \textit{tempo polinomiale} se \begin{itemize}
    \item L'esecuzione di $V$ è in $O(|x|^k)$ per qualche $k\in\Z^+$
    \item $|y|=poly(|x|)$, ossia la cardinalità di $y$ è polinomiale rispetto la cardinalità di $x$.
\end{itemize}
\defi{($NP$)} La classe dei linguaggi $NP$ contiene tutti i linguaggi $L$ che ammettono un verificatore polinomiale.\acc 
\textbf{Esempio} : Si consideri il linguaggio $3-COL$, una soluzione è data sotto-forma di grafo colorato, se $n$ è il numero di nodi,  è possibile controllare ogni coppia di nodi in $O(n^2)$, e verificare che, se questi sono adiacenti, sono anche di colori diversi. La verifica avviene in tempo polinomiale. Nonostante questo, la ricerca di una soluzione dato un grafo non colorato è un problema in $EXP$. Quindi $3-COL$ è un problema $NP$. Si vuole dimostrare formalmente.\acc
\prop{} $3-COL\in NP$\acc 
\dimo{} Sia $V$ un verificatore per $3-COL$ che prende in input un grafo $G$ ed una colorazione 
$y=(c_1,c_2\dots c_n)$, con $c_i$ colore dell'$i$-esimo nodo di $G$. La procedura di $V$ è definita come segue\begin{itemize}
    \item controlla ogni coppia $(c_i,c_j)$ se $(i,j)\in E(G)$ 
    \item rifiuta se e solo se $c_i\ne c_j$ per ogni coppia 
\end{itemize}
Chiaramente $V$ ha una complessità polinomiale in quanto al più controlla ogni arco del grafo, e verifica che $y$ sia una soluzione. Se $V(<G,y>)$ accetta, allora $y$ è una colorazione valida per $G$ e $G$ è 3-colorabile.\hfill$\blacksquare$\acc  
\teo{} $P\subseteq NP \subseteq EXP$
\acc\dimo{}
Per il teorema delle gerarchie dei tempo è noto che $P\ne EXP$, si ha che 
$$ P\ne NP \lor NP \ne EXP$$
Si consideri un linguaggio $L\in P$, ossia $\exists M$ che è una TM e $x\in L \iff M(x)$ accetta ed ha complessità polinomiale in funzione di $|x|$. Si definisce un verificatore $V(<x,y>)$ la cui procedura è definita come segue\begin{itemize}
    \item ignora l'input $y$
    \item esegue $M(x)$
    \item se $M$ accetta su $x$, $V$ accetta, altrimenti rifiuta.
\end{itemize}
è chiaro che $V$ soddisfi le seguenti proprietà 
\begin{itemize}
    \item $V$ ha la stessa complessità di $M$, è quindi polinomiale 
    \item $V$ è un verificatore per $L$
\end{itemize}
Per definizione $L\in NP$. Sapendo che esiste un verificatore polinomiale per $L$, si considera una $TM$ che considera tutti i possibili certificati $y$ tali che $|y|=poly(|x|)$, e accetta solo se $V(<x,y>)$ accetta, quindi tale TM è un decisore per $L$ che opera in tempo esponenziale, quindi $L\in EXP$.\hfill$\blacksquare$\acc  
Si considererà ora la TM non deterministica per dare una definizione alternativa di NP. Una TM non deterministica ha la funzione di transizione che, piuttosto che portare da uno stato ad un altro, porta da uno stato ad un insieme di stati. \acc 
Definiamo la complessità computazionale di una TM non deterministica come il massimo numero di passi impiegati in funzione di $n$ per ogni stringa $x$ di lunghezza $n$, considerando il ramo di computazione più lungo.\acc 
\defi{} : Sia $t:\N\rightarrow \N$ una funzione, si definisce 
$$ Ntime(t(n))=\Bigg\{L \ |\ \begin{matrix}\exists M \in TM(\text{non deterministica}) \text{ tale che }\\ L(M)=L\land M\text{ ha complessità }O(t(n))\end{matrix}\Bigg\}$$
Per semplicità, denoteremo NTM una TM non deterministica.\acc 
\defi{} $\displaystyle NP=\bigcup_{k\in\Z^+}Ntime(n^k)$
\acc 
\defi{} $\displaystyle NEXP=\bigcup_{k\in\N}Ntime(2^{n^k})$
Si vuole dare un esempio di linguaggio $NP$ usando tale definizione, si consideri $SAT$, si definisce una NTM $N$ che decide $SAT$, ossia $N(<\phi>)$ accetta se $\phi$ è soddisfacibile. La procedura di tale TM è definita come segue\begin{itemize}
    \item sia $n$ il numero di variabili, si definisce un array $x$ lungo $n$ non inizializzato, e si definisce un contatore $i=0$. 
    \item "TOP" : si incrementa $i$ di 1, se $i>n$, esegui la linea "CHECK", altrimenti esegui entrambe le seguenti linee in modo parallelo (diramazione della computazione)\begin{itemize}
        \item $x[i]=0$, ed esegui la linea "TOP"
        \item $x[i]=1$, ed esegui la linea "TOP"
    \end{itemize}
    \item "CHECK" : accetta se e solo se $\phi(x)=1$
\end{itemize}
\teo{} Le due definizioni di $NP$ sono equivalenti.\acc 
\dimo{} Verranno dimostrate le due implicazioni separatamente.\\ 
\boxedMath{$\implies$} Se $L$ ha un verificatore polinomiale $V$, è possibile definire una NTM $N$ la cui procedura è descritta come segue:\begin{itemize}
    \item su input $x$
    \item $N$ utilizzando il non determinismo determina ogni possibile soluzione $y$ tale che $|y|=poly(|x|)$
    \item $N$ accetta solo se $V(<x,y>)$ accetta 
\end{itemize}
è chiaro che $N$ è un decisore non deterministico per $L$ che opera in tempo polinomiale.\acc 
\boxedMath{$\impliedby$} Se $L$ è decidibile in tempo polinomiale da una NTM $N$, allora su input $x$, $poly(|x|)$ è un upper bound sui passi di computazione di ogni ramo parallelo di $N$. Si definisce un verificatore $V$, che opera su input $y$, definito come l'insieme dei passi deterministici di un ramo di $N$, in particolare\begin{itemize}
    \item $V(x,y)$ intepreta $y$ come un insieme di (al più) lunghezza $poly(|x|)$ che simula $N(x)$ in modo deterministico seguendo le scelte di "fork" determinate da $y$. 
\end{itemize}
$V(<x,y>)$ ha complessità $poly(|x|)$, ed inoltre $x\in L \iff N(x)$ accetta $\iff \ \exists y \ | \ V(<x,y>)$  accetta.\hfill$\blacksquare$\acc   
\flowerLine 
\section{Riduzioni di Linguaggi $NP$}
Sono stati presentati molti linguaggi in $NP$ che sono anche in $P$, tramite la riduzione, si può dimostrare che un vasto insieme di linguaggi è in $P$ se e solo se un singolo (e determinato) linguaggio è in $P$.\acc 
\teo{} $SAT\in P \implies 4-COL\in P$\acc 
\dimo{} Per ipotesi, sia $M_{SAT}$ la TM che decide $SAT$ in tempo polinomiale, si definisce una TM $M_{4COL}$ per decidere $4-COL$. La riduzione deve "associare" l'affermazione \begin{quote}
    $G$ è 4-colorabile
\end{quote}
a \begin{quote}
    $\phi_G$ è soddisfacibile 
\end{quote}
Si definisce quindi una formula associata ad un grafo, che è soddisfacibile solo se il grafo è 4-colorabile.\acc 
Dato $G=(V,E)$, si definisce una formula $\phi_G$ con $2n$ variabili, dove $n=|V|$ è il numero di nodi. 
$$ \phi_G(x_1,x_1',x_2,x_2',\dots x_n,x_n')$$
per ogni nodo $i$ esiste la coppia di variabili $x_i,x_i'$. Il colore di un nodo è codificato da queste due variabili come segue \begin{center}
    \begin{tabular}{|c|c|c|}
        \hline
        $x_i$ & $x_i'$ & colore di $i$                 \\ \hline
        0     & 0      & {\color[HTML]{9A0000} rosso}  \\ \hline
        0     & 1      & {\color[HTML]{036400} verde}  \\ \hline
        1     & 0      & {\color[HTML]{C69A00} giallo} \\ \hline
        1     & 1      & {\color[HTML]{3531FF} blu}    \\ \hline
        \end{tabular}
\end{center}
è necessario codificare il vincolo che, per ogni arco $(i,j)\in E$, si ha che i nodi $i$ e $j$ hanno colori diversi, ossia che $$ (x_i,x_i')\ne (x_j,x_j')$$ deve essere necessariamente vera per far si che $\phi_G$ sia soddisfacibile, in logica:
$$ \phi_{i,j}=\overline{ 
x_i\leftrightarrow x_j \land x_i'\leftrightarrow x_j'
}$$
che è equivalente a 
$$ 
\phi_{i,j}=((\bar x_i\lor x_j)\land (\bar x_j \lor x_i))\land  
((\bar x_i'\lor x_j')\land (\bar x_j' \lor x_i'))
$$
alla fine, la formula finale $\phi_G$ sarà l'and logico fra tutte queste clausole $\phi_{i,j}$ al variare di $(i,j)\in E$ 
$$ \phi_G=\bigwedge\limits_{(i,j)\in E}\phi_{i,j}$$
chiaramente la funzione di riduzione è polinomiale, inoltre, se $G\in4-COL$, esiste una 4-colorazione $C=(c_1,c_2\dots, c_n)$ che ha codifica binaria $x=(x_1,x_1',\dots x_n,x_n')$ che soddisfa $\phi_G$, ossia $\phi_G(x)=1$. \hfill$\blacksquare$\acc   
Formalizziamo il concetto di riduzione polinomiale \acc 
\defi{} Siano $A,B$ due linguaggi diremo che $A$ \textit{è riducibile in tempo polinomiale} a $B$, e denoteremo $$ A\le_m^P B$$ 
se esiste una TM $R:\Sigma^*\rightarrow \Sigma^*$ tale che 
$$ \forall x \in \Sigma^*, \ \ \ x\in A\iff R(x)\in B$$
e $R$ ha complessità di tempo polinomiale.\acc 
\teo{} se $A\rid B$ e $B\in P$ allora $A\in P$. \acc 
\dimo{} data una TM $M_B$ che decide $B$ in tempo polinomiale, è vero che $M_B(R(x))$ decide ogni $x\in A$, in tempo polinomiale in $|x|$, dove $R$ è la TM della riduzione, per definizione anch'essa polinomiale.\hfill$\blacksquare$\acc  
\teo{} $3-COL\rid 4-COL$\acc
\dimo{} si definisce la TM della riduzione $R$ tale che $R(<G>)=<H>$, dove $G$ è un grafo 3-colorabile ed $H$ è un grafo 4-colorabile. $H$ si ottiene aggiungendo a $G$ un nodo connesso a tutti gli altri nodi, con un quarto colore nuovo non presente in $G$.\begin{center}
    \includegraphics[width=0.45\textwidth ]{images/3COL4COL.drawio.pdf}
\end{center}
Se $G$ è 3-colorabile, $H$ sarà 4-colorabile, e se $H$ è 4-colorabile, $G$ deve essere 3-colorabile.\hfill$\blacksquare$\acc  
\teo{} $A\rid B \land B\rid C \implies A\rid C$\acc 
\dimo{} sia $R_1$ la riduzione da $A$ a $B$ 
$$ x\in A \iff R_1(x)\in B$$
sia $R_2$ la riduzione da $B$ a $C$ 
$$ x\in B \iff R_2(x)\in C$$
allora, per ogni $x$
$$ x\in A\implies R_1(x)\in B \implies R_2(R_1(x))\in C$$
ma allora $R_1\circ R_2$ è una riduzione da $A$ a $C$ perché 
$$ x\in A \iff R_1(R_2(x))\in C$$\hfill$\blacksquare$\acc  
Non tutte le riduzioni hanno complessità polinomiale, si consideri il seguente linguaggio 
$$ 4-CROMA = \{<G>\ | \ \mathcal X(G)=4\}$$
$\mathcal X(G)$ è il \textit{numero cromatico} di $G$, $\mathcal X(G)=k$ se $G$ è $k-$colorabile, ma per ogni $k'<k$ non è $k'$-colorabile. È vero che 
$$ 4-CROMA \le_T^P SAT$$ 
Dove $\le_T^P$ è una riduzione ma non di tipo map-reduction, la $T$ sta per turing, la riduzione avviene mediante una procedura/algoritmo.\acc 
Dato $G$, si considera una TM (descritta da un algoritmo) la cui procedura è definita come segue\begin{enumerate}
    \item si considera una riduzione $R$ che mappa $4-COL$ in $SAT$ 
    \item si stabilisce se $G$ è o non è 4-colorabile tramite la risoluzione della formula logica associata. 
    \item si considera una riduzione $R'$ che mappa $3-COL$ in $SAT$ 
    \item  si stabilisce se $G$ è o non è 3-colorabile tramite la risoluzione della formula logica associata  
\end{enumerate}
La procedura mostra che, se è possibile decidere $SAT$, è possibile decidere $4-CROMA$, ma tale riduzione non avviene mediante una funzione.\acc 
\teo{} se $A\rid B$ e $B\in NP$ allora $A\in NP$. \acc 
\dimo{} la riduzione $R$ è una TM (di tempo polinomiale) tale che $$ x\in A\iff R(x)\in B$$
sia $N_B$ la NTM che decide (in tempo polinomiale) $B$, si definisce $N_A(x)=N_B(R(x))$, è chiaro che $N_A$ è una NTM che decide $A$ in tempo polinomiale.\hfill$\blacksquare$\acc 
La seguente definizione è fondamentale. \acc 
\defi{($NP$ completezza)} Un linguaggio $A$ si dice \textit{$NP$ difficile} se, per ogni singolo linguaggio $L\in NP$, è vero che $L\rid A$. Se $A$ è sia $NP$ difficile che in $NP$, allora si dice che $A$ è \textbf{$NP$-completo}. \acc 
Se si dimostra che un linguaggio $NP$ completo è in $P$, allora ogni singolo linguaggio in $NP$ sarebbe in $P$. Ad oggi non è mai stato dimostrato, infatti: 
\sapbox{\huge non è noto se $ P=NP$
sia vero o falso.\normalsize}
il seguente risultato è uno dei passi più importanti nella teoria della complessità.\acc
\teo{(Cook-Levin)} $SAT$ è $NP$ completo.\acc 
\dimo{} si è già dimostrato che $SAT\in NP$, bisogna dimostrare solo che $SAT$ sia $NP$ difficile, ovvero che ogni linguaggio $NP$ si riduce polinomialmente a $SAT$.\acc 
Sia $A$ un generico linguaggio in $NP$, e sia $N$ la NTM che lo decide in tempo polinomiale, in particolare, in $O(n^k)$. $$ L(N)=A$$
Per definizione, per ogni input $x\in A$, $N(x)$ ha almeno un ramo accettante.\acc 
Per la dimostrazione si introduce la definizione di \textbf{tabella di computazione}, per ogni ramo di computazione di $N$ su input $w=w_1w_2\dots w_n$, è associata una tabella di computazione, della forma \begin{center}
    \includegraphics[width=0.6\textwidth ]{images/tabellaComputazione.eps}
\end{center}
\begin{itemize}
    \item sia $n$ la dimensione dell'input $w$
    \item ogni riga della tabella inizia e finisce con il simbolo $\#$, o un qualsiasi altro arbitrario simbolo che non sia nell'alfabeto della NTM. 
    \item ogni elemento $(i,j)$ della tabella è detto \textit{cella}
    \item ogni riga ha $n^k+3$ (ma scriveremo per semplicità $n^k$) celle, gli spazi che non sono contenuti dall'input o dallo stato hanno il carattere $\blank$.\item una computazione può essere lunga al più $n^k$ passi. La tabella avrà ordine $n^k\times n^k$
    \item ogni cella può contenere un solo simbolo, questo può essere uno stato, un carattere dell'alfabeto del nastro, il simbolo $\#$ oppure $\blank$.
    \item ogni riga contiene una configurazione della computazione della TM.
\end{itemize}
Una \textit{finestra} nella tabella, è una sotto tabella di 2 righe e 3 colonne, rappresentata in verde nell'immagine.\acc 
Le righe della tabella rappresentano una successione di configurazioni nella computazione (andando dall'alto verso il basso), una tabella è valida se la successione è coerente con l'evoluzione della computazione descritta dalla funzione di transizione $\delta$ della NTM.\acc 
Una tabella è \textit{accettante} se contiene in una riga lo stato $q_{acc}$ ed è valida. Una NTM accetta un input $w$ se esiste almeno una tabella accettante su $w$.\acc 
Definiremo una riduzione che data $N$ produrrà una formula booleana $\phi$ che simula l'esecuzione di un input $w$ su $N$. Siano \begin{itemize}
    \item $Q$ l'insieme degli stati di $N$
    \item $\Gamma$ l'alfabeto del nastro di $N$
\end{itemize}
ogni variabile della formula $\phi$ sarà del tipo 
$$ x_{i,j,s}$$
dove 
$$ i,j \in [1,2\dots n^k] \ \ \ \ \ \ s\in\mathbb S = Q\cup \Gamma \cup \{\#\}$$
esiste quindi una variabile per ogni cella $(i,j)$ della tabella, ed ogni possibile valore che può assumere. Vogliamo definire $\phi$ in modo tale che $$ \phi(x)=1\iff \text{ la tabella è accettante}$$
\textit{Osservazione} : Il numero di variabili è $n^{2k}\cdot |\mathbb{S}|$, è quindi polinomiale in $n$, dato che $|\mathbb{S}|$ non dipende da $n$.\acc 
Le variabili saranno definite in tal modo $$x_{i,j,s}=\begin{cases}
    1 \text{ se nella cella $(i,j)$ c'è il valore }s \\ 
    0 \text{ altrimenti }
\end{cases} $$
Si progetta la formula $\phi$ in modo tale che, un assegnamento che la soddisfa, corrisponde ad una tabella accettante per $N$. Definiamo diverse sottoformule di $phi$.\acc 
\textbf{prima formula} : $\phi_{start}$\\
questa formula serve a descrivere il fatto che la prima regola di una tabella valida deve necessariamente contenere la configurazione iniziale, che sappiamo essere 
$$ \# \  q_0 \ w_1 \ w_2 \ w_3 \ \dots w_n \ \blank \ \dots \ \blank \ \#$$
la formula sarà quindi 
$$ 
\phi_{start}=x_{1,1,\#}\land x_{1,2,q_0}\land x_{1,3,w_1} \land \dots \land  
x_{1,n+2,w_n}\land x_{1,n+3,\blank }\land \dots \land  
x_{1,n^k-1,\blank }\land x_{1,n^k,\#}
$$
è chiaro che questa formula deve essere necessariamente vera per permettere che una tabella sia valida, dato che lo stato iniziale è sempre determinato in tal modo per ogni ramo di computazione.\acc
\textbf{seconda formula} : $\phi_{acc}$\\ 
questa formula descrive il fatto che, una tabella per essere accettante deve contenere \textit{almeno} una cella con lo stato accettante, è quindi (intuitivamente) definita come segue $$ 
\phi_{acc}=\bigvee_{1 \leq i,j \leq n^k} x_{i,j,q_{acc}}$$
\textbf{terza formula} : $\phi_{cell}$\\
questa formula deve descrivere che, per ogni variabile $x_{i,j,s}=1$ (la cella $(i,j)$ contiene $s$), deve essere vero che $x_{i,j,t}=0$ per ogni $t\ne s$, ossia, una cella può avere un solo valore. Inoltre deve essere vero che una cella ha esattamente un valore, quindi deve esistere necessariamente $s$ tale che $x_{i,j,s}=1$ per ogni cella $(i,j)$. 
$$\bigvee_{s\in\mathbb S} x_{i,j,s} \ \ \ \   \text{ almeno un valore}$$
$$ \bigwedge\limits_{\begin{matrix}s,t\in\mathbb{S}\\ s\ne t\end{matrix}}
(\overline{x_{i,j,s}}\lor \overline{x_{i,j,t}})\ \ \ \   \text{ al massimo un valore}$$
la terza formula completa è 
$$ \phi_{cell}=\bigwedge\limits_{1 \leq i,j \leq n^k} \Big(
    \Big(\bigvee_{s\in\mathbb S} x_{i,j,s}\Big) \land
    \Big(\bigwedge\limits_{\begin{matrix}s,t\in\mathbb{S}\\ s\ne t\end{matrix}}
    (\overline{x_{i,j,s}}\lor \overline{x_{i,j,t}})\Big) 
\Big)$$
\textbf{quarta formula} : $\phi_{move}$\\
questa formula assicura che la riga $l+1$ è coerente con la riga $l$, ossia che le due configurazioni che si susseguono sono coerenti e in accordo con la funzione di transizione $\delta$ della NTM $N$. Per fare ciò, basta controllare ogni possibile finestra su due righe, questa deve essere \textit{lecita} (non violare le regole di $\delta$).\begin{itemize}
    \item in una finestra, tutti i simboli che non sono adiacenti ad uno stato $q$ devono rimanere invariati 
    \item i simboli adiacenti ad uno stato $q$ devono evolvere nella riga successiva seguento uno dei possibili risultati della $\delta$
\end{itemize}\begin{center}
    \includegraphics[width=\textwidth ]{images/finestraLecita.eps}
\end{center}
denotiamo la finestra $[i,j]$ quella composta dalle celle$$ \begin{matrix}
    (i,j) & (i+1,j) & (i+2,j)\\ 
    (i,j+1) & (i+1,j+1) & (i+2,j+1)
\end{matrix}$$
$$ \phi_{move}=\bigwedge\limits{1 \leq i,j \leq n^k}
(\text{ la finestra $[i,j]$ è lecita})$$
Senza entrare troppo nel formalismo, una finestra $[i,j]$ è lecita se la seguente è vera 
$$ \bigvee_{\begin{matrix}
a_1\dots,a_6\\ 
\text{rendono la finestra}\\\text{lecita}
\end{matrix}}
(x_{i,j,a_1}\land x_{i+1,j,a_2}\land \dots \land x_{i+2,j+1,a_6})
$$
La formula $\phi$ tale che, un assegnamento che la soddisfa, corrisponde ad una tabella accettante per $N$, è la seguente 
$$ \phi = \phi_{start}\land \phi_{acc}\land \phi_{cell}\land \phi_{move}$$
la riduzione ha costo $$ n^kn^k|\mathbb S|=poly(n)$$
quindi un qualsiasi linguaggio $A$ si riduce polinomialmente a $SAT$.\hfill$\blacksquare$\acc   
Utilizzando la riduzione è possibile dimostrare che molti linguaggi incontrati sono $NP$ completi, ad esempio, essendo che $$ SAT\rid CIRCUIT-SAT$$
ne consegue che anche $CIRCUIT-SAT$ è $NP$ completo. Ed essendo che 
$$CIRCUIT-SAT\rid 3-SAT$$
anche $3-SAT$ è $NP$ completo. Essendo vere le seguenti relazioni 
$$ 
3-COL \rid 4-COL \rid SAT \rid CIRCUIT-SAT \rid 3-SAT \rid CLIQUE
$$
tutti i linguaggi sopra elencati sono $NP$ completi.\acc 
Una conseguenza della $NP$ completezza è il seguente risultato.\acc 
\teo{} Se $S$ è un linguaggio $NP$ completo, allora $S\in P \iff P=NP$\acc 
\dimo{} $\forall L \in NP$, $L\rid S$. Se $S\in P$, allora $L$, riducendosi ad $S$ in tempo polinomiale, risulta anch'esso in $P$. D'altra parte, se $P=NP$ ed $S$ è $NP$ completo, allora $S\in P$.\hfill$\blacksquare$\acc   
Si è mostrato che la decidibilità di $SAT$ è un problema difficile, e ad oggi non sembra essere decidibile in tempo polinomiale, si vuole ora analizzare la sua risolvibilità, ossia la ricerca di una soluzione per una formula $\phi$.\acc 
$SAT$ è \textbf{self-riducibile}, si assuma che $P=NP$, allora per definizione una TM deterministica $M$ può decidere $SAT$ in tempo polinomiale. \begin{quote}
    $M(\phi)$ accetta se $\phi$ è soddisfacibile
    $M(\phi)$ rifiuta se $\phi$ non è soddisfacibile
\end{quote}
Si può trovare un assegnamento per $\phi$ considerando la seguente procedura \begin{enumerate}
    \item data $\phi(x_1,x_2\dots x_n)$
    \item si considera $\phi'=\phi(0,x_2\dots x_n)$
    \item si controlla $M(\phi')$, questa per ipotesi avrà tempo polinomiale 
    \item se $M(\phi')$ accetta si pone nella soluzione $x_1=0$, altrimenti si pone $x_1=1$
    \item si controllano nello stesso modo tutti gli altri letterali $x_2,x_3\dots ,x_n$
    \item al termine si avrà un assegnamento valido per $\phi$, trovato in tempo polinomiale.
\end{enumerate}
Essendo che ogni problema $NP$ si riduce a $SAT$, si ha che se $P=NP$, una soluzione valida per ogni problema $NP$ può essere trovata in tempo polinomiale.\acc 
\teo{} $P=NP\implies EXP=NEXP$\acc 
\dimo{} Si utilizza una tecnica chiamata \textit{padding}, l'assunzione è che $P=NP$, si vuole dimostrare che $NEXP\subseteq EXP$ (sappiamo già che $EXP \subseteq NEXP$), sia $L\in NEXP$ e sia $N$ la NTM che decide $L$ in tempo $O(2^{n^k})$ per qualche $k$.\acc 
Si considera un linguaggio derivato
$$L'=\{<x,1^{2^{|x|^k}}> \ | \ x\in L \} $$
Nota bene : $1^{2^{|x|^k}}$ è la stringa composta da ${2^{|x|^k}}$ caratteri (che sono appunto, 1). Il carattere 1 non deve essere presente nell'alfabeto della NTM.
\begin{quote}
    \textbf{Fatto} : $L'\in NP$, la dimostrazione è semplice, si consideri una NTM $N'$, la cui procedura è descritta come segue\begin{enumerate}
        \item controlla che l'input è della forma $x'=<x,1^{2^{|x|^k}}>$. Tale controllo avviene in tempo $O(|x'|)$ 
        \item se il controllo ha successo, esegue $N(x)$
    \end{enumerate}
    chiaramente $N'$ decide $L'$ in tempo polinomiale in $|x'|$.
\end{quote}
Visto che per ipotesi $P=NP$, si ha che $L'\in P$, allora esiste una TM deterministica $M'$ che decide $L'$ in tempo polinomiale. Allora è possibile decidere $L$ in tempo esponenziale\begin{itemize}
    \item dato $x$, si crea la stringa $x'=<x,1^{2^{|x|^k}}>$, la creazione di tale stringa avviene in tempo $O(2^{|x|^k})$ 
    \item avvio $L'(x')$, questa opera in tempo polinomiale, se $L'$ accetta, allora $x\in L$, altrimenti rifiuta 
\end{itemize}
è quindi chiaro che $L$ viene deciso da tale procedura in tempo esponenziale $\implies L\in EXP$.\hfill$\blacksquare$\acc   
\teo{(di Ladner)} : Se $P\ne NP$, allora esiste un linguaggio $L\in NP$ tale che $N$ non è $NP$ completo. Se $P=NP$, ogni linguaggio in $NP$ è $NP$ completo. 
\flowerLine
\section{La Classe $coNP$}
Definiamo ora una nuova classe di complessità. La definizione di $NP$ riguarda la decidibilità, ossia la certificazione che un certo elemento sia in un linguaggio, la classe $coNP$ riguarda la certificazione che un elemento \textit{non sia} in un linguaggio.\acc 
Ad e sempio, esiste il linguaggio $$ UNSAT=\overline{SAT}$$
e ci si chiede: sapendo che $SAT\in P$, $UNSAT\in NP$?\acc 
\defi{($coNP$)} $coNP=\{L \ | \ \overline{L}\in NP\}$\acc 
\textbf{Osservazione} : $coNP \ne \overline{NP}$\acc 
\teo{} $SAT\in P \iff UNSAT \in P$\acc 
\dimo{} dato un decisore per $SAT$, si decide $UNSAT$ negando la risposta di tale decisore.
\hfill$\blacksquare$\acc  
\teo{} $P=coP$, ovvero, $P$ è chiuso per il complemento.\acc 
\dimo{} Dato un decisore per un linguaggio in $P$, è necessario invertire il risultato della TM.\hfill$\blacksquare$\acc   
\teo{} $coNP\subseteq EXP$\acc 
\dimo{} $L\in coNP \implies \overline L \in NP \subseteq EXP \implies L\in COEXP$ ma $COEXP=EXP$.
\hfill$\blacksquare$\acc  
Se $P\ne NP$ è valida la seguente relazione fra insiemi 
\begin{center}
    \includegraphics[width=0.3\textwidth ]{images/coNP.eps}
\end{center}
\teo{} $P\subseteq coNP$\acc 
\dimo{} $L\in P\implies \overline L \in P \subseteq NP \implies 
\overline L \in NP \implies L\in coNP$.\hfill$\blacksquare$\acc  
\teo{} $P=NP\implies P=coNP=NP$\acc 
\dimo{} $L\in coNP\implies \overline L \in NP=P \implies \overline L \in P $ ma essendo che $P=coP$ si ha $L \in P$.\hfill$\blacksquare$\acc   
\textbf{Corollario} : $coNP\ne NP\implies P \ne NP$.\acc
\defi{} $L$ è $coNP$ completo se\begin{itemize}
    \item $L\in coNP$
    \item $\forall A \in coNP, \ \ \ \ A\rid L$
\end{itemize} 
\teo{} $UNSAT$ è $coNP$ completo.\acc 
\dimo{} Ssappiamo che $UNSAT\in coNP$, è vero che 
$$ A\rid UNSAT \iff \overline A \rid \overline{UNSAT}=SAT$$
siccome $\overline A$ è in $NP$ e $SAT$ è $NP$ completo, è vero che $\overline A$ si riduce a $SAT$, quindi $UNSAT$ è $coNP$ completo.\hfill$\blacksquare$\acc  
In passato, diversi problemi che sono in $NP\cap coNP$, sono stati dimostrati essere anche in $P$, si consideri l'insieme 
$$ PRIMES=\{<x> \ | \ x\text{ è un numero primo}\}$$
nel 1975 è stato dimostrato che $PRIMES\in NP$, e successivamente, nel 2001, è stato dimostrato che $PRIMES\in P$. Altri problemi sono in $NP\cap coNP$, ma non è stato ancora verificato se questi siano o non siano in $P$.
\chapter{Complessità Spaziale}
Data una TM $M$, si definisce \textit{complessità spaziale} la funzione $S:\N\rightarrow \N$ tale che 
$$ S(n)=\max_{x \ t.c. \ |x|=n}\Big(
\text{numero di celle distinte utilizzate da $M$ su input $x$}    
\Big) $$
C'è una differenza fondamentale fra complessità spaziale e temporale, lo spazio può essere riutilizzato, il tempo no.\acc  La complessità spaziale non considera le dimensioni dell'input, in quanto non permetterebbe a nessuna TM di avere una complessità spaziale inferiore a $O(n)$, dato che l'input occupa delle celle di memoria. Si considerano sempre quindi 2 nastri, uno riservato all'input, ed uno riservato al calcolo. Le celle del nastro di input non saranno considerate nella misura di complessità spaziale.\acc 
\defi{} $SPACE(S(n))=\{ L \ | \ \exists \ TM \ M \text{ tale che }L(M)=L \land  M \text{ ha complessità }O(S(n))\}$
Da ciò, ne derivano alcune classi importanti 
$$ PSPACE = \bigcup_{k\in \Z^+}SPACE(n^k)$$
$$ EXPSPACE = \bigcup_{k\in \Z^+}SPACE(2^{n^k})$$
$$ LOG = \bigcup_{k\in \Z^+}SPACE(\log(n))$$
Vediamo alcuni esempi 
$$ A=\{0^n1^n \ | \ n\in \N\}$$
Si vuole mostrare che $A\in LOG$, ossia che esiste un algoritmo che, se la dimensione dell'input è $n$, utilizzera un numero in $O(\log(n))$ di celle distinte per completare la computazione e fornire un risultato.\acc 
In precedenza, è stata già presentata una TM che decidesse questo linguaggio, la seguente procedura è differente, ed è descritta dai seguenti punti\begin{itemize}
    \item Sul nastro di lavoro, viene memorizzato un contatore per memorizzare il numero di 0 ed 1 dell'input 
    \item il numero di 0 ed 1 è al più $2n$, per rappresentare tale numero in binario servono $O(\log_2(n))$ celle 
    \item la TM, inizierà a contare il numero di 0, incrementando il contatore 
    \item quando troverà il primo 1, inizierà a contare gli 1 decrementando il contatore 
    \item se trova uno 0 che segue un 1, rifiuta 
    \item al termine, se il contatore è uguale a 0 (il numero di 1 è identico al numero di 0) accetta 
\end{itemize}
è chiaro che tale TM decide $A$ in complessità di spazio $O(\log(n))$.\acc 
\textbf{Osservazione} : In termini di complessità di spazio, la TM multinastro è equivalente alla TM classica.\acc 
Si consideri ora il linguaggio $PAL$ contenente tutte le stringhe palindrome, ad alto livello, la TM che lo decide dovrebbe\begin{enumerate}
    \item su input $x$, considerare $n=|x|$
    \item per ogni $i\in[1,2 \dots , n]$\begin{enumerate}
        \item rifiuta se $x_i\ne x_{n+1-i}$
    \end{enumerate}
    \item accetta
\end{enumerate}
La TM userà un numero costante di nastri, ad alto livello la procedura è descritta dai seguenti punti\begin{itemize}
    \item sul nastro 1, si realizza l'indice $i$, che occupa $O(\log_2(n))$
    \item si memorizza l'indicie sul nastro 2 e si incrementa ad ogni passo, tale operazione richiede sempre complessità spaziale logaritmica 
    \item sul nastro 3 ad ogni passo si calcola $n+1-i$
    \item ad ogni iterazione si controlla che $x_i\ne x_{n+1-i}$
\end{itemize}
è chiaro che tale TM decide $A$ in complessità di spazio $O(\log(n))$.\acc 
Gli esempi visti hanno in comune lo stesso modello per decidere i linguaggi in complessità spaziale logaritmica, in particolare, su input $x$, si può valutare la lunghezza dell'input $x=|n|$ in spazio logaritmico. \acc 
In tale spazio, è anche possibile gestire un contatore/indice con il quale eseguire un numero di iterazioni che è polinomiale rispetto ad $n$, si possono inoltre considerare simboli presi dai nastri ed eseguire su di essi semplici operazioni aritmetiche.\acc 
\textbf{Esempio} : è possibile eseguire il prodotto fra due numero $a\cdot b = c$ in complessità di spazio logaritmica seguendo tale procedura:\begin{enumerate}
    \item si inizializza $c=0$
    \item si inizializza un indice $i=0$
    \item per $i$ che va da $0$ a $b$:\begin{itemize}
        \item si incrementa  $c$ di $a$
    \end{itemize}
\end{enumerate}
Non è efficiente in termini di tempo, ma lo è in termini di spazio. Per molti problemi, non sono state trovate delle TM che li decidessero in complessità di spazio logaritmica, non si sa quindi se siano o non siano in $LOG$, un seguente esempio è il linguaggio
$$ PATH = \{<G,s,t> \ | \ s,t\in V(G) \land \text{ esiste un cammino }s\rightarrow t\text{ in }G\}$$
Per cui si è considerato un algoritmo nella sezione \ref{classiComplessità}, questo, opera in tempo polinomiale, ma la complessità di spazio non è logaritmica, dato che per marcare i nodi, usa al più un bit per nodo, è quindi in $O(n)$ (si ricordi che nelle TM che operano sui grafi, le dimensioni dell'input identificano il numero dei nodi del grafo).\acc 
\textbf{Esempio} : $3SAT\in PSPACE$, questo è certo perché è possibile provare ogni singolo assegnamento della formula, essendo $n$ il numero dei letterali, ad ogni assegnamento provato si usano al più $n$ celle.\acc 
Essendo che $3SAT$ è $NP$ completo, ne consegue  che $$NP\subseteq PSPACE $$
\flowerLine
\section{Relazione fra Spazio e Tempo}
In questa sezione verrà resa chiara la relazione che intercorre fra spazio e tempo, in particolare, le relazioni di inclusione fra gli insiemi che definiscono le varie classi.  \acc 
\teo{} $DTIME(f(n))\subseteq SPACE(f(n))$\acc 
\dimo{} Supponiamo che una TM ha complessità temporale $f(n)$, per assurdo, ha complessità spaziale $g(n)$, con $g(n)=\Omega(f(n))$ (è asintoticamente maggiore), ad esempio, $f(n)$ potrebbe essere polinomiale e $g(n)$ esponenziale.  \acc 
Ma allora, essendo che ogni cella per essere riempita richiede almeno 1 passo di computazione, la TM necessariamente esegue $g(n)$ passi di computazione, ma per ipotesi ha complessità temporale $f(n)$, allora non può avere complessità spaziale maggiore di quella temporale.\hfill$\blacksquare$\acc   
Il  teorema ha un significato molto chiaro:
\sapbox{il tempo limita lo spazio}
Vediamo ora in che modo lo spazio può limitare il tempo. \acc
\teo{} Per ogni $f(n)\ge \log(n)$\footnote{con $\ge$ si intende, che è asintoticamente maggiore, ossia $f(n)\in\Omega(\log(n))$}, si ha che $SPACE(f(n))\subseteq DTIME(2^{O(f(n))})$\acc 
\dimo{} Nella valutazione della complessità spaziale, una TM ha un nastro di lavoro ed un nastro per l'input, una qualsiasi configurazione (snapshot durante la computazione) di una TM $M$, può essere rappresentato come segue 
$$ a_1 \ a_2 \ a_3 \ q_i \ a_4 \ a_5 \ a_6 \ ;\ k$$
dove \begin{itemize}
    \item $a_j$ sono caratteri sul nastro di lavoro 
    \item $q_i$ è lo stato attuale, e la sua posizione indica la posizione della testina sul nastro di lavoro 
    \item $k$ è il numero della cella su cui è presenta la testina sul nastro dell'input
\end{itemize}
Essendo $M$ deterministica, ed essendo un decisore, nessuna configurazione può ripetersi due volte durante una computazione. L'implicazione è chiaria :\begin{quote}
    Il numero MASSIMO di passi di computazione che può fare $M$ su un qualsiasi input, è limitato dal numero totale di configurazioni, in quanto al più, una computazione può includere nella sequenza ogni singola configurazione, ma queste non possono ripetersi
\end{quote}\begin{center}
    \includegraphics[width=0.75\textwidth ]{images/confTM2.eps}
\end{center}
Il numero totale di configurazioni è dato da\begin{itemize}
    \item ogni possibile contenuto del nastro di lavoro, ossia $|\Gamma|^{f(n)}$
    \item un possibile stato $|Q|$
    \item una posizione del nastro di input $n$
\end{itemize}
$$ \#\text{configurazioni} =|\Gamma|^{f(n)}|\cdot |Q|\cdot n $$
essendo per ipotesi $f(n)\ge \log(n)$ si ha che 
$$|\Gamma|^{f(n)}\cdot |Q|\cdot n \le |\Gamma|^{f(n)}\cdot |Q|\cdot 2^{f(n)} $$
quindi il numero di configurazioni è in $2^{O(f(n))}$, dato che $|\Gamma|$ e $|Q|$ sono costanti. Per le osservazioni precedenti, anche il numero di passi di computazione è limitato da $2^{O(f(n))}$.\hfill$\blacksquare$\acc  
\textbf{Corollario} : $PSPACE\subseteq EXP$, inoltre 
$$ LOG\subseteq P \subseteq PSPACE \subseteq EXP$$
ma $P\ne EXP$ quindi una delle inclusioni è stretta, in particolare, almeno una delle due seguenti affermazioni è vera\begin{itemize}
    \item $P\ne PSPACE$
    \item $PSPACE\ne EXP$
\end{itemize}\flowerLine
\section{Non Determinismo}\label{ndetSpace}
In termini di complessità spaziale, il non determinismo non ha lo stesso impatto che ha nella complessità temporale. Il seguente risultato è \textit{cruciale} ed è alla base di molti altri risultati importanti nell'ambito della complessità spaziale.\acc 
\teo{} $PATH \in SPACE(\log^2(n))$\acc 
\dimo{} Sia $M$ la TM che deve decidere $PATH$, l'input è la codifica del grafo $G=(V,E)$, e la coppia di nodi $s,t$ per cui si vuole stabilire se esiste un cammino fra di essi. La TM può calcolare $n=|V|$ in spazio logaritmico. \acc 
\textbf{Osservazione} : Se esiste un cammino fra $s$ e $t$, questo è composto da al più $n$ nodi.\acc 
Verrà adoperata una procedura ricorsiva, che ricercherà un nodo $u$ per cui \begin{itemize}
    \item esiste un cammino da $s$ ad $u$ 
    \item esiste un cammino da $u$ a $t$
\end{itemize}
se tale nodo $u$ esiste, allora esiste un cammino da $s$ a $t$. L'algoritmo opererà ricorsivamente "spezzando" in 2 il grafo ad ogni passo. \acc 
Si definisce la seguente procedure \texttt{FIND\_PATH(x,y,k)} definita come segue 
\begin{lstlisting}
FIND_PATH(x,y,k){
    if(k==0){
        if(x=y OR (x,y)=arco in E){ ACCEPT }
    }
    per ogni nodo w{
        if(FIND_PATH(x,w,k-1) AND FIND_PATH(w,y,k-1)){
            ACCEPT
        }
    }
    REJECT 
}
\end{lstlisting}
Al termine, si avrà che la procedura accetterà solo se esiste un cammino da $x$ ad $y$ con al più $k$ nodi. Sarà necessario fra si che la TM $M$ esegua la procedura \begin{quote}
    \texttt{FIND\_PATH(s,t,$\lceil \log(n)\rceil $)}
\end{quote}
Osservazioni\begin{itemize}
    \item ad ogni passo ricorsivo è necessario memorizzare un numero costante di variabili, ciò richiede spazio $O(\log(n))$
    \item la ricorsione si divide in 2 ad ogni passo, l'altezza di ogni ramo ricorsivo è quindi al più $O(\log(n))$
\end{itemize}
Ne consegue che lo spazio totale necessario sarà in $O(\log^2(n))$.\hfill$\blacksquare$\acc  
Occupiamoci adesso di definire le classi di spazio per le NTM (TM non deterministiche). \acc 
\defi{} $NSPACE(f(n))=\{L \ | \ \exists \text{ NTM } N \text{ t.c. } L(N)=L \land N \text{ ha complessità di spazio } f(n)\}$\acc 
la complessità di spazio di una NTM si valuta sul ramo di computazione che ha occupato più celle distinte. Ne conseguno naturalmente le seguenti classi 
$$ NPSPACE=\bigcup_{k\in \Z^+}NSPACE(n^k)$$
$$ NEXPSPACE=\bigcup_{k\in \N}NSPACE(2^{n^k})$$
$$ NLOG=NSPACE(\log(n))$$
Vedremo che $PSPACE=NPSPACE$, ossia, che il non determinismo non ha alcuna influenza sulla complessità di spazio.\acc 
\textbf{Lemma} : $PATH\in NLOG$\acc 
\dimo{} Si considera una NTM $N$ per $PATH$ che esegue la seguente procedura\begin{itemize}
    \item su input $G,s,t$ accetta se $s=t$
    \item calcola deterministicamente in spazio logaritmico $n=|V|$
    \item definisce $curNode=s$
    \item per ogni $i$ da 1 ad $n$\begin{itemize}
        \item \textit{non deterministicamente} (fork) considera ogni possibile nodo $u\in V$
        \item se $(curNode,u)\in E$, allora $curNode=u$
        \item se $curNode=t$, accetta 
        \item se $(curNode,u)\notin E$, allora rifiuta
    \end{itemize}
    \item rifiuta 
 \end{itemize}
 Su ogni ramo il nuemro di variabili utilizzate è costante, l'algritmo opera in complessità spaziale $O(\log(n))$ e accetta solo se esiste un cammino da $s$ a $t$.
\hfill$\blacksquare$\acc  
Vedremo che $PATH$ è $NLOG$ completo.\acc 
\teo{(di Savitch)} $NLOG\subseteq P\cap SPACE(\log^2(n))$\acc 
\dimo{} Sia $A\in NLOG$, ossia $\exists \  NTM \ N$ tale che $L(N)=A$ e $N$ ha complessità spaziale $O(\log(n))$, si ricordi che una configurazione è della forma 
$$ C=afjej\ q_i \ sfj \ ;\ r$$
dove $q_i$ è lo stato, che indica anche la posizione della testina sul nastro di lavoro, ed $r$ è la posizione della testina sul nastro di input. Esistono 
$2^{O(\log(n))}$ possibili configurazioni, si associa, per ogni coppia $N,x$ ($x$ input di $N$) un grafo $G_{N,x}$, i cui nodi sono le possibili configurazioni $C$, ed esiste un arco $(C,C')$ se $C'$ è una configurazione che può seguire da $C$ in accordo con le regole stabilite dalla funzione di transizione $\delta$. Inoltre esisterà una configurazione iniziale $C_{start}$ (naturalmente definita). 
\begin{center}
    \includegraphics[width=0.5\textwidth ]{images/grafoGNx.drawio.pdf}
\end{center}
Definiamo configurazioni \textit{accettanti}, quelle in cui lo stato corrente è lo stato accettante $q_{acc}$, è possibile (senza perdita di generalità) trasformare ogni possibile grafo $G_{N,x}$ in modo che abbia una sola configurazione accettante, ossia 
$$ C_{acc}=q_{acc}\blank\  ; \ 1$$
Basta assumere che $N(x)$, prima di accettare, si occupi di cancellare ogni elemento sul nastro di lavoro, e spostare le testine (di entrambi i nastri) nell'estrema sinistra. Tale trasformazione preserva la complessità di spaziale di $N$.
\begin{center}
    \includegraphics[width=0.5\textwidth ]{images/Cstart.drawio.pdf}
\end{center}
Naturalmente, $N(x)$ accetta se e solo se esiste un cammino da $C_{start}$ a $C_{acc}$ nel grafo $G_{N,x}$. Pertanto, $A\in NLOG \implies A\in P$, dato che esiste una $TM$ $M$ (deterministica) che decide in tempo polinomiale se $x\in A$ tramite la seguente procedura\begin{itemize}
    \item si scrive $G_{N,x}$ enumerando vertici ed archi, ciò richiede complessità temporale $poly(n)$.
    \item $M$ utilizza l'algoritmo di marcatura (che ha complessità polinomiale) per verificare che esiste un cammino fra i due nodi del grafo.
\end{itemize}
$$ NLOG\subseteq P$$
In maniera simile, è possibile utilizzare l'algoritmo ricorsivo visto ad inizio della sezione \ref{ndetSpace} per trovare un cammino fra i due nodi in complessità spaziale $O(\log^2(n))$.\acc 
Non è però possibile scrivere il grafo $G_{N,x}$ nel nastro di lavoro in quanto violerebbe lo spazio logaritmico, ma ciò non è necessario in quanto si può valutare di volta in volta se esiste un arco $(C_i,C_j)$ valutando l'input $x$.\hfill$\blacksquare$\acc  
Il teorema di Savitch si può generalizzare:\acc 
\teo{} $NSPACE(f(n))\subseteq DTIME(2^{O(f(n))})\cap SPACE(f(n)^2)$\acc 
\textbf{Corollario} : $NPSPACE\subseteq EXP\cap PSPACE$, ma allora, sapendo già che $PSPACE\subseteq NPSPACE$ ne consegue che\sapbox{$$NPSPACE=PSPACE$$}
\flowerLine 
\section{$NLOG$ completezza}
\defi{($NLOG$ completezza)} $B$ è $NLOG$ completo se\begin{itemize}
    \item $B\in NLOG$
    \item se $A\in NLOG$, allora $A\le_m^L B$
\end{itemize}
$\le_m^L$ è un nuovo tipo di riduzione che deve garantire la seguente proprietà
$$ A\le_m^L B\implies A\in LOG \land B\in LOG$$
\defi{} si definisce la riducibilità \textbf{log space reduction} $\le_m^L$ definita come segue:\\  
$A\le_m^L B \text{ se }\exists R:{0,1}^*\rightarrow {0,1}^*$  computabile in spazio $O(\log(n))$ tale che 
$$ x\in A \iff R(x)=B$$
Bisogna definire il termine "computabile in spazio $O(\log(n))$", il termine $R(x)$ potrebbe avere dimensione $poly(n)$, in tal caso scrivendo $R(x)$ sul nastro di lavoro, si violerebbe lo spazio logaritmico, si considera nel modello della TM un \textit{nastro di output} che può essere scritto una sola volta, senza mai modificare le celle utilizzate, tale nastro verrà usato per scrivere l'output $R(x)$.\acc 
\teo{} $PATH$ è $NLOG$ completo.\acc 
\dimo{} Sappiamo già che $PATH$ è un linguaggio in $NLOG$, bisogna dimostrare che tutti i linguaggi in tale insieme si riducono a $PATH$. Sia $A$ un qualsiasi linguaggio in $NLOG$, e sia $N$ la NTM (di complessità logaritmica) tale che $L(A)=N$\begin{itemize} \item si definisce una TM $R$ che, dato un input $x$ per $N$, costruisce il grafo $G_{N,x}$ delle cofnigurazioni visto nella dimostrazione del teroema di Savitch. \item L'output di $R$ (ossia $R(x)$) sarà ($G_{N,x}, C_{start}, C_{acc}$) \item Ne consegue naturalmente che $x\in A \iff$ esiste un cammino $C_{start}\rightarrow C_{acc}$ in $G_{N,x}\iff$ $R(x)=(G_{N,x}, C_{start}, C_{acc})\in PATH$
\end{itemize}
In particolare, è possibile enumerare tutte le possibili configurazioni (che sono polinomiali in $n$) con un numero $O(\log(n))$ celle, e con la medesima complessità di spazio è possibile verificare se, date due configurazioni $C,C'$, esiste o no l'arco $(C,C')$.\hfill$\blacksquare$\acc 
Si vuole mostrare che la definizione di \textit{log space reduction} soddisfi le seguenti proprietà\begin{itemize}
    \item $A\le_m^L B \implies \begin{cases}
        B\in LOG \implies A\in LOG\\ 
        B\in NLOG \implies A\in NLOG
    \end{cases}$
    \item $A\le_m^L B \land B \le_m^L C \implies A \le_m^L C$
\end{itemize}
\teo{} Se $P$ e $Q$ sono computabili in spazio $O(\log(n))$, allora lo è anche $R=P\circ Q$\acc 
\dimo{} Essendo che $P$ e $Q$ sono computabili in spazio $O(\log(n))$, e che \textit{"lo spazio limita il tempo"}, si ha che $P$ e $Q$ hanno complessità di tempo polinomiale\begin{itemize}
    \item $P$ ha complessità temporale $O(n^p)$ per qualche $p$
    \item $Q$ ha complessità temporale $O(n^q)$ per qualche $q$
\end{itemize}
Si vuole definire una TM $M$ capace di scrivere l'output $R(x)=Q(P(x))$ in complessità spaziale $O(\log(n))$.\acc 
Il problema, è che $y:=P(x)$ potrebbe essere lungo $poly(n)$, quindi bisogna calcolare $R(x)=Q(P(x))=Q(y)$ senza calcolare esplicitamente $y$.\acc 
\redText{la seguente parte la copio parola per parola da ciò che ha dettato il professore ma non l'ho capita}\acc 
La TM $M$ tiene traccia della posizione corrispondente a quello che sarebbe il nastro di input di $Q$, ciò richiede spazio $O(\log(n))$, $M$ deve determinare $y[i]$ ($i$-esimo carattere di $y$) e simulare un passo della computazione di $Q$, ciò può essere fatto in spazio $O(\log(n))$. \acc 
Si ricalcola $P(x)$ fino ad ottenere $y[i]$, scartando il resto, questo permette di simulare un passo di computazione di $Q$. \redText{ fine parte ambigua}\hfill$\blacksquare$\acc 
\textbf{Corollario} : $A\le_m^L B \land B \in LOG \implies A \in LOG$\acc 
Fino ad ora abbiamo visto esempi di linguaggi\begin{itemize}
    \item $NP$ completi
    \item $NLOG$ completi
    \item $coNP$ completi
\end{itemize}
Vediamo adesso alcuni esempi di linguaggi $P$ completi e $PSPACE$ completi.\acc 
\defi{($P$ completezza)} $C$ è $P$ completo se $C\in P$ e $\forall A\in P$ si ha che $A\le_m^L C$\acc 
\textbf{Osservazione} : La definizione riguarda la classe $P$ (complessità temporale) ma utilizza la \textit{log space reduction} (inerente alla complessità spaziale).
\sapbox{\textbf{Problema aperto} : La seguente proposizione non è stata dimostrata, e tutt'ora non si sa se sia vera o false $$ C\in LOG\implies P\subseteq LOG$$ 

}
Vediamo un esempio di problema $P$ completo: 
$$ CIRCUIT-EVAL=\{<C,x> \ | \ C\text{ è un circuito booleano }\land C(x)=1\}$$
\teo{} $CIRCUIT-EVAL$ è $P$ completo.\acc 
\dimo{} \redText{TODO}\acc 
Vediamo adesso un particolare linguaggio, conosciamo il linguaggio $$ SAT=\{<\phi>\ | \ \exists x,\ \phi(x)=1 \}$$
Ed il linguaggio delle tautologie 
$$ TAUT=\{<\phi> \ | \ \forall x, \ \phi(x)=1\}$$
\defi{} Una formula booleana è \textbf{totalmente quantificata} se ogni variabile è coinvolta in un quantificatore (universale o esistenziale). Un esempio:$$\exists z \forall x \exists y (x\rightarrow y) \lor z $$
Una formula di questo tipo è una \textit{sentenza}, o è vera o falsa, non ci sono assegnamenti di variabili in quanto sono già considerate dai quantificatori.
Si definisce il seguente linguaggio delle \textbf{Totally Quantified Boolean Formula}
$$ TQBF=\{<\phi> \ | \ \phi \text{ è una TQBF} \}$$
\teo{} $TQBF\in PSPACE$\acc 
\dimo{} si considera il seguente algoritmo ricorsivo \begin{quote}
    \code{IsTrue($Q_1x_1,Q_2x_2\dots , Q_nx_n,\phi(x)$)} \\ 
    dove $x=x_1,x_2\dots ,x_n$
\end{quote}
La procedura è descritta dai seguenti passi \begin{itemize}
    \item Se $n=0\implies \phi$ ha solo costanti, ritorna la valutazione 
    \item Altrimenti, se $Q_1=\exists$, ritorna \begin{quote}
        \code{IsTrue($Q_2x_2,Q_3x_3\dots , Q_nx_n,\phi(0,x_2,x_3\dots ,x_n)$)} 
\\ $\lor$\\
        \code{IsTrue($Q_2x_2,Q_3x_3\dots , Q_nx_n,\phi(1,x_2,x_3\dots ,x_n)$)} 
    \end{quote}
    \item Altrimenti, se $Q_1=\forall$, ritorna \begin{quote}
        \code{IsTrue($Q_2x_2,Q_3x_3\dots , Q_nx_n,\phi(0,x_2,x_3\dots ,x_n)$)} 
\\ $\land$\\
        \code{IsTrue($Q_2x_2,Q_3x_3\dots , Q_nx_n,\phi(1,x_2,x_3\dots ,x_n)$)} 
    \end{quote}
\end{itemize}
L'algoritmo è corretto, la profondità di ricorsione è $n$ e ad ogni chiamata ricorsiva lo spazio utilizzato è $O(n)$. Lo spazio totale utilizzato dall'algoritmo è $O(n^2)\implies TQBF\in PSPACE$.\hfill$\blacksquare$\acc 
\teo{} $TQBF$ è $PSPACE$-difficile. Ossia, ogni problema in $PSPACE$ si riduce in tempo plinomiale (quindi anche in spazio polinomiale) a $TQBF$.\acc 
\dimo{} Sia $A$ un generico linguaggio in $PSPACE$, esiste una TM che decide $A$ in spazio polinomiale, sia questo $O(n^a)$ per qualche $a\in \N$. Vogliamo definire una riduzione $R : \{0,1\}^*\rightarrow \{0,1\}^*$, che resituisca in output una Totally Quantified Boolean Formula $\phi_M$ tale che 
$$ x\in A \iff M(x)\text{ accetta }\iff R(x)=\phi_M \text{ è soddisfatta }$$
\textbf{Osservazione} : Una stessa TQBF può avere più formulazioni equivalenti, ad esempio, si può assumere che tutti i quntificatori siano all'inizioe della formula 
$$ \exists x_1 \forall x_2 [x_1\rightarrow x_2]\land \dots$$
Sappiamo che $M(x)$ opera in spazio polinomiale, ha $2^{O(n^a)}$ possibili configurazioni, una di queste sarà $C_{start}$ definita $$ q_0 \ \blank \ ; \  1$$
Come già visto, si può assumere (senza perdita di generalità) che esista un un'unica configurazione accettante $C_{acc}$.\acc 
Se dovessimo considerare il grafo $G_{M,x}$ considerato nella dimostrazione del teorema di Savitch, sappiamo che $M(x)$ accetta se e solo se esiste un cammino $C_{start}\longrightarrow C_{acc}$ in $G_{M,x}$.\acc 
La riduzione $R$ dato $x$, dovrà calcolare una formula $\phi_M$ tale che, questa è soddisfacibile solo se esiste un cammino nel $C_{start}\longrightarrow C_{acc}$ in $G_{M,x}$. La formula $\phi_M$ deve avere dimensione polinomiale rispetto alle dimensioni dell'input $n$, e deve essere computabile in spazio polinomiale.\acc 
Le variabili di $\phi_M$ sono le possibili configurazioni $C_i$ dell'esecuzione di $M$ su $x$.\acc 
\textbf{Idea 1)} : Consideriamo una sotto formula $\phi_{yields}$ tale che  $\phi_{yields}(C_i,C_j)=1\iff$ $C_j$ segue la configurazione $C_i$ in accordo con le regole stabilite dalla funzione di transazione $\delta$. Potremmo definire $\phi_M$ come segue 
$$ \begin{matrix}
    \phi_M=\exists C_1\exists C_2\exists C_3 \dots \exists C_l\\ 
    \phi_{yields}(C_1,C_2)\land\phi_{yields}(C_2,C_3)\dots \phi_{yields}(C_{l-1},C_l)
\end{matrix}$$
Con $C_1=C_{start}$ e $C_l=C_{acc}$. Seppure l'idea è logicamente corrette, il numero $l$ di possibili configurazioni è in $2^{O(n^a)}$, è quindi esponenziale, allora la riduzione $R$ in questione non è computabile in spazio polinomiale, e quindi non va bene.\acc
\textbf{Idea 2)} : Un'altra idea è quella di utilizzare l'algoritmo ricorsivo visto nel teorema di Savitch, definiamo la procedura $\phi_k(C_0,C_1)$, che è vera se e solo se esiste un cammino da $C_0$ a $C_1$ di lunghezza $2^k$ passi. La riduzione $R(x)$ dovrà solamente restituire $\phi_{O(n^a)}(C_{start},C_{acc})$.\begin{itemize}
    \item caso base ($k=0$) : allora $\phi_0(C_0,C_1)$ è vera se e solo se $\phi_{yields}(C_0,C_1)$ è vera, oppure $C_0=C_1$. 
    \item caso induttivo : $\phi_k(C_0,C_1)$ è definita come:\begin{itemize}
        \item $\exists \ C_{mid}$ tale che $$\phi_{k-1}(C_0,C_{mid})\land \phi_{k-1}(C_{mid},C_1) $$ 
    \end{itemize}
\end{itemize}
Anche in questo caso la logica è corretta ma le dimensioni di $\phi_k$ sono esponenziali dato che raddoppiano ad ogni passo $$|\phi_k|=O(n^a)+2|\phi_{k-1}|$$\acc
\textbf{Idea Finale)} : Si considera l'idea 2 ma scritta in maniera diversa,  
$$\phi_k(C_0,C_1) $$ è definita come segue : $\exists C_{mid}\forall D\forall D'$ tale che 
$$ \begin{pmatrix}
    (D,D')=(C_0,C_{mid})\\ \lor \\ 
    (D,D')=(C_{mid},C_1)
\end{pmatrix}\implies \phi_{k-1}(D,D')$$
In questo caso le dimensioni di $\phi_k$ sono $O(n^a)+|\phi_{k-1}|=O(n^{2a})$, che è polinomiale.\hfill$\blacksquare$\acc 
\teo{(Immerman-Szelepcsényi)}: La classe $NLOG$ è chiusa rispetto al complemento $$NLOG=coNLOG$$\dimo{}
Essendo che $PATH$ è $NLOG$ completo, è necessario dimostrare che $PATH$ è in $coNLOG$, ossia che $\overline{PATH}$ è in $LOG$.\acc 
Bisogna quindi trovare un certificato in spazio logaritmico che verifichi che, dato un grafo, $(G,s,t)$, non esiste il cammino $s\longrightarrow t$. \acc 
Sia $V$ il \textbf{verificatore}, questo ha 3 nastri\begin{itemize}
    \item Nastro di input, contiene $<G,s,t>$
    \item Nastro di lavoro, il cui numero di celle usate deve essere logaritmico
    \item Nastro \textit{read once}, può essere letto una sola volta e conterrà il certificato per dimostrare che il cammino non esiste 
\end{itemize}
Definiamo $R_l$ l'insieme dei nodi raggiungibili da $s$ in (al più) $l$ passi, e denotiamo $r_l=|R_l|$ il numero di questi nodi. Chiaramente $R_0=\{s\}\implies r_0=1$. È ovvio che $R_l\subseteq R_{l+1}$.\acc 
Il certificato contenuto nel nastro read once, sarà diviso in diverse parti (qui sotto organizzate in righe) del tipo $$\begin{matrix}
    \text{certificato }r_0\\ 
    \text{certificato }r_1\\ 
    \text{certificato }r_2\\ 
    \vdots \\ 
    \text{certificato }r_n \\ 
    \text{certificato : non esiste il cammino }s\longrightarrow t
\end{matrix}$$
\textit{per ora si omette la struttura interna dei certificati}. \acc
Il verificatore, per definire il certificato $r_{l+1}$, deve avere in possesso $r_l$ e $l$, e non tutti i certificati precedenti.\\
\redText{finire/riscrivere la dimostrazione}
\flowerLine 
\section{Teoremi di Gerarchia}
I risultati presentati in questa sezione permettono di separare le classi di complessità utilizzando la tecnica della \textit{diagonalizzazione}.\acc 
Il teorema di \textit{gerarchia dei tempi} afferma che se $t_1(n)$ è asintoticamente, molto più piccola di $t_2(n)$, allora esiste un linguaggio $L\in Dtime(t_2(n))$ tale che 
$L\notin Dtime(t_1(n))$\acc 
Vediamo un \textg{idea di dimostrazione}, si definisce una codifica $$[.]_{TM}:\sigma^*\rightarrow TM $$
che associa ad ogni stringa una TM, se la stringa non corrisponde ad alcuna TM sensata, allora se ne attribuisce una di default. In tal modo 
$$ \forall TM \ M  \ \exists x \text{ tale che }[x]_{TM}=M$$
Definisco una TM $D$ in modo tale che termini sicuramente in $O(t_2(n))$ passi, il comportamento è descritto dalla seguente procedura \begin{itemize}
    \item 
\end{itemize}














\end{document}